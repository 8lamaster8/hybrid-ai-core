"""
üîÑ –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —Ü–∏–∫–ª–æ–≤ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π AI-—Å–∏—Å—Ç–µ–º—ã.
–£–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ü–∏–∫–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã,
–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã (–¥–µ—Ç–µ–∫—Ç–∏–≤, –∞–Ω–∞–ª–∏—Ç–∏–∫, –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä, –∫–æ–º–∏—Ç–µ—Ç, —Ö—Ä–∞–Ω–∏–ª–∏—â–∞).
"""

import asyncio
import logging
import random
import time
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple

logger = logging.getLogger(__name__)

class CycleType(Enum):
    DISCOVERY = "discovery"      # –û—Ç–∫—Ä—ã—Ç–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ–º
    DEEPENING = "deepening"      # –£–≥–ª—É–±–ª–µ–Ω–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–º—ã
    EXPANSION = "expansion"      # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
    META_ANALYSIS = "meta"       # –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã
    MAINTENANCE = "maintenance"  # –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ (–æ—á–∏—Å—Ç–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)


class LearningCycle:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, cycle_type: CycleType, services: Dict[str, Any]):
        self.cycle_type = cycle_type
        self.services = services  # { 'detective': ..., 'analyst': ..., 'interviewer': ..., 'committee': ..., 'engram': ..., 'graph': ..., 'chroma': ... }
        
        self.stats = {
            'executions': 0,
            'successful': 0,
            'failed': 0,
            'avg_duration': 0.0,
            'last_execution': None
        }
    
    async def execute(self) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Ü–∏–∫–ª ‚Äì –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö."""
        raise NotImplementedError
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            'cycle_type': self.cycle_type.value,
            **self.stats
        }
    
    def _update_stats(self, success: bool, duration: float):
        self.stats['executions'] += 1
        if success:
            self.stats['successful'] += 1
        else:
            self.stats['failed'] += 1
        # —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        old_avg = self.stats['avg_duration']
        self.stats['avg_duration'] = old_avg + (duration - old_avg) / self.stats['executions']
        self.stats['last_execution'] = datetime.now().isoformat()


class DiscoveryCycle(LearningCycle):
    """
    –¶–∏–∫–ª –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–æ–≤—ã—Ö —Ç–µ–º.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ç–µ–º, –¥–µ—Ç–µ–∫—Ç–∏–≤ –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏,
    –∫–æ–º–∏—Ç–µ—Ç –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    """
    
    def __init__(self, services: Dict[str, Any]):
        super().__init__(CycleType.DISCOVERY, services)
        self.min_topics = 3
        self.max_topics = 10
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ DISCOVERY")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'discovered_topics': [],
            'research_completed': [],
            'errors': []
        }
        
        try:
            interviewer = self.services.get('interviewer')
            detective = self.services.get('detective')
            analyst = self.services.get('analyst')
            committee = self.services.get('committee')
            engram = self.services.get('engram')
            graph = self.services.get('graph_db')
            chroma = self.services.get('chroma_db')
            
            if not all([interviewer, detective, analyst, committee]):
                raise RuntimeError("–ù–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã –¥–æ—Å—Ç—É–ø–Ω—ã")
            
            # 1. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ–º—ã –æ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–æ—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π)
            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞; –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É.
            new_topics = await self._discover_potential_topics()
            results['discovered_topics'] = [{'name': t, 'priority': 0.5} for t in new_topics[:self.max_topics]]
            
            if not new_topics:
                logger.info("–ù–æ–≤—ã—Ö —Ç–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                self._update_stats(success=False, duration=time.time() - start_time)
                results['success'] = False
                return results
            
            # 2. –ë–µ—Ä—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
            topics_to_research = new_topics[:self.min_topics]
            
            for topic in topics_to_research:
                try:
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    questions = await interviewer.generate_research_questions(topic, depth=1, num_questions=5)
                    
                    # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–µ—Ç–µ–∫—Ç–∏–≤
                    investigation = await detective.investigate_topic_advanced(topic, questions[:3])
                    if not investigation.get('success'):
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ç–µ–º—É {topic}")
                        continue
                    
                    chunks = investigation.get('content_chunks', [])
                    if not chunks:
                        continue
                    
                    # –ê–Ω–∞–ª–∏–∑
                    analysis = await analyst.analyze(chunks, query=topic)
                    key_points = analysis.get('key_points', [])
                    confidence = analysis.get('confidence', 0.0)
                    
                    # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–º–∏—Ç–µ—Ç–æ–º (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞)
                    if key_points:
                        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –∫–ª—é—á–µ–≤–æ–π –ø—É–Ω–∫—Ç –∫–∞–∫ –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                        committee_result = await committee.evaluate_data({
                            'topic': topic,
                            'text': key_points[0],
                            'url': investigation.get('metadata', [{}])[0].get('url', '')
                        })
                        if committee_result.get('final_decision', {}).get('decision') != 'approve':
                            logger.info(f"–¢–µ–º–∞ {topic} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –∫–æ–º–∏—Ç–µ—Ç–æ–º")
                            continue
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è
                    await self._store_knowledge(topic, {
                        'summary': ' '.join(key_points[:2]),
                        'key_points': key_points,
                        'confidence': confidence,
                        'source': 'discovery_cycle'
                    })
                    
                    results['research_completed'].append({
                        'topic': topic,
                        'chunks': len(chunks),
                        'key_points': len(key_points),
                        'confidence': confidence
                    })
                    
                    logger.info(f"‚úÖ –¢–µ–º–∞ '{topic}' –∏–∑—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–º—ã {topic}: {e}"
                    logger.error(error_msg)
                    results['errors'].append(error_msg)
            
            success = len(results['research_completed']) > 0
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ DiscoveryCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results
    
    async def _discover_potential_topics(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–æ–≤—ã–µ —Ç–µ–º—ã.
        –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
        - –ê–Ω–∞–ª–∏–∑ –Ω–µ–ø–æ–∫—Ä—ã—Ç—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
        - –°–ª—É—á–∞–π–Ω—ã–µ —Ç–µ–º—ã –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã).
        - –ò–∑ Engram —Ç–µ–º—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é.
        """
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—â–∏—Ö —Ç–µ–º
        return [
            "–ö–≤–∞–Ω—Ç–æ–≤–∞—è –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å",
            "–¢–µ–æ—Ä–µ–º–∞ –ì—ë–¥–µ–ª—è –æ –Ω–µ–ø–æ–ª–Ω–æ—Ç–µ",
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
            "–ê–ª–≥–æ—Ä–∏—Ç–º—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏",
            "–ò—Å—Ç–æ—Ä–∏—è –î—Ä–µ–≤–Ω–µ–≥–æ –†–∏–º–∞",
            "–§–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑",
            "–¢–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫–∞",
            "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏–∑–∏—Å—ã",
            "–§–∏–ª–æ—Å–æ—Ñ–∏—è –ö–∞–Ω—Ç–∞"
        ]
    
    async def _store_knowledge(self, topic: str, knowledge: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞–Ω–∏—è –≤–æ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        tasks = []
        engram = self.services.get('engram')
        chroma = self.services.get('chroma_db')
        graph = self.services.get('graph_db')
        
        if engram:
            tasks.append(engram.store(
                key=topic,
                content=knowledge.get('summary', ''),
                metadata={
                    'topic': topic,
                    'confidence': knowledge.get('confidence', 0.5),
                    'timestamp': datetime.now().isoformat(),
                    'source': 'discovery_cycle',
                    'key_points': knowledge.get('key_points', [])[:5]
                },
                confidence=knowledge.get('confidence', 0.5)
            ))
        
        if chroma and knowledge.get('key_points'):
            for i, point in enumerate(knowledge['key_points'][:5]):
                if len(point) > 50:
                    tasks.append(chroma.add_document(
                        text=point,
                        metadata={
                            'topic': topic,
                            'type': 'key_point',
                            'source': 'discovery_cycle',
                            'confidence': knowledge.get('confidence', 0.5),
                            'index': i
                        }
                    ))
        
        if graph:
            # –í –≥—Ä–∞—Ñ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ —É–∑–µ–ª —Å –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ (–ø–æ–∫–∞ –±–µ–∑ –æ—Ç–Ω–æ—à–µ–Ω–∏–π)
            tasks.append(graph.add_knowledge_chunk(
                topic=topic,
                chunk={
                    'summary': knowledge.get('summary', '')[:500],
                    'key_points': knowledge.get('key_points', [])[:5],
                    'confidence': knowledge.get('confidence', 0.5)
                },
                relations=[]
            ))
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.debug(f"–ó–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ '{topic}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {len(tasks)} —Ö—Ä–∞–Ω–∏–ª–∏—â")


class DeepeningCycle(LearningCycle):
    """
    –¶–∏–∫–ª —É–≥–ª—É–±–ª–µ–Ω–∏—è –≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–º—ã.
    –í—ã–±–∏—Ä–∞–µ—Ç —Ç–µ–º—É —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã,
    –∏—â–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    """
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ DEEPENING")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'deepened_topics': [],
            'errors': []
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â
            existing_topics = await self._get_existing_topics()
            if not existing_topics:
                logger.warning("–ù–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è")
                self._update_stats(False, time.time() - start_time)
                results['success'] = False
                return results
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–µ–º—É –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª—É—á–∞–π–Ω—É—é –∏–∑ —Ç–æ–ø-10)
            import random
            topic = random.choice(existing_topics[:10])
            logger.info(f"–í—ã–±—Ä–∞–Ω–∞ —Ç–µ–º–∞ –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è: {topic}")
            
            interviewer = self.services.get('interviewer')
            detective = self.services.get('detective')
            analyst = self.services.get('analyst')
            committee = self.services.get('committee')
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–≥–ª—É–±–ª—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã (–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ)
            questions = await interviewer.generate_deepening_questions(
                knowledge_chunks=None,  # –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞–Ω–∏—è
                current_depth=1,
                max_questions=3
            )
            if not questions:
                questions = [f"–ö–∞–∫–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã {topic}?"]
            
            results['deepened_topics'].append({
                'topic': topic,
                'questions': questions
            })
            
            # –ò—Å—Å–ª–µ–¥—É–µ–º
            investigation = await detective.investigate_topic_advanced(topic, questions)
            if not investigation.get('success'):
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {investigation.get('error')}")
            
            chunks = investigation.get('content_chunks', [])
            if chunks:
                analysis = await analyst.analyze(chunks, query=topic)
                key_points = analysis.get('key_points', [])
                confidence = analysis.get('confidence', 0.5)
                
                if key_points:
                    # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                    if committee:
                        sample_text = key_points[0]
                        committee_result = await committee.evaluate_data({
                            'topic': topic,
                            'text': sample_text,
                            'url': investigation.get('metadata', [{}])[0].get('url', '')
                        })
                        if committee_result.get('final_decision', {}).get('decision') != 'approve':
                            logger.info(f"–ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–µ–º–µ {topic} –æ—Ç–∫–ª–æ–Ω–µ–Ω—ã –∫–æ–º–∏—Ç–µ—Ç–æ–º")
                        else:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                            await self._store_knowledge(topic, {
                                'summary': ' '.join(key_points[:2]),
                                'key_points': key_points,
                                'confidence': confidence,
                                'source': 'deepening_cycle'
                            })
                            results['deepened_topics'][0]['key_points_added'] = len(key_points)
                    else:
                        # –±–µ–∑ –∫–æ–º–∏—Ç–µ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                        await self._store_knowledge(topic, {
                            'summary': ' '.join(key_points[:2]),
                            'key_points': key_points,
                            'confidence': confidence,
                            'source': 'deepening_cycle'
                        })
                        results['deepened_topics'][0]['key_points_added'] = len(key_points)
            
            success = True
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ DeepeningCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results
    
    async def _get_existing_topics(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â."""
        topics = set()
        graph = self.services.get('graph_db')
        if graph and hasattr(graph, 'get_all_topics'):
            topics.update(await graph.get_all_topics())
        engram = self.services.get('engram')
        if engram and hasattr(engram, 'get_all_keys'):
            topics.update(await engram.get_all_keys())
        return list(topics)
    
    async def _store_knowledge(self, topic: str, knowledge: Dict):
        """–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ DiscoveryCycle._store_knowledge."""
        tasks = []
        engram = self.services.get('engram')
        chroma = self.services.get('chroma_db')
        graph = self.services.get('graph_db')
        
        if engram:
            tasks.append(engram.store(
                key=topic,
                content=knowledge.get('summary', ''),
                metadata={
                    'topic': topic,
                    'confidence': knowledge.get('confidence', 0.5),
                    'timestamp': datetime.now().isoformat(),
                    'source': 'deepening_cycle',
                    'key_points': knowledge.get('key_points', [])[:5]
                },
                confidence=knowledge.get('confidence', 0.5)
            ))
        
        if chroma and knowledge.get('key_points'):
            for i, point in enumerate(knowledge['key_points'][:5]):
                if len(point) > 50:
                    tasks.append(chroma.add_document(
                        text=point,
                        metadata={
                            'topic': topic,
                            'type': 'key_point',
                            'source': 'deepening_cycle',
                            'confidence': knowledge.get('confidence', 0.5),
                            'index': i
                        }
                    ))
        
        if graph:
            tasks.append(graph.add_knowledge_chunk(
                topic=topic,
                chunk={
                    'summary': knowledge.get('summary', '')[:500],
                    'key_points': knowledge.get('key_points', [])[:5],
                    'confidence': knowledge.get('confidence', 0.5)
                },
                relations=[]
            ))
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)


class ExpansionCycle(LearningCycle):
    """
    –¶–∏–∫–ª —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏.
    –í—ã–±–∏—Ä–∞–µ—Ç –¥–≤–µ —Ç–µ–º—ã, –∏—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤—è–∑–∏ –≤ –≥—Ä–∞—Ñ.
    """
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ EXPANSION")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'connections': [],
            'errors': []
        }
        
        try:
            topics = await self._get_existing_topics()
            if len(topics) < 2:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–º –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–≤—è–∑–µ–π")
                self._update_stats(False, time.time() - start_time)
                results['success'] = False
                return results
            
            import random
            topic1 = random.choice(topics)
            topic2 = random.choice([t for t in topics if t != topic1])
            
            logger.info(f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É '{topic1}' –∏ '{topic2}'")
            
            detective = self.services.get('detective')
            analyst = self.services.get('analyst')
            graph = self.services.get('graph_db')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –æ —Å–≤—è–∑–∏
            query = f"–°–≤—è–∑—å –º–µ–∂–¥—É {topic1} –∏ {topic2}"
            investigation = await detective.investigate_topic_advanced(query, [query])
            if not investigation.get('success'):
                raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–≤—è–∑—å")
            
            chunks = investigation.get('content_chunks', [])
            if chunks:
                analysis = await analyst.analyze(chunks, query=query)
                key_points = analysis.get('key_points', [])
                confidence = analysis.get('confidence', 0.5)
                
                if key_points and graph:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤—è–∑—å –≤ –≥—Ä–∞—Ñ –∫–∞–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ
                    await graph.add_relation(topic1, topic2, relation_type="—Å–≤—è–∑–∞–Ω–æ_—Å", weight=confidence)
                    # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏–µ
                    await graph.add_knowledge_chunk(
                        topic=f"{topic1}_{topic2}",
                        chunk={
                            'summary': ' '.join(key_points[:2]),
                            'key_points': key_points[:5],
                            'confidence': confidence
                        },
                        relations=[(topic1, topic2, "—Å–≤—è–∑–∞–Ω–æ_—Å")]
                    )
                    
                    results['connections'].append({
                        'topic1': topic1,
                        'topic2': topic2,
                        'key_points': len(key_points),
                        'confidence': confidence
                    })
            
            success = len(results['connections']) > 0
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ ExpansionCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results
    
    async def _get_existing_topics(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –∏–∑ –≥—Ä–∞—Ñ–∞ –∏–ª–∏ —ç–Ω–≥—Ä–∞–º–∞."""
        topics = set()
        graph = self.services.get('graph_db')
        if graph and hasattr(graph, 'get_all_topics'):
            topics.update(await graph.get_all_topics())
        engram = self.services.get('engram')
        if engram and hasattr(engram, 'get_all_keys'):
            topics.update(await engram.get_all_keys())
        return list(topics)


class MetaAnalysisCycle(LearningCycle):
    """
    –¶–∏–∫–ª –º–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑–∞ —Å–∏—Å—Ç–µ–º—ã.
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–æ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å,
    –≤—ã–¥–∞—ë—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ META_ANALYSIS")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'analysis': {},
            'adjustments': [],
            'errors': []
        }
        
        try:
            # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ —Å–æ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
            stats = {}
            for name, service in self.services.items():
                if service and hasattr(service, 'get_metrics'):
                    try:
                        stats[name] = await service.get_metrics()
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç {name}: {e}")
            
            # –ê–Ω–∞–ª–∏–∑ (–ø—Ä–∏–º–µ—Ä)
            analysis = {
                'total_knowledge_chunks': stats.get('chroma_db', {}).get('total_documents', 0),
                'graph_nodes': stats.get('graph_db', {}).get('nodes', 0),
                'graph_edges': stats.get('graph_db', {}).get('edges', 0),
                'engram_entries': stats.get('engram', {}).get('entries', 0),
                'detective_requests': stats.get('detective', {}).get('requests_processed', 0),
                'analyst_confidence_avg': stats.get('analyst', {}).get('avg_confidence', 0),
            }
            
            # –í—ã—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            adjustments = []
            if analysis['total_knowledge_chunks'] < 100:
                adjustments.append("–£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ü–∏–∫–ª–∞ DISCOVERY")
            if analysis['graph_edges'] < analysis['graph_nodes'] * 0.5:
                adjustments.append("–£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ü–∏–∫–ª–∞ EXPANSION")
            if analysis['analyst_confidence_avg'] < 0.4:
                adjustments.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ, —É–ª—É—á—à–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ–º–∏—Ç–µ—Ç–∞")
            
            results['analysis'] = analysis
            results['adjustments'] = adjustments
            
            success = True
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ MetaAnalysisCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results


class LearningCycleCoordinator:
    """
    –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.
    –£–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º, –≤—ã–±–∏—Ä–∞–µ—Ç —Ü–∏–∫–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤,
    –∑–∞–ø—É—Å–∫–∞–µ—Ç –∏—Ö –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.
    """
    
    def __init__(self, services: Dict[str, Any], config: Optional[Dict] = None):
        self.services = services
        self.config = config or {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤
        self.cycles = {
            CycleType.DISCOVERY: DiscoveryCycle(services),
            CycleType.DEEPENING: DeepeningCycle(services),
            CycleType.EXPANSION: ExpansionCycle(services),
            CycleType.META_ANALYSIS: MetaAnalysisCycle(services),
            # MAINTENANCE –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ, –µ—Å–ª–∏ –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        }
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ü–∏–∫–ª–æ–≤ (—Å—É–º–º–∞ = 1)
        self.cycle_priorities = {
            CycleType.DISCOVERY: 0.40,
            CycleType.DEEPENING: 0.30,
            CycleType.EXPANSION: 0.20,
            CycleType.META_ANALYSIS: 0.10,
        }
        
        # –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ)
        self.schedule_intervals = {
            CycleType.DISCOVERY: timedelta(hours=1),
            CycleType.DEEPENING: timedelta(hours=2),
            CycleType.EXPANSION: timedelta(hours=4),
            CycleType.META_ANALYSIS: timedelta(days=1),
        }
        
        self.last_execution = {ctype: None for ctype in CycleType}
        self.execution_history = []
        self.is_running = False
        self._task: Optional[asyncio.Task] = None
        
        logger.info("LearningCycleCoordinator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏."""
        if self.is_running:
            logger.warning("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
            return
        
        self.is_running = True
        logger.info("–ó–∞–ø—É—Å–∫ LearningCycleCoordinator")
        
        while self.is_running:
            try:
                cycle_type = self._select_cycle_to_run()
                if cycle_type:
                    logger.info(f"–í—ã–±—Ä–∞–Ω —Ü–∏–∫–ª: {cycle_type.value}")
                    cycle = self.cycles[cycle_type]
                    result = await cycle.execute()
                    
                    self.last_execution[cycle_type] = datetime.now()
                    self.execution_history.append({
                        'timestamp': datetime.now().isoformat(),
                        'cycle': cycle_type.value,
                        'result': result,
                        'duration': result.get('duration_seconds', 0)
                    })
                    
                    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    self._adapt_priorities(cycle_type, result)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–º –∏—Å—Ç–æ—Ä–∏—é
                    if len(self.execution_history) > 1000:
                        self.execution_history = self.execution_history[-500:]
                
                # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å)
                await asyncio.sleep(60)  # –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
                
            except asyncio.CancelledError:
                logger.info("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É")
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞: {e}", exc_info=True)
                await asyncio.sleep(300)  # –ø—Ä–∏ –æ—à–∏–±–∫–µ –∂–¥—ë–º 5 –º–∏–Ω—É—Ç
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä."""
        self.is_running = False
        if self._task and not self._task.done():
            self._task.cancel()
        logger.info("LearningCycleCoordinator –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    async def run_cycle(self, cycle_type: CycleType) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ü–∏–∫–ª –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ-–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)."""
        if cycle_type not in self.cycles:
            return {'error': f'–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ü–∏–∫–ª–∞: {cycle_type}'}
        
        logger.info(f"–ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ {cycle_type.value} –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é")
        cycle = self.cycles[cycle_type]
        result = await cycle.execute()
        self.last_execution[cycle_type] = datetime.now()
        return {
            'cycle': cycle_type.value,
            'result': result,
            'timestamp': datetime.now().isoformat()
        }
    
    def _select_cycle_to_run(self) -> Optional[CycleType]:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ü–∏–∫–ª –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞."""
        now = datetime.now()
        candidates = []
        
        for cycle_type, interval in self.schedule_intervals.items():
            last = self.last_execution[cycle_type]
            if last is None or (now - last) >= interval:
                # –¶–∏–∫–ª –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É
                priority = self.cycle_priorities.get(cycle_type, 0)
                candidates.append((cycle_type, priority))
        
        if not candidates:
            return None
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        total = sum(p for _, p in candidates)
        if total == 0:
            return None
        
        # –í—ã–±–æ—Ä —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Å —É—á—ë—Ç–æ–º –≤–µ—Å–æ–≤
        r = random.random()
        cumulative = 0.0
        for cycle, prob in candidates:
            cumulative += prob / total
            if r <= cumulative:
                return cycle
        
        return candidates[-1][0]  # –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
    
    def _adapt_priorities(self, cycle_type: CycleType, result: Dict[str, Any]):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."""
        success = result.get('success', False)
        if success:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
            self.cycle_priorities[cycle_type] *= 1.1
        else:
            # –£–º–µ–Ω—å—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ
            self.cycle_priorities[cycle_type] *= 0.9
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        total = sum(self.cycle_priorities.values())
        for ctype in self.cycle_priorities:
            self.cycle_priorities[ctype] /= total
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ –∏ –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤."""
        cycle_stats = {ctype.value: self.cycles[ctype].get_stats() for ctype in self.cycles}
        return {
            'is_running': self.is_running,
            'cycle_priorities': {k.value: v for k, v in self.cycle_priorities.items()},
            'last_executions': {k.value: v.isoformat() if v else None for k, v in self.last_execution.items()},
            'cycle_stats': cycle_stats,
            'total_executions': len(self.execution_history)
        }


# –î–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_learning_coordinator: Optional[LearningCycleCoordinator] = None

def get_learning_coordinator(services: Optional[Dict[str, Any]] = None) -> LearningCycleCoordinator:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞—ë—Ç –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)."""
    global _learning_coordinator
    if _learning_coordinator is None:
        if services is None:
            raise RuntimeError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ")
        _learning_coordinator = LearningCycleCoordinator(services)
    return _learning_coordinator