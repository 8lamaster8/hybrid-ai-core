"""
üîÑ –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —Ü–∏–∫–ª–æ–≤ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π AI-—Å–∏—Å—Ç–µ–º—ã.
–£–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ü–∏–∫–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã,
–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã.
"""

import asyncio
import logging
import random
import time
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urlparse


import os
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ª–æ–≥-—Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
learning_log_file = './data/logs/learning_ai.log'
os.makedirs(os.path.dirname(learning_log_file), exist_ok=True)

# –°–æ–∑–¥–∞—ë–º handler –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª
fh = logging.FileHandler(learning_log_file, encoding='utf-8', mode='a')
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)

# –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è
logger = logging.getLogger(__name__)
logger.addHandler(fh)

# –ï—Å–ª–∏ —Ö–æ—á–µ—à—å, —á—Ç–æ–±—ã –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è –ù–ï –¥—É–±–ª–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ñ–∞–π–ª–µ autonomous_ai.log:
logger.propagate = False

# –ï—Å–ª–∏ —Ö–æ—á–µ—à—å —Ç–∞–∫–∂–µ –≤–∏–¥–µ—Ç—å –∏—Ö –≤ –∫–æ–Ω—Å–æ–ª–∏ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):
# ch = logging.StreamHandler()
# ch.setFormatter(formatter)
# logger.addHandler(ch)




logger = logging.getLogger(__name__)


class CycleType(Enum):
    DISCOVERY = "discovery"      # –û—Ç–∫—Ä—ã—Ç–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ–º
    DEEPENING = "deepening"      # –£–≥–ª—É–±–ª–µ–Ω–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–º—ã
    EXPANSION = "expansion"      # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
    META_ANALYSIS = "meta"       # –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã
    MAINTENANCE = "maintenance"  # –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ (–æ—á–∏—Å—Ç–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)


class LearningCycle:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, cycle_type: CycleType, services: Dict[str, Any]):
        self.cycle_type = cycle_type
        self.services = services  # { 'detective': ..., 'analyst': ..., 'interviewer': ..., 'committee': ..., 'engram': ..., 'graph': ..., 'chroma': ... }
        
        self.stats = {
            'executions': 0,
            'successful': 0,
            'failed': 0,
            'avg_duration': 0.0,
            'last_execution': None
        }
    
    async def execute(self) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Ü–∏–∫–ª ‚Äì –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö."""
        raise NotImplementedError
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            'cycle_type': self.cycle_type.value,
            **self.stats
        }
    
    def _update_stats(self, success: bool, duration: float):
        self.stats['executions'] += 1
        if success:
            self.stats['successful'] += 1
        else:
            self.stats['failed'] += 1
        # —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        old_avg = self.stats['avg_duration']
        self.stats['avg_duration'] = old_avg + (duration - old_avg) / self.stats['executions']
        self.stats['last_execution'] = datetime.now().isoformat()


class DiscoveryCycle(LearningCycle):
    """
    –¶–∏–∫–ª –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–æ–≤—ã—Ö —Ç–µ–º.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ç–µ–º, –¥–µ—Ç–µ–∫—Ç–∏–≤ –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏,
    –∫–æ–º–∏—Ç–µ—Ç –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    """
    
    def __init__(self, services: Dict[str, Any], config: Dict = None):
        super().__init__(CycleType.DISCOVERY, services)
        self.config = config or {}
        self.min_topics = self.config.get('min_topics', 1)
        self.max_topics = self.config.get('max_topics', 3)
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ DISCOVERY")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'discovered_topics': [],
            'research_completed': [],
            'errors': []
        }
        
        try:
            interviewer = self.services.get('interviewer')
            detective = self.services.get('detective')
            analyst = self.services.get('analyst')
            committee = self.services.get('committee')
            engram = self.services.get('engram')
            graph = self.services.get('graph_db')
            chroma = self.services.get('chroma_db')
            
            if not all([interviewer, detective, analyst, committee]):
                raise RuntimeError("–ù–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã –¥–æ—Å—Ç—É–ø–Ω—ã")
            
            # 1. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
            candidate_topics = await self._discover_potential_topics()
            results['discovered_topics'] = [{'name': t, 'priority': p} for t, p in candidate_topics[:self.max_topics]]
            
            if not candidate_topics:
                logger.info("–ù–æ–≤—ã—Ö —Ç–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                self._update_stats(success=False, duration=time.time() - start_time)
                results['success'] = False
                return results
            
            # 2. –ë–µ—Ä—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
            topics_to_research = [t for t, p in candidate_topics[:self.min_topics]]
            logger.info(f"   üìã –í—ã–±—Ä–∞–Ω—ã —Ç–µ–º—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è: {topics_to_research}")
            
            for topic in topics_to_research:
                try:
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    questions = await interviewer.generate_research_questions(topic, depth=1, num_questions=5)
                    
                    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 3 –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
                    search_queries = questions[:3] if questions else [topic]
                    
                    # --- –≠—Ç–∞–ø—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ simple_question ---
                    all_docs = []
                    for query in search_queries:
                        # –ü–æ–∏—Å–∫
                        search_result = await detective.search(query, num_results=10)
                        if not search_result.get('success') or not search_result.get('results'):
                            continue
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–∏—Ç–µ—Ç–æ–º
                        filtered = await committee.batch_evaluate(search_result['results'][:10])
                        if not filtered:
                            continue
                        
                        # –û—Ç–±–æ—Ä URL (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã + –Ω–µ –±–æ–ª–µ–µ 3 –≤—Å–µ–≥–æ)
                        priority_domains = ['ru.wikipedia.org', 'habr.com', 'postnauka.ru', 'nplus1.ru', 'elementy.ru']
                        trash_domains = [
                            'otvet.mail.ru', 'answer.mail.ru', 'bolshoyvopros.ru',
                            'dzen.ru', 'yandex.ru/q', 'traveler.ru', 'rtraveler.ru',
                            'rambler.ru', 'mail.ru', 'ok.ru', 'vk.com',
                            'reverso.net', 'translate.', 'wordhippo.com', 'academic.ru',
                            '24smi.org', 'uznayvse.ru', 'socionika.info'
                        ]
                        
                        candidates = []
                        for doc in filtered:
                            url = doc.get('url', '')
                            domain = urlparse(url).netloc.lower()
                            if any(bad in domain for bad in trash_domains):
                                continue
                            candidates.append((url, domain, doc))
                        
                        priority_urls = []
                        other_urls = []
                        for url, domain, doc in candidates:
                            if any(p in domain for p in priority_domains):
                                if 'wikipedia' in domain and not domain.startswith('ru.wikipedia'):
                                    continue
                                priority_urls.append((url, doc))
                            else:
                                other_urls.append((url, doc))
                        
                        urls_to_fetch = []
                        for url, _ in priority_urls[:2]:
                            urls_to_fetch.append(url)
                        for url, _ in other_urls:
                            if len(urls_to_fetch) >= 3:
                                break
                            if url not in urls_to_fetch:
                                urls_to_fetch.append(url)
                        
                        if not urls_to_fetch:
                            continue
                        
                        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
                        fetch_tasks = [detective.fetch_page_content(url, query) for url in urls_to_fetch]
                        pages = await asyncio.gather(*fetch_tasks, return_exceptions=True)
                        valid_pages = [p for p in pages if isinstance(p, dict) and p.get('success')]
                        
                        for page in valid_pages:
                            all_docs.append({
                                'url': page['url'],
                                'title': page.get('title', ''),
                                'content': page.get('content', ''),
                            })
                    
                    if not all_docs:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Ç–µ–º—ã {topic}")
                        if graph and hasattr(graph, 'increment_failed_attempts'):
                            await graph.increment_failed_attempts(topic)
                        continue
                    
                    # –ê–Ω–∞–ª–∏–∑ (—Å —Ñ–ª–∞–≥–æ–º is_discovery)
                    analysis = await analyst.analyze(all_docs, query=topic, is_discovery=True)
                    key_points = analysis.get('key_points', [])
                    confidence = analysis.get('confidence', 0.0)
                    
                    # –ï—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ—Ç, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –Ω–µ—É–¥–∞—á
                    if len(key_points) == 0:
                        if graph and hasattr(graph, 'increment_failed_attempts'):
                            await graph.increment_failed_attempts(topic)
                        logger.info(f"‚ö†Ô∏è –¢–µ–º–∞ '{topic}' –Ω–µ –¥–∞–ª–∞ —Ñ–∞–∫—Ç–æ–≤, —Å—á—ë—Ç—á–∏–∫ –Ω–µ—É–¥–∞—á —É–≤–µ–ª–∏—á–µ–Ω")
                        continue
                    
                    # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–º–∏—Ç–µ—Ç–æ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
                    #if key_points and committee:
                    #   committee_result = await committee.evaluate({
                    #       'url': all_docs[0].get('url', ''),
                    #       'title': topic,
                    #       'content': key_points[0],
                    #        'snippet': key_points[0]
                    #    })
                    #    if not committee_result.get('approved', False):
                    #        logger.info(f"–¢–µ–º–∞ {topic} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –∫–æ–º–∏—Ç–µ—Ç–æ–º")
                    #        continue
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è
                    await self._store_knowledge(topic, {
                        'summary': ' '.join(key_points[:2]),
                        'key_points': key_points,
                        'confidence': confidence,
                        'source': 'discovery_cycle'
                    })
                    
                    results['research_completed'].append({
                        'topic': topic,
                        'pages': len(all_docs),
                        'key_points': len(key_points),
                        'confidence': confidence
                    })
                    
                    logger.info(f"‚úÖ –¢–µ–º–∞ '{topic}' –∏–∑—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (—Ñ–∞–∫—Ç–æ–≤: {len(key_points)})")
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–º—ã {topic}: {e}"
                    logger.error(error_msg)
                    results['errors'].append(error_msg)
            
            success = len(results['research_completed']) > 0
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ DiscoveryCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results
    
    async def _discover_potential_topics(self) -> List[Tuple[str, float]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–æ–≤—ã–µ —Ç–µ–º—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
        - –°–ª–∞–±—ã–µ —Ç–µ–º—ã –∏–∑ –≥—Ä–∞—Ñ–∞ (–º–∞–ª–æ —Å–≤—è–∑–µ–π)
        - –°—Ç–∞—Ä—ã–µ —Ç–µ–º—ã –∏–∑ –≥—Ä–∞—Ñ–∞ (–¥–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å)
        - –ò–∑ Engram —Ç–µ–º—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        """
        topics = []
        graph = self.services.get('graph_db')
        engram = self.services.get('engram')
        
        # 1. –°–ª–∞–±—ã–µ —Ç–µ–º—ã –∏–∑ –≥—Ä–∞—Ñ–∞ (–º–∞–ª–æ —Å–≤—è–∑–µ–π)
        if graph and hasattr(graph, 'get_weak_topics'):
            try:
                weak = await graph.get_weak_topics(limit=5)
                for t in weak:
                    topics.append((t, 0.8))
                    logger.debug(f"   –°–ª–∞–±–∞—è —Ç–µ–º–∞ –∏–∑ –≥—Ä–∞—Ñ–∞: {t}")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ª–∞–±—ã–µ —Ç–µ–º—ã –∏–∑ –≥—Ä–∞—Ñ–∞: {e}")
        
        # 2. –°—Ç–∞—Ä—ã–µ —Ç–µ–º—ã –∏–∑ –≥—Ä–∞—Ñ–∞ (–¥–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å)
        if graph and hasattr(graph, 'get_old_topics'):
            try:
                old = await graph.get_old_topics(days_threshold=7, limit=5)
                for t in old:
                    topics.append((t, 0.6))
                    logger.debug(f"   –°—Ç–∞—Ä–∞—è —Ç–µ–º–∞ –∏–∑ –≥—Ä–∞—Ñ–∞: {t}")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Ç–µ–º—ã –∏–∑ –≥—Ä–∞—Ñ–∞: {e}")
        
        # 3. –ò–∑ Engram —Ç–µ–º—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (confidence < 0.6)
        if engram and hasattr(engram, 'get_all_keys') and hasattr(engram, 'cache'):
            try:
                keys = await engram.get_all_keys()
                for key in keys:
                    record = engram.cache.get(key)
                    if record:
                        conf = record.get('metadata', {}).get('confidence', 1.0)
                        if conf < 0.6:
                            topics.append((key, 0.5))
                            logger.debug(f"   –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ Engram: {key} (conf={conf})")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–º—ã –∏–∑ Engram: {e}")
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        if not topics:
            return []
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        unique = {}
        for t, p in topics:
            if t not in unique or p > unique[t]:
                unique[t] = p
        sorted_topics = sorted(unique.items(), key=lambda x: x[1], reverse=True)
        return sorted_topics
    
    async def _store_knowledge(self, topic: str, knowledge: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞–Ω–∏—è –≤–æ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        tasks = []
        engram = self.services.get('engram')
        chroma = self.services.get('chroma_db')
        graph = self.services.get('graph_db')
        
        if engram:
            tasks.append(engram.store(
                key=topic,
                content=knowledge.get('summary', ''),
                metadata={
                    'topic': topic,
                    'confidence': knowledge.get('confidence', 0.5),
                    'timestamp': datetime.now().isoformat(),
                    'source': 'discovery_cycle',
                    'key_points': knowledge.get('key_points', [])[:5]
                },
                confidence=knowledge.get('confidence', 0.5)
            ))
        
        if chroma and knowledge.get('key_points'):
            for i, point in enumerate(knowledge['key_points'][:5]):
                if len(point) > 50:
                    tasks.append(chroma.add_document(
                        text=point,
                        metadata={
                            'topic': topic,
                            'type': 'key_point',
                            'source': 'discovery_cycle',
                            'confidence': knowledge.get('confidence', 0.5),
                            'index': i
                        }
                    ))
        
        if graph and hasattr(graph, 'add_knowledge_chunk'):
            tasks.append(graph.add_knowledge_chunk(
                topic=topic,
                chunk={
                    'summary': knowledge.get('summary', '')[:500],
                    'key_points': knowledge.get('key_points', [])[:5],
                    'confidence': knowledge.get('confidence', 0.5)
                },
                relations=[]
            ))
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.debug(f"–ó–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ '{topic}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {len(tasks)} —Ö—Ä–∞–Ω–∏–ª–∏—â")


class DeepeningCycle(LearningCycle):
    """
    –¶–∏–∫–ª —É–≥–ª—É–±–ª–µ–Ω–∏—è –≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–º—ã.
    –í—ã–±–∏—Ä–∞–µ—Ç —Ç–µ–º—É —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã,
    –∏—â–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    """
    
    def __init__(self, services: Dict[str, Any], config: Dict = None):
        super().__init__(CycleType.DEEPENING, services)
        self.config = config or {}
        self.depth = self.config.get('depth', 2)  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π —É–≥–ª—É–±–ª–µ–Ω–∏—è
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ DEEPENING")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'deepened_topics': [],
            'errors': []
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â
            existing_topics = await self._get_existing_topics()
            if not existing_topics:
                logger.warning("–ù–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è")
                self._update_stats(False, time.time() - start_time)
                results['success'] = False
                return results
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–µ–º—É –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è (—Å–ª—É—á–∞–π–Ω—É—é –∏–∑ —Ç–æ–ø-10)
            import random
            topic = random.choice(existing_topics[:10])
            logger.info(f"–í—ã–±—Ä–∞–Ω–∞ —Ç–µ–º–∞ –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è: {topic}")
            
            interviewer = self.services.get('interviewer')
            detective = self.services.get('detective')
            analyst = self.services.get('analyst')
            committee = self.services.get('committee')
            
            all_key_points = []
            current_depth = 0
            while current_depth < self.depth:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–≥–ª—É–±–ª—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
                if current_depth == 0:
                    questions = [f"–ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ {topic}"]
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
                    questions = await interviewer.generate_deepening_questions(
                        knowledge_chunks=[{'text': ' '.join(all_key_points[-5:])}] if all_key_points else None,
                        current_depth=current_depth,
                        max_questions=3
                    )
                
                if not questions:
                    questions = [f"–ö–∞–∫–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã {topic}?"]
                
                results.setdefault('deepened_topics', []).append({
                    'topic': topic,
                    'depth_level': current_depth,
                    'questions': questions
                })
                
                # –ò—Å—Å–ª–µ–¥—É–µ–º
                investigation = await detective.investigate_topic_advanced(topic, questions)
                if not investigation.get('success'):
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {investigation.get('error')}")
                
                chunks = investigation.get('content_chunks', [])
                if chunks:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∫–∞–∫ –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
                    analysis_query = questions[0] if questions else topic
                    analysis = await analyst.analyze(chunks, query=analysis_query, is_discovery=True)
                    key_points = analysis.get('key_points', [])
                    confidence = analysis.get('confidence', 0.5)
                    
                    if key_points:
                        approved_count = 0
                        for sample in key_points[:3]:
                            committee_result = await committee.evaluate({
                                'url': investigation.get('metadata', [{}])[0].get('url', ''),
                                'title': topic,
                                'content': sample,
                                'snippet': sample
                            })
                            if committee_result.get('approved', False):
                                approved_count += 1
                        if approved_count < 2:  # –º–µ–Ω—å—à–µ –¥–≤—É—Ö –æ–¥–æ–±—Ä–µ–Ω–æ ‚Äì –æ—Ç–∫–ª–æ–Ω—è–µ–º
                            logger.info(f"–¢–µ–º–∞ {topic} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –∫–æ–º–∏—Ç–µ—Ç–æ–º (–æ–¥–æ–±—Ä–µ–Ω–æ {approved_count}/3)")

                            logger.info(f"–ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–µ–º–µ {topic} (–≥–ª—É–±–∏–Ω–∞ {current_depth}) –æ—Ç–∫–ª–æ–Ω–µ–Ω—ã –∫–æ–º–∏—Ç–µ—Ç–æ–º")
                            continue
                        
                    all_key_points.extend(key_points)
                    logger.info(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(key_points)} –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ (–≤—Å–µ–≥–æ {len(all_key_points)})")
                current_depth += 1
            
            if all_key_points:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è
                await self._store_knowledge(topic, {
                    'summary': ' '.join(all_key_points[:2]),
                    'key_points': all_key_points,
                    'confidence': confidence if 'confidence' in locals() else 0.5,
                    'source': 'deepening_cycle',
                    'depth_achieved': self.depth
                })
                results['deepened_topics'][-1]['key_points_added'] = len(all_key_points)
                logger.info(f"‚úÖ –¢–µ–º–∞ '{topic}' —É–≥–ª—É–±–ª–µ–Ω–∞: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(all_key_points)} –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤")
            
            success = len(all_key_points) > 0
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ DeepeningCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results
    
    async def _get_existing_topics(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â."""
        topics = set()
        graph = self.services.get('graph_db')
        if graph and hasattr(graph, 'get_all_topics'):
            try:
                topics.update(await graph.get_all_topics())
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–º –∏–∑ –≥—Ä–∞—Ñ–∞: {e}")
        
        engram = self.services.get('engram')
        if engram and hasattr(engram, 'get_all_keys'):
            try:
                topics.update(await engram.get_all_keys())
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–º –∏–∑ Engram: {e}")
        
        return list(topics)
    
    async def _store_knowledge(self, topic: str, knowledge: Dict):
        """–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ DiscoveryCycle."""
        tasks = []
        engram = self.services.get('engram')
        chroma = self.services.get('chroma_db')
        graph = self.services.get('graph_db')
        
        if engram:
            tasks.append(engram.store(
                key=topic,
                content=knowledge.get('summary', ''),
                metadata={
                    'topic': topic,
                    'confidence': knowledge.get('confidence', 0.5),
                    'timestamp': datetime.now().isoformat(),
                    'source': 'deepening_cycle',
                    'key_points': knowledge.get('key_points', [])[:5],
                    'depth': knowledge.get('depth_achieved', 1)
                },
                confidence=knowledge.get('confidence', 0.5)
            ))
        
        if chroma and knowledge.get('key_points'):
            for i, point in enumerate(knowledge['key_points'][:5]):
                if len(point) > 50:
                    tasks.append(chroma.add_document(
                        text=point,
                        metadata={
                            'topic': topic,
                            'type': 'key_point',
                            'source': 'deepening_cycle',
                            'confidence': knowledge.get('confidence', 0.5),
                            'index': i
                        }
                    ))
        
        if graph and hasattr(graph, 'add_knowledge_chunk'):
            tasks.append(graph.add_knowledge_chunk(
                topic=topic,
                chunk={
                    'summary': knowledge.get('summary', '')[:500],
                    'key_points': knowledge.get('key_points', [])[:5],
                    'confidence': knowledge.get('confidence', 0.5)
                },
                relations=[]
            ))
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)


class ExpansionCycle(LearningCycle):
    """
    –¶–∏–∫–ª —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏.
    –í—ã–±–∏—Ä–∞–µ—Ç –¥–≤–µ —Ç–µ–º—ã, –∏—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤—è–∑–∏ –≤ –≥—Ä–∞—Ñ.
    """
    
    def __init__(self, services: Dict[str, Any], config: Dict = None):
        super().__init__(CycleType.EXPANSION, services)
        self.config = config or {}
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ EXPANSION")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'connections': [],
            'errors': []
        }
        
        try:
            topics = await self._get_existing_topics()
            if len(topics) < 2:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–º –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–≤—è–∑–µ–π")
                self._update_stats(False, time.time() - start_time)
                results['success'] = False
                return results
            
            import random
            topic1 = random.choice(topics)
            topic2 = random.choice([t for t in topics if t != topic1])
            
            logger.info(f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É '{topic1}' –∏ '{topic2}'")
            
            detective = self.services.get('detective')
            analyst = self.services.get('analyst')
            graph = self.services.get('graph_db')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –æ —Å–≤—è–∑–∏
            query = f"–°–≤—è–∑—å –º–µ–∂–¥—É {topic1} –∏ {topic2}"
            investigation = await detective.investigate_topic_advanced(query, [query])
            if not investigation.get('success'):
                raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–≤—è–∑—å")
            
            chunks = investigation.get('content_chunks', [])
            if chunks:
                analysis = await analyst.analyze(chunks, query=query, is_discovery=True)
                key_points = analysis.get('key_points', [])
                confidence = analysis.get('confidence', 0.5)
                
                if key_points and graph and hasattr(graph, 'add_relation'):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤—è–∑—å –≤ –≥—Ä–∞—Ñ –∫–∞–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ
                    await graph.add_relation(topic1, topic2, relation_type="—Å–≤—è–∑–∞–Ω–æ_—Å", weight=confidence)
                    # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏–µ
                    if hasattr(graph, 'add_knowledge_chunk'):
                        await graph.add_knowledge_chunk(
                            topic=f"{topic1}_{topic2}",
                            chunk={
                                'summary': ' '.join(key_points[:2]),
                                'key_points': key_points[:5],
                                'confidence': confidence
                            },
                            relations=[(topic1, topic2, "—Å–≤—è–∑–∞–Ω–æ_—Å")]
                        )
                    
                    results['connections'].append({
                        'topic1': topic1,
                        'topic2': topic2,
                        'key_points': len(key_points),
                        'confidence': confidence
                    })
                    logger.info(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Å–≤—è–∑—å –º–µ–∂–¥—É '{topic1}' –∏ '{topic2}' (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {confidence:.2f})")
            
            success = len(results['connections']) > 0
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ ExpansionCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results
    
    async def _get_existing_topics(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–º –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â."""
        topics = set()
        graph = self.services.get('graph_db')
        if graph and hasattr(graph, 'get_all_topics'):
            try:
                topics.update(await graph.get_all_topics())
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–º –∏–∑ –≥—Ä–∞—Ñ–∞: {e}")
        
        engram = self.services.get('engram')
        if engram and hasattr(engram, 'get_all_keys'):
            try:
                topics.update(await engram.get_all_keys())
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–º –∏–∑ Engram: {e}")
        
        return list(topics)


class MetaAnalysisCycle(LearningCycle):
    """
    –¶–∏–∫–ª –º–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑–∞ —Å–∏—Å—Ç–µ–º—ã.
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–æ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å,
    –≤—ã–¥–∞—ë—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    
    def __init__(self, services: Dict[str, Any], config: Dict = None):
        super().__init__(CycleType.META_ANALYSIS, services)
        self.config = config or {}
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ META_ANALYSIS")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'analysis': {},
            'adjustments': [],
            'errors': []
        }
        
        try:
            # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ —Å–æ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
            stats = {}
            for name, service in self.services.items():
                if service and hasattr(service, 'get_metrics'):
                    try:
                        stats[name] = await service.get_metrics()
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç {name}: {e}")
            
            # –ê–Ω–∞–ª–∏–∑ (–ø—Ä–∏–º–µ—Ä)
            analysis = {
                'total_knowledge_chunks': stats.get('chroma_db', {}).get('collection_size', 0),
                'graph_nodes': stats.get('graph_db', {}).get('nodes', 0),
                'graph_edges': stats.get('graph_db', {}).get('edges', 0),
                'engram_entries': stats.get('engram', {}).get('total_records', 0),
                'detective_requests': stats.get('detective', {}).get('requests', 0),
                'analyst_confidence_avg': stats.get('analyst', {}).get('avg_confidence', 0),
            }
            
            # –í—ã—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            adjustments = []
            if analysis['total_knowledge_chunks'] < 100:
                adjustments.append("–£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ü–∏–∫–ª–∞ DISCOVERY")
            if analysis['graph_edges'] < analysis['graph_nodes'] * 0.5:
                adjustments.append("–£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ü–∏–∫–ª–∞ EXPANSION")
            if analysis['analyst_confidence_avg'] < 0.4:
                adjustments.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ, —É–ª—É—á—à–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ–º–∏—Ç–µ—Ç–∞")
            
            results['analysis'] = analysis
            results['adjustments'] = adjustments
            
            logger.info(f"üìä –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑: —É–∑–ª–æ–≤={analysis['graph_nodes']}, —Å–≤—è–∑–µ–π={analysis['graph_edges']}, Engram={analysis['engram_entries']}")
            if adjustments:
                logger.info(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {', '.join(adjustments)}")
            
            success = True
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ MetaAnalysisCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results


class MaintenanceCycle(LearningCycle):
    """
    –¶–∏–∫–ª –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã.
    –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞, –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤.
    """
    
    def __init__(self, services: Dict[str, Any], config: Dict = None):
        super().__init__(CycleType.MAINTENANCE, services)
        self.config = config or {}
    
    async def execute(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ MAINTENANCE")
        
        results = {
            'cycle_type': self.cycle_type.value,
            'start_time': datetime.now().isoformat(),
            'operations': {},
            'errors': []
        }
        
        try:
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥–µ—Ç–µ–∫—Ç–∏–≤–∞
            if self.services.get('detective'):
                try:
                    await self.services['detective'].clear_cache()
                    results['operations']['detective_cache_cleared'] = True
                except Exception as e:
                    results['errors'].append(f"detective.clear_cache: {e}")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞
            if self.services.get('graph_db') and hasattr(self.services['graph_db'], 'optimize'):
                try:
                    opt_result = await self.services['graph_db'].optimize()
                    results['operations']['graph_optimized'] = opt_result
                except Exception as e:
                    results['errors'].append(f"graph.optimize: {e}")
            
            # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ Engram (–µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–æ–¥ cleanup)
            if self.services.get('engram') and hasattr(self.services['engram'], 'cleanup'):
                try:
                    await self.services['engram'].cleanup()
                    results['operations']['engram_cleaned'] = True
                except Exception as e:
                    results['errors'].append(f"engram.cleanup: {e}")
            
            # –ü–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ Chroma (–µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–æ–¥ optimize)
            if self.services.get('chroma_db') and hasattr(self.services['chroma_db'], 'optimize'):
                try:
                    await self.services['chroma_db'].optimize()
                    results['operations']['chroma_optimized'] = True
                except Exception as e:
                    results['errors'].append(f"chroma.optimize: {e}")
            
            success = True
            duration = time.time() - start_time
            self._update_stats(success, duration)
            
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = duration
            results['success'] = success
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ MaintenanceCycle: {e}", exc_info=True)
            results['errors'].append(str(e))
            results['success'] = False
            self._update_stats(False, time.time() - start_time)
            return results


class LearningCycleCoordinator:
    """
    –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.
    –£–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º, –≤—ã–±–∏—Ä–∞–µ—Ç —Ü–∏–∫–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤,
    –∑–∞–ø—É—Å–∫–∞–µ—Ç –∏—Ö –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.
    """
    
    def __init__(self, services: Dict[str, Any], config: Optional[Dict] = None):
        self.services = services
        self.config = config or {}
        
        # –§–ª–∞–≥ –≤–∫–ª—é—á–µ–Ω–∏—è/–≤—ã–∫–ª—é—á–µ–Ω–∏—è
        self.enabled = self.config.get('enabled', True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤ —Å –∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        cycle_configs = self.config.get('cycles', {})
        self.cycles = {
            CycleType.DISCOVERY: DiscoveryCycle(services, cycle_configs.get('discovery', {})),
            CycleType.DEEPENING: DeepeningCycle(services, cycle_configs.get('deepening', {})),
            CycleType.EXPANSION: ExpansionCycle(services, cycle_configs.get('expansion', {})),
            CycleType.META_ANALYSIS: MetaAnalysisCycle(services, cycle_configs.get('meta', {})),
            CycleType.MAINTENANCE: MaintenanceCycle(services, cycle_configs.get('maintenance', {})),
        }
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ü–∏–∫–ª–æ–≤ (—Å—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        self.cycle_priorities = self.config.get('priorities', {
            CycleType.DISCOVERY.value: 0.40,
            CycleType.DEEPENING.value: 0.30,
            CycleType.EXPANSION.value: 0.20,
            CycleType.META_ANALYSIS.value: 0.07,
            CycleType.MAINTENANCE.value: 0.03,
        })
        
        # –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Ü–∏–∫–ª–∞)
        self.schedule_intervals = {
            CycleType.DISCOVERY: timedelta(seconds=self.config.get('intervals', {}).get('discovery', 3600)),
            CycleType.DEEPENING: timedelta(seconds=self.config.get('intervals', {}).get('deepening', 7200)),
            CycleType.EXPANSION: timedelta(seconds=self.config.get('intervals', {}).get('expansion', 14400)),
            CycleType.META_ANALYSIS: timedelta(seconds=self.config.get('intervals', {}).get('meta_analysis', 86400)),
            CycleType.MAINTENANCE: timedelta(seconds=self.config.get('intervals', {}).get('maintenance', 43200)),
        }
        
        self.last_execution = {ctype: None for ctype in CycleType}
        self.execution_history = []
        self.is_running = False
        self._task: Optional[asyncio.Task] = None
        
        logger.info("LearningCycleCoordinator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. enabled=%s", self.enabled)
    
    async def start(self):
        logger.info("üî• LearningCycleCoordinator.start() –≤—ã–∑–≤–∞–Ω")
        logger.info(f"   enabled={self.enabled}, is_running={self.is_running}")
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏."""
        if not self.enabled:
            logger.info("–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return
        
        if self.is_running:
            logger.warning("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
            return
        
        self.is_running = True
        logger.info("–ó–∞–ø—É—Å–∫ LearningCycleCoordinator")
        
        while self.is_running:
            try:
                cycle_type = self._select_cycle_to_run()
                if cycle_type:
                    logger.info(f"–í—ã–±—Ä–∞–Ω —Ü–∏–∫–ª: {cycle_type.value}")
                    cycle = self.cycles[cycle_type]
                    result = await cycle.execute()
                    
                    self.last_execution[cycle_type] = datetime.now()
                    self.execution_history.append({
                        'timestamp': datetime.now().isoformat(),
                        'cycle': cycle_type.value,
                        'result': result,
                        'duration': result.get('duration_seconds', 0)
                    })
                    
                    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    self._adapt_priorities(cycle_type, result)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–º –∏—Å—Ç–æ—Ä–∏—é
                    if len(self.execution_history) > 1000:
                        self.execution_history = self.execution_history[-500:]
                
                # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å)
                await asyncio.sleep(self.config.get('check_interval', 60))
                
            except asyncio.CancelledError:
                logger.info("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É")
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞: {e}", exc_info=True)
                await asyncio.sleep(300)  # –ø—Ä–∏ –æ—à–∏–±–∫–µ –∂–¥—ë–º 5 –º–∏–Ω—É—Ç
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä."""
        self.is_running = False
        if self._task and not self._task.done():
            self._task.cancel()
        logger.info("LearningCycleCoordinator –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    async def run_cycle(self, cycle_type: CycleType) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ü–∏–∫–ª –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ-–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)."""
        if cycle_type not in self.cycles:
            return {'error': f'–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ü–∏–∫–ª–∞: {cycle_type}'}
        
        logger.info(f"–ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ {cycle_type.value} –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é")
        cycle = self.cycles[cycle_type]
        result = await cycle.execute()
        self.last_execution[cycle_type] = datetime.now()
        return {
            'cycle': cycle_type.value,
            'result': result,
            'timestamp': datetime.now().isoformat()
        }
    
    def _select_cycle_to_run(self) -> Optional[CycleType]:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ü–∏–∫–ª –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞."""
        now = datetime.now()
        candidates = []
        
        for cycle_type, interval in self.schedule_intervals.items():
            last = self.last_execution[cycle_type]
            if last is None or (now - last) >= interval:
                # –¶–∏–∫–ª –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É
                priority = self.cycle_priorities.get(cycle_type.value, 0)
                candidates.append((cycle_type, priority))
        
        if not candidates:
            return None
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        total = sum(p for _, p in candidates)
        if total == 0:
            return None
        
        # –í—ã–±–æ—Ä —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Å —É—á—ë—Ç–æ–º –≤–µ—Å–æ–≤
        r = random.random()
        cumulative = 0.0
        for cycle, prob in candidates:
            cumulative += prob / total
            if r <= cumulative:
                return cycle
        
        return candidates[-1][0]  # –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
    
    def _adapt_priorities(self, cycle_type: CycleType, result: Dict[str, Any]):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."""
        success = result.get('success', False)
        if success:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
            self.cycle_priorities[cycle_type.value] *= 1.1
        else:
            # –£–º–µ–Ω—å—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ
            self.cycle_priorities[cycle_type.value] *= 0.9
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (—á—Ç–æ–±—ã —Å—É–º–º–∞ –Ω–µ —É—Ö–æ–¥–∏–ª–∞ –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—å)
        total = sum(self.cycle_priorities.values())
        for ctype in self.cycle_priorities:
            self.cycle_priorities[ctype] /= total
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ –∏ –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤."""
        cycle_stats = {ctype.value: self.cycles[ctype].get_stats() for ctype in self.cycles}
        return {
            'enabled': self.enabled,
            'is_running': self.is_running,
            'cycle_priorities': self.cycle_priorities.copy(),
            'last_executions': {k.value: v.isoformat() if v else None for k, v in self.last_execution.items()},
            'cycle_stats': cycle_stats,
            'total_executions': len(self.execution_history)
        }


# –î–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_learning_coordinator: Optional[LearningCycleCoordinator] = None

def get_learning_coordinator(services: Optional[Dict[str, Any]] = None, config: Optional[Dict] = None) -> LearningCycleCoordinator:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞—ë—Ç –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)."""
    global _learning_coordinator
    if _learning_coordinator is None:
        if services is None:
            raise RuntimeError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ")
        _learning_coordinator = LearningCycleCoordinator(services, config)
    return _learning_coordinator