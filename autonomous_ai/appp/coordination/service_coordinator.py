"""
üéØ –°–ï–†–í–ò–°–ù–´–ô –ö–û–û–†–î–ò–ù–ê–¢–û–† - –ú–æ–∑–≥ —Å–∏—Å—Ç–µ–º—ã
–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, —É–ø—Ä–∞–≤–ª—è–µ—Ç –æ—á–µ—Ä–µ–¥—è–º–∏, –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É
"""
import os
import json
import asyncio
import logging
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Set
from enum import Enum
import heapq
import uuid
from urllib.parse import urlparse
import re
import time

from appp.core.config import Config
from appp.core.logging import logger
from appp.utils.response_templates import format_rich_response

class TaskPriority(Enum):
    CRITICAL = 0
    HIGH = 1
    MEDIUM = 2
    LOW = 3


class TaskStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    TIMEOUT = "timeout"


class TaskType(Enum):
    SIMPLE_QUESTION = "simple_question"
    DEEP_RESEARCH = "deep_research"
    SELF_LEARNING = "self_learning"
    TOPIC_EXPLORATION = "topic_exploration"
    KNOWLEDGE_UPDATE = "knowledge_update"
    SYSTEM_MAINTENANCE = "system_maintenance"
    CACHE_CLEANUP = "cache_cleanup"
    GRAPH_OPTIMIZATION = "graph_optimization"


class ServiceCoordinator:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ —Å–∏—Å—Ç–µ–º—ã.
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á–∞–º–∏, –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å.
    """
    
    def __init__(self, config: Config, **services):
        self.config = config.coordinator
        self.services = services  # detective, committee, analyst, interviewer, chroma_db, graph_db, engram, embedder
        
        # –û—á–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        self.priority_queue = []
        self.task_queue = asyncio.Queue(maxsize=self.config.max_queue_size)
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–¥–∞—á
        self.tasks: Dict[str, Dict] = {}
        self.task_results: Dict[str, Any] = {}
        
        # –í–æ—Ä–∫–µ—Ä—ã
        self.worker_tasks: List[asyncio.Task] = []
        self.num_workers = self.config.num_workers
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.metrics = {
            'tasks_processed': 0,
            'tasks_failed': 0,
            'avg_processing_time': 0,
            'queue_wait_time': 0,
            'worker_utilization': 0,
            'memory_usage_mb': 0,
            'errors': {}
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á
        self.task_stats = {task_type.value: {'processed': 0, 'failed': 0, 'avg_time': 0} 
                          for task_type in TaskType}
        
        # –°–µ–º–∞—Ñ–æ—Ä—ã
        self.processing_semaphore = asyncio.Semaphore(self.config.max_concurrent_tasks)
        
        # –ü–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ —Å–æ–±—ã—Ç–∏—è
        self.event_subscribers: Dict[str, List[Callable]] = {
            'task_started': [],
            'task_completed': [],
            'task_failed': [],
            'system_alert': []
        }
        
        # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
        self.scheduled_tasks = []
        
        self.is_running = False
        self.is_shutting_down = False
        
        logger.info("üéØ ServiceCoordinator —Å–æ–∑–¥–∞–Ω")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞"""
        logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ServiceCoordinator...")
        
        try:
            await self._start_workers()
            await self._start_monitoring()
            await self._schedule_background_tasks()
            await self._check_services_health()
            
            self.is_running = True
            logger.info("‚úÖ ServiceCoordinator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞: {e}")
            return False
    
    async def _start_workers(self):
        for i in range(self.num_workers):
            worker_task = asyncio.create_task(
                self._worker_loop(f"worker-{i+1}"),
                name=f"coordinator_worker_{i}"
            )
            self.worker_tasks.append(worker_task)
    
    async def _worker_loop(self, worker_name: str):
        logger.debug(f"–í–æ—Ä–∫–µ—Ä {worker_name} –∑–∞–ø—É—â–µ–Ω")
        while not self.is_shutting_down:
            try:
                async with self.processing_semaphore:
                    task_data = await self._get_next_task()
                    if task_data is None:
                        await asyncio.sleep(0.1)
                        continue

                    task_id = task_data['task_id']
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ª–∏ –∑–∞–¥–∞—á–∞
                    if self.tasks.get(task_id, {}).get('status') == TaskStatus.CANCELLED.value:
                        logger.info(f"‚è≠Ô∏è –ó–∞–¥–∞—á–∞ {task_id} –æ—Ç–º–µ–Ω–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue
                    
                    result = await self._process_task(task_data)
                    self.task_results[task_id] = result
                    
                    if result.get('success', False):
                        await self._update_task_status(task_id, TaskStatus.COMPLETED)
                    else:
                        await self._update_task_status(task_id, TaskStatus.FAILED)
                        logger.error(f"‚ùå {worker_name} –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª –∑–∞–¥–∞—á—É {task_id}: {result.get('error')}")
                    
                    await self._emit_event('task_completed', {
                        'task_id': task_id,
                        'worker': worker_name,
                        'result': result,
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    self._update_metrics(task_data, result)
            
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –≤–æ—Ä–∫–µ—Ä–µ {worker_name}: {e}")
                self.metrics['errors'][worker_name] = self.metrics['errors'].get(worker_name, 0) + 1
                await asyncio.sleep(1)
    
    async def _get_next_task(self) -> Optional[Dict]:
        try:
            if self.priority_queue:
                _, task_id = heapq.heappop(self.priority_queue)
                if task_id in self.tasks:
                    return self.tasks[task_id]
            if not self.task_queue.empty():
                return await self.task_queue.get()
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏: {e}")
            return None
    
    async def _process_task(self, task_data: Dict) -> Dict:
        task_id = task_data['task_id']
        task_type = task_data['type']
        start_time = time.time()
        
        try:
            task_timeout = task_data.get('timeout', self.config.task_timeout)
            
            if task_type == TaskType.SIMPLE_QUESTION.value:
                result = await self._process_simple_question(task_data)
            elif task_type == TaskType.DEEP_RESEARCH.value:
                result = await self._process_deep_research(task_data)
            elif task_type == TaskType.SELF_LEARNING.value:
                result = await self._process_self_learning(task_data)
            elif task_type == TaskType.TOPIC_EXPLORATION.value:
                result = await self._process_topic_exploration(task_data)
            elif task_type == TaskType.KNOWLEDGE_UPDATE.value:
                result = await self._process_knowledge_update(task_data)
            elif task_type == TaskType.SYSTEM_MAINTENANCE.value:
                result = await self._process_system_maintenance(task_data)
            elif task_type == TaskType.CACHE_CLEANUP.value:
                result = await self._process_cache_cleanup(task_data)
            elif task_type == TaskType.GRAPH_OPTIMIZATION.value:
                result = await self._process_graph_optimization(task_data)
            else:
                result = {'success': False, 'error': f'Unknown task type: {task_type}'}
            
            processing_time = time.time() - start_time
            result['processing_time'] = processing_time
            result['task_id'] = task_id
            
            return result
        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –∑–∞–¥–∞—á–∏ {task_id}")
            return {'success': False, 'error': f'Task timeout after {task_timeout} seconds', 'task_id': task_id}
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ {task_id}: {e}")
            return {'success': False, 'error': str(e), 'task_id': task_id, 'traceback': traceback.format_exc()}



    async def _process_self_learning(self, task_data: Dict) -> Dict:
        """
        –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ: –≤—ã–±–∏—Ä–∞–µ—Ç —Ç–µ–º—É –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π –∏–ª–∏ —Å–ª—É—á–∞–π–Ω—É—é,
        –ø—Ä–æ–≤–æ–¥–∏—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è.
        """
        logger.info("üß† –ó–∞–ø—É—Å–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è")
        start_time = time.time()

        # 1. –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–∏—Å–∞–º
        engram = self.services.get('engram')
        graph = self.services.get('graph_db')
        chroma = self.services.get('chroma_db')
        detective = self.services.get('detective')
        analyst = self.services.get('analyst')
        interviewer = self.services.get('interviewer')

        if not detective or not analyst or not interviewer:
            return {'success': False, 'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã'}

        # 2. –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–º—É –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
        topic = None
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ Engram –∑–∞–ø–∏—Å—å —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –∏–ª–∏ –¥–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–≤—à—É—é—Å—è
        if engram:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª—é—á–µ–π (–º–µ—Ç–æ–¥ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –Ω–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ get_all_keys() –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π
                # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: —Å–ª—É—á–∞–π–Ω–∞—è —Ç–µ–º–∞
                topics_pool = [
                    "–ö–≤–∞–Ω—Ç–æ–≤–∞—è –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å",
                    "–¢–µ–æ—Ä–µ–º–∞ –ì—ë–¥–µ–ª—è –æ –Ω–µ–ø–æ–ª–Ω–æ—Ç–µ",
                    "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
                    "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
                    "–ê–ª–≥–æ—Ä–∏—Ç–º—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏",
                    "–ò—Å—Ç–æ—Ä–∏—è –î—Ä–µ–≤–Ω–µ–≥–æ –†–∏–º–∞",
                    "–§–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑",
                    "–¢–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫–∞",
                    "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏–∑–∏—Å—ã",
                    "–§–∏–ª–æ—Å–æ—Ñ–∏—è –ö–∞–Ω—Ç–∞"
                ]
                import random
                topic = random.choice(topics_pool)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–º—É –∏–∑ Engram: {e}")
        
        if not topic:
            # –ë–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω—É—é —Ç–µ–º—É –∏–∑ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            import random
            topics_pool = [
                "–ö–≤–∞–Ω—Ç–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞", "–¢–µ–æ—Ä–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
                "–ë–∏–æ–ª–æ–≥–∏—è –∫–ª–µ—Ç–∫–∏", "–ò—Å—Ç–æ—Ä–∏—è –í–∏–∑–∞–Ω—Ç–∏–∏", "–†—É—Å—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ XIX –≤–µ–∫–∞",
                "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ë–ª–æ–∫—á–µ–π–Ω", "–ù–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"
            ]
            topic = random.choice(topics_pool)

        logger.info(f"üìö –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ: –≤—ã–±—Ä–∞–Ω–∞ —Ç–µ–º–∞ '{topic}'")

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        questions = await interviewer.generate_research_questions(
            topic, depth=2, num_questions=8
        )

        # 4. –ü—Ä–æ–≤–æ–¥–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–µ—Ç–µ–∫—Ç–∏–≤
        investigation = await detective.investigate_topic_advanced(topic, questions)
        if not investigation.get('success'):
            return {'success': False, 'error': '–û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç–µ–º—ã'}

        chunks = investigation.get('content_chunks', [])
        if not chunks:
            return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç'}

        # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        analysis = await analyst.analyze(chunks, query=topic)

        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è
        await self._store_knowledge(topic, analysis)

        processing_time = time.time() - start_time
        logger.info(f"‚úÖ –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ø–æ —Ç–µ–º–µ '{topic}' –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {processing_time:.2f} —Å–µ–∫")

        return {
            'success': True,
            'topic': topic,
            'pages_processed': investigation.get('pages_processed', 0),
            'chunks_analyzed': len(chunks),
            'key_points_count': len(analysis.get('key_points', [])),
            'confidence': analysis.get('confidence', 0),
            'processing_time': processing_time
        }


    async def _process_topic_exploration(self, task_data: Dict) -> Dict:
        """
        –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–º—ã —Å —Ü–∏–∫–ª–∞–º–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã, –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã,
        —É–≥–ª—É–±–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
        """
        topic = task_data.get('topic')
        max_cycles = task_data.get('max_cycles', 3)
        if not topic:
            return {'success': False, 'error': '–ù–µ —É–∫–∞–∑–∞–Ω–∞ —Ç–µ–º–∞'}

        logger.info(f"üåÄ –¶–∏–∫–ª –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–ª—è —Ç–µ–º—ã '{topic}', –º–∞–∫—Å. —Ü–∏–∫–ª–æ–≤: {max_cycles}")
        start_time = time.time()

        detective = self.services.get('detective')
        analyst = self.services.get('analyst')
        interviewer = self.services.get('interviewer')

        if not detective or not analyst or not interviewer:
            return {'success': False, 'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã'}

        all_chunks = []
        all_key_points = []          # ‚Üê —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–∫—Ç—ã –∑–∞ –≤—Å–µ —Ü–∏–∫–ª—ã
        cycle_results = []
        current_depth = 0
        current_questions = [f"–ß—Ç–æ —Ç–∞–∫–æ–µ {topic}?"]  # –Ω–∞—á–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å

        for cycle in range(max_cycles):
            logger.info(f"üîÑ –¶–∏–∫–ª {cycle+1}/{max_cycles}, –≥–ª—É–±–∏–Ω–∞ {current_depth}")

            # –ü–æ–∏—Å–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Ç–µ–∫—É—â–∏–º –≤–æ–ø—Ä–æ—Å–∞–º
            investigation = await detective.investigate_topic_advanced(
                topic, 
                questions=current_questions[:3]
            )
            if not investigation.get('success'):
                logger.warning(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ {cycle+1}: {investigation.get('error')}")
                break

            chunks = investigation.get('content_chunks', [])
            if not chunks:
                logger.warning(f"–ù–µ—Ç –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ —Ü–∏–∫–ª–µ {cycle+1}")
                break

            all_chunks.extend(chunks)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
            analysis = await analyst.analyze(chunks, query=topic)
            cycle_key_points = analysis.get('key_points', [])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            if cycle_key_points:
                all_key_points.extend(cycle_key_points)
                logger.info(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(cycle_key_points)} —Ñ–∞–∫—Ç–æ–≤ (–≤—Å–µ–≥–æ {len(all_key_points)})")
            else:
                logger.warning(f"   ‚ö†Ô∏è –ê–Ω–∞–ª–∏—Ç–∏–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∞–∫—Ç–æ–≤ –≤ —ç—Ç–æ–º —Ü–∏–∫–ª–µ")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ü–∏–∫–ª–∞
            cycle_results.append({
                'cycle': cycle + 1,
                'questions': current_questions,
                'pages': investigation.get('pages_processed', 0),
                'chunks': len(chunks),
                'key_points': cycle_key_points[:5],
                'confidence': analysis.get('confidence', 0)
            })

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–≥–ª—É–±–ª—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            deepening = await interviewer.generate_deepening_questions(
                knowledge_chunks=chunks,
                current_depth=current_depth,
                max_questions=3
            )

            if deepening:
                current_questions = deepening
                current_depth += 1
            else:
                break

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑
        if all_key_points:
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ –ø–µ—Ä–≤—ã–º 100 —Å–∏–º–≤–æ–ª–∞–º)
            seen = set()
            unique_points = []
            for point in all_key_points:
                sig = point[:100].lower()
                if sig not in seen:
                    seen.add(sig)
                    unique_points.append(point)
            all_key_points = unique_points

            # –ë–µ—Ä—ë–º —Ç–æ–ø-15 —Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞
            key_insights = all_key_points[:15]
            synthesis = "\n".join([f"‚Ä¢ {p}" for p in key_insights[:10]])  # –∫—Ä–∞—Ç–∫–∏–π —Å–∏–Ω—Ç–µ–∑
            confidence = min(0.9, 0.5 + 0.1 * len(all_key_points))  # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        else:
            # Fallback: –µ—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ—Ç, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –ª—É—á—à–∏—Ö —á–∞–Ω–∫–æ–≤
            logger.warning("‚ö†Ô∏è –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–∫–æ–ø–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback (–ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —á–∞–Ω–∫–æ–≤)")
            import re
            fallback_texts = []
            for chunk in all_chunks[:5]:
                text = chunk.get('text', '')
                if text:
                    sentences = re.split(r'(?<=[.!?])\s+', text)
                    for sent in sentences[:3]:
                        sent = sent.strip()
                        if 30 < len(sent) < 500 and not re.search(r'https?://|¬©|—Ñ–æ—Ç–æ|–∫—É–ø–∏—Ç—å', sent, re.I):
                            fallback_texts.append(sent)
            if fallback_texts:
                key_insights = fallback_texts[:10]
                synthesis = "\n".join([f"‚Ä¢ {p}" for p in key_insights[:5]])
                confidence = 0.3
            else:
                key_insights = ["–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã."]
                synthesis = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
                confidence = 0.0

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if all_key_points or fallback_texts:
            await self._store_knowledge(topic, {
                'summary': synthesis,
                'key_points': key_insights,
                'confidence': confidence,
                'cycles': len(cycle_results)
            })

        processing_time = time.time() - start_time

        return {
            'success': True,
            'topic': topic,
            'cycles_completed': len(cycle_results),
            'knowledge_chunks': len(all_chunks),
            'synthesis': synthesis,
            'key_insights': key_insights,
            'confidence': confidence,
            'cycle_details': cycle_results,
            'processing_time': processing_time
        }

    async def _process_system_maintenance(self, task_data: Dict) -> Dict:
        """
        –°–∏—Å—Ç–µ–º–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ: –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤, —Ä–æ—Ç–∞—Ü–∏—è, –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–∫–∞.
        """
        logger.info("üõ†Ô∏è –°–∏—Å—Ç–µ–º–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ")
        start_time = time.time()

        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        import shutil
        cache_dirs = [
            './data/cache/detective',
            './data/cache/embeddings',
        ]

        cleared = 0
        errors = []

        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):
                try:
                    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã —Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π
                    now = time.time()
                    for filename in os.listdir(cache_dir):
                        filepath = os.path.join(cache_dir, filename)
                        if os.path.isfile(filepath):
                            if os.path.getmtime(filepath) < now - 7 * 86400:
                                os.remove(filepath)
                                cleared += 1
                except Exception as e:
                    errors.append(f"{cache_dir}: {e}")

        # –†–æ—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤ (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ)
        log_file = self.config.get('log_file', './data/logs/autonomous_ai.log')
        max_size = self.config.get('max_log_size', 10 * 1024 * 1024)
        backup_count = self.config.get('backup_count', 5)

        if os.path.exists(log_file):
            try:
                size = os.path.getsize(log_file)
                if size > max_size:
                    # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Ä–æ—Ç–∞—Ü–∏—è
                    for i in range(backup_count - 1, 0, -1):
                        old = f"{log_file}.{i}"
                        new = f"{log_file}.{i+1}"
                        if os.path.exists(old):
                            os.rename(old, new)
                    os.rename(log_file, f"{log_file}.1")
                    open(log_file, 'w').close()
                    logger.info("‚ôªÔ∏è –õ–æ–≥-—Ñ–∞–π–ª —Ä–æ—Ç–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                errors.append(f"—Ä–æ—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤: {e}")

        processing_time = time.time() - start_time

        return {
            'success': True,
            'cleaned_files': cleared,
            'errors': errors,
            'processing_time': processing_time
        }


    async def _process_cache_cleanup(self, task_data: Dict) -> Dict:
        """
        –û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –∫—ç—à–µ–π.
        """
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π")
        start_time = time.time()

        detective = self.services.get('detective')
        embedder = self.services.get('embedder')
        searcher = self.services.get('internet_searcher')  # –µ—Å–ª–∏ –µ—Å—Ç—å

        results = {}

        if detective and hasattr(detective, 'clear_cache'):
            try:
                await detective.clear_cache()
                results['detective'] = '–∫—ç—à –æ—á–∏—â–µ–Ω'
            except Exception as e:
                results['detective'] = f'–æ—à–∏–±–∫–∞: {e}'

        if embedder and hasattr(embedder, 'clear_cache'):
            try:
                await embedder.clear_cache()
                results['embedder'] = '–∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—á–∏—â–µ–Ω'
            except Exception as e:
                results['embedder'] = f'–æ—à–∏–±–∫–∞: {e}'

        if searcher and hasattr(searcher, 'clear_cache'):
            try:
                await searcher.clear_cache()
                results['internet_searcher'] = '–∫—ç—à –ø–æ–∏—Å–∫–∞ –æ—á–∏—â–µ–Ω'
            except Exception as e:
                results['internet_searcher'] = f'–æ—à–∏–±–∫–∞: {e}'

        processing_time = time.time() - start_time

        return {
            'success': True,
            'results': results,
            'processing_time': processing_time
        }

    async def _process_graph_optimization(self, task_data: Dict) -> Dict:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
        """
        logger.info("üï∏Ô∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π")
        start_time = time.time()

        graph = self.services.get('graph_db')
        if not graph:
            return {'success': False, 'error': 'GraphDB –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω'}

        try:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —É NetworkXGraphService –µ—Å—Ç—å –º–µ—Ç–æ–¥ optimize()
            if hasattr(graph, 'optimize'):
                result = await graph.optimize()
            else:
                # –†–µ–∞–ª–∏–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –º–µ—Å—Ç–µ
                # –ù–∞–ø—Ä–∏–º–µ—Ä, —É–¥–∞–ª—è–µ–º —É–∑–ª—ã –±–µ–∑ —Å–≤—è–∑–µ–π
                import networkx as nx
                isolated = list(nx.isolates(graph.graph))
                graph.graph.remove_nodes_from(isolated)
                result = {'nodes_removed': len(isolated), 'edges_remaining': graph.graph.number_of_edges()}
                await graph.save()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞: {e}")
            return {'success': False, 'error': str(e)}

        processing_time = time.time() - start_time

        return {
            'success': True,
            'optimization_result': result,
            'processing_time': processing_time
        }


    async def _process_knowledge_update(self, task_data: Dict) -> Dict:
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π: –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è ChromaDB, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∏ —Ç.–ø.
        """
        logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π")
        start_time = time.time()

        chroma = self.services.get('chroma_db')
        graph = self.services.get('graph_db')
        engram = self.services.get('engram')

        results = {}

        if chroma:
            try:
                # –í ChromaDBService –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ rebuild_indexes()
                # –ï—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if hasattr(chroma, 'rebuild_indexes'):
                    await chroma.rebuild_indexes()
                    results['chroma'] = '–∏–Ω–¥–µ–∫—Å—ã –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω—ã'
                else:
                    results['chroma'] = '–º–µ—Ç–æ–¥ rebuild_indexes –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω'
            except Exception as e:
                results['chroma'] = f'–æ—à–∏–±–∫–∞: {e}'

        if graph:
            try:
                # –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å —Å–≤—è–∑–∏
                if hasattr(graph, 'optimize'):
                    await graph.optimize()
                    results['graph'] = '–≥—Ä–∞—Ñ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω'
                else:
                    results['graph'] = '–Ω–µ—Ç –º–µ—Ç–æ–¥–∞ optimize'
            except Exception as e:
                results['graph'] = f'–æ—à–∏–±–∫–∞: {e}'

        if engram:
            try:
                # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏ —Ç.–ø.
                if hasattr(engram, 'cleanup'):
                    await engram.cleanup()
                    results['engram'] = '–æ—á–∏—Å—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞'
                else:
                    results['engram'] = '–Ω–µ—Ç –º–µ—Ç–æ–¥–∞ cleanup'
            except Exception as e:
                results['engram'] = f'–æ—à–∏–±–∫–∞: {e}'

        processing_time = time.time() - start_time

        return {
            'success': True,
            'results': results,
            'processing_time': processing_time
        }


    def _detect_question_type(self, question: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —à–∞–±–ª–æ–Ω–∞."""
        q = question.lower()
        if '—Ç–µ–æ—Ä–µ–º' in q or '–ø–∏—Ñ–∞–≥–æ—Ä' in q or '—Ñ–æ—Ä–º—É–ª' in q:
            return 'mathematical_theorem'
        elif '–≤–æ–π–Ω' in q or '–∏—Å—Ç–æ—Ä–∏' in q or '—Å–æ–±—ã—Ç–∏' in q or '–≥–æ–¥' in q:
            return 'historical_event'
        elif '—Ñ—É–Ω–∫—Ü–∏' in q or '–∫–ª–∞—Å—Å' in q or '–º–µ—Ç–æ–¥' in q or '–ø–µ—Ä–µ–º–µ–Ω–Ω' in q or '—è–∑—ã–∫' in q:
            return 'programming_concept'
        elif '—Ñ–∏–∑–∏–∫' in q or '—Ö–∏–º–∏' in q or '–±–∏–æ–ª–æ–≥' in q or '–∫–≤–∞–Ω—Ç–æ–≤' in q:
            return 'scientific_concept'
        else:
            return 'default'  # –æ–±—â–∏–π —Å–ª—É—á–∞–π
    
    # ---------- –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–°–¢–û–ì–û –í–û–ü–†–û–°–ê (—Å Engram, ChromaDB, –î–µ—Ç–µ–∫—Ç–∏–≤–æ–º) ----------
        # ---------- –ó–ê–ú–ï–ù–ò –≠–¢–û–¢ –ú–ï–¢–û–î –ü–û–õ–ù–û–°–¢–¨–Æ ----------
    async def _process_simple_question(self, task_data: Dict) -> Dict:
        question = task_data['question']
        start = time.time()

        # 1. Engram
        engram = self.services.get('engram')
        if engram:
            try:
                mem = await engram.retrieve(question, top_k=1, min_confidence=0.6)
                if mem:
                    return {
                        'success': True,
                        'source': 'engram',
                        'answer': mem[0]['content'],
                        'confidence': mem[0]['confidence'],
                        'sources': ['üß† Engram'],
                        'profile': 'default',
                        'key_facts_metadata': [],
                        'query': question,
                        'processing_time': 0.1
                    }
            except Exception as e:
                logger.warning(f"Engram retrieve error: {e}")

        # 2. –î–µ—Ç–µ–∫—Ç–∏–≤
        detective = self.services.get('detective')
        committee = self.services.get('committee')
        analyst = self.services.get('analyst')

        if not (detective and committee and analyst):
            return {'success': False, 'error': '–°–µ—Ä–≤–∏—Å—ã –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã'}

        try:
            # 2.1 –ü–æ–∏—Å–∫
            search_result = await detective.search(question, num_results=15)
            if not search_result.get('success') or not search_result.get('results'):
                return {'success': False, 'error': '–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞'}

            # 2.2 –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–∏—Ç–µ—Ç–æ–º
            filtered = await committee.batch_evaluate(search_result['results'][:10])
            if not filtered:
                return {'success': False, 'error': '–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–±—Ä–∞–∫–æ–≤–∞–Ω—ã'}

            # 2.3 –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–æ–º–µ–Ω–æ–≤
            priority_domains = ['ru.wikipedia.org', 'habr.com', 'postnauka.ru', 'nplus1.ru', 'elementy.ru']
            trash_domains = [
                'otvet.mail.ru', 'answer.mail.ru', 'bolshoyvopros.ru',
                'dzen.ru', 'yandex.ru/q', 'traveler.ru', 'rtraveler.ru',
                'rambler.ru', 'mail.ru', 'ok.ru', 'vk.com',
                'reverso.net', 'translate.', 'wordhippo.com', 'academic.ru',
                '24smi.org', 'uznayvse.ru', 'socionika.info'
            ]

            candidates = []
            for doc in filtered:
                url = doc.get('url', '')
                domain = urlparse(url).netloc.lower()
                if any(bad in domain for bad in trash_domains):
                    continue
                candidates.append((url, domain, doc))

            priority_urls = []
            other_urls = []
            for url, domain, doc in candidates:
                if any(p in domain for p in priority_domains):
                    if 'wikipedia' in domain and not domain.startswith('ru.wikipedia'):
                        continue
                    priority_urls.append((url, doc))
                else:
                    other_urls.append((url, doc))

            urls_to_fetch = []
            for url, _ in priority_urls[:2]:
                urls_to_fetch.append(url)
            for url, _ in other_urls:
                if len(urls_to_fetch) >= 3:
                    break
                if url not in urls_to_fetch:
                    urls_to_fetch.append(url)

            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º: {urls_to_fetch}")
            fetch_tasks = [detective.fetch_page_content(url, question) for url in urls_to_fetch]
            pages = await asyncio.gather(*fetch_tasks, return_exceptions=True)
            valid_pages = [p for p in pages if isinstance(p, dict) and p.get('success')]
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(valid_pages)}/{len(fetch_tasks)}")

            if valid_pages:
                analyst_docs = []
                for page in valid_pages:
                    analyst_docs.append({
                        'url': page['url'],
                        'title': page.get('title', ''),
                        'content': page.get('content', ''),
                    })

                # –ê–Ω–∞–ª–∏–∑
                analysis = await analyst.analyze(analyst_docs, query=question)
                # analysis —Å–æ–¥–µ—Ä–∂–∏—Ç: profile, key_facts_metadata, confidence, key_points, summary –∏ —Ç.–¥.

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Engram (–µ—Å–ª–∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
                if analysis.get('confidence', 0) > 0.5 and analysis.get('key_points'):
                    if engram:
                        try:
                            content = '\n'.join(analysis['key_points'][:5])
                            await engram.store(
                                key=question,
                                content=content,
                                metadata={
                                    'source': 'detective',
                                    'confidence': analysis['confidence'],
                                    'sources': [page['url'] for page in valid_pages[:3]]
                                },
                                confidence=analysis['confidence']
                            )
                        except Exception as e:
                            logger.error(f"Engram store error: {e}")

                # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ì–†–ê–§ ---
                if analysis.get('key_points') and analysis.get('confidence', 0) > 0.6:
                    clean_topic = question.strip().rstrip('?').strip()[:100]
                    if clean_topic:
                        analysis_for_graph = {
                            'summary': analysis.get('key_points', [''])[0],
                            'key_points': analysis['key_points'],
                            'confidence': analysis['confidence']
                        }
                        await self._store_knowledge(clean_topic, analysis_for_graph)
                        logger.info(f"üìå –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ–ø—Ä–æ—Å–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –≥—Ä–∞—Ñ –ø–æ —Ç–µ–º–µ '{clean_topic}'")
                # ------------------------------------

                return {
                    'success': True,
                    'source': 'detective',
                    # –ø–æ–ª–µ 'answer' –Ω–µ –∑–∞–ø–æ–ª–Ω—è–µ–º, –æ–Ω–æ –±—É–¥–µ—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ —à–∞–±–ª–æ–Ω –≤ autonomous_ai.py
                    'confidence': analysis.get('confidence', 0.5),
                    'sources': [page['url'] for page in valid_pages[:3]],
                    'profile': analysis.get('profile', 'default'),
                    'key_facts_metadata': analysis.get('key_facts_metadata', []),
                    'key_points': analysis.get('key_points', []),
                    'query': question,
                    'processing_time': time.time() - start
                }
            else:
                # Fallback ‚Äî —Å–Ω–∏–ø–ø–µ—Ç—ã
                fallback = self._synthesize_from_snippets(filtered[:3])
                return {
                    'success': True,
                    'source': 'detective_fallback',
                    'answer': fallback,
                    'confidence': 0.3,
                    'sources': [d.get('url', '') for d in filtered[:3]],
                    'profile': 'default',
                    'key_facts_metadata': [],
                    'key_points': [],
                    'query': question,
                    'processing_time': time.time() - start
                }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
 

    # ---------- –î–û–ë–ê–í–¨ –≠–¢–û–¢ –ú–ï–¢–û–î (–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π) ----------
    def _synthesize_from_snippets(self, docs: List[Dict]) -> str:
        """–°–∫–ª–µ–∏–≤–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–Ω–∏–ø–ø–µ—Ç—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        parts = []
        for i, doc in enumerate(docs, 1):
            title = doc.get('title', '')
            snippet = doc.get('snippet', '')
            if title:
                parts.append(f"{i}. {title}")
            if snippet:
                parts.append(f"   {snippet[:200]}")
        return '\n'.join(parts) if parts else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
    
    # ---------- –°–û–•–†–ê–ù–ï–ù–ò–ï –ó–ù–ê–ù–ò–ô (Engram, ChromaDB, Graph) ----------
    async def _store_knowledge(self, topic: str, analysis: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤–æ –≤—Å–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        storage_tasks = []

        # Engram –ø–∞–º—è—Ç—å
        engram = self.services.get('engram')
        if engram:
            storage_tasks.append(
                engram.store(
                    key=topic,
                    content=analysis.get('summary', '') or '\n'.join(analysis.get('key_points', [])),
                    metadata={
                        'topic': topic,
                        'confidence': analysis.get('confidence', 0.5),
                        'timestamp': datetime.now().isoformat(),
                        'source': 'coordinator',
                        'key_points_count': len(analysis.get('key_points', []))
                    },
                    confidence=analysis.get('confidence', 0.5)
                )
            )

        # ChromaDB (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
        chroma = self.services.get('chroma_db')
        if chroma and 'key_points' in analysis:
            for i, point in enumerate(analysis['key_points'][:5]):  # –æ–≥—Ä–∞–Ω–∏—á–∏–º 5
                if len(point) > 50:  # —Ç–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ
                    storage_tasks.append(
                        chroma.add_document(
                            text=point,
                            metadata={
                                'topic': topic,
                                'type': 'key_point',
                                'source': 'analyst',
                                'confidence': analysis.get('confidence', 0.5),
                                'index': i
                            }
                        )
                    )

        # –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
        graph = self.services.get('graph_db')
        if graph:
            # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–µ–ª —Ç–µ–º—ã, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            storage_tasks.append(
                graph.add_knowledge_chunk(
                    topic=topic,
                    chunk={
                        'summary': analysis.get('summary', '')[:500],
                        'key_points': analysis.get('key_points', [])[:5],
                        'confidence': analysis.get('confidence', 0.5)
                    },
                    relations=[]  # –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –ø–æ–∑–∂–µ
                )
            )

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏)
        if storage_tasks:
            await asyncio.gather(*storage_tasks, return_exceptions=True)
            logger.info(f"üíæ –ó–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ '{topic}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {len(storage_tasks)} —Ö—Ä–∞–Ω–∏–ª–∏—â")
    
    # ---------- –û–°–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –æ—Å—Ç–∞–≤–ª—è–µ–º) ----------
    async def _process_deep_research(self, task_data: Dict) -> Dict:
        """
        –ì–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–º—ã —Å –∑–∞–¥–∞–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π.
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã, –∏—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç.
        """
        topic = task_data.get('topic')
        depth = task_data.get('depth', 2)
        
        if not topic:
            return {'success': False, 'error': '–ù–µ —É–∫–∞–∑–∞–Ω–∞ —Ç–µ–º–∞'}
        
        logger.info(f"üîç –ì–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: '{topic}' (–≥–ª—É–±–∏–Ω–∞ {depth})")
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Ä–≤–∏—Å—ã
        detective = self.services.get('detective')
        analyst = self.services.get('analyst')
        interviewer = self.services.get('interviewer')
        
        if not detective or not analyst or not interviewer:
            return {'success': False, 'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã'}
        
        try:
            # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥–ª—É–±–∏–Ω—ã
            questions = await interviewer.generate_research_questions(
                topic,
                depth=depth,
                num_questions=5 + depth * 2  # 7 –¥–ª—è depth=1, 9 –¥–ª—è depth=2, 11 –¥–ª—è depth=3
            )
            logger.info(f"   ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
            
            # 2. –ó–∞–ø—É—Å–∫–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–µ—Ç–µ–∫—Ç–∏–≤
            investigation = await detective.investigate_topic_advanced(topic, questions)
            if not investigation.get('success'):
                return {'success': False, 'error': investigation.get('error', '–û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è')}
            
            chunks = investigation.get('content_chunks', [])
            if not chunks:
                return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç'}
            
            logger.info(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            analysis = await analyst.analyze(chunks, query=topic)
            
            # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã
            key_points = analysis.get('key_points', [])
            
            # 5. –û—á–∏—â–∞–µ–º —Ñ–∞–∫—Ç—ã –æ—Ç –º—É—Å–æ—Ä–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º quality.yaml –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            if key_points:
                # –ï—Å–ª–∏ –µ—Å—Ç—å quality_config, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
                if hasattr(self, 'quality_config') and self.quality_config:
                    junk_phrases = self.quality_config.get('junk_phrases', [])
                else:
                    junk_phrases = [
                        '–∞—Ä—Ö–∏–≤–Ω–∞—è –∫–æ–ø–∏—è', 'wayback machine', '‚Üë', '–∏—Å—Ç–æ—á–Ω–∏–∫:',
                        '–¥–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è:', '–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ', '–∞–≤—Ç–æ—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª–∞',
                        '–ª–∏—Ü–µ–Ω–∑–∏—è creative commons', '—ç—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑',
                        '—É —ç—Ç–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –¥—Ä—É–≥–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è', '—Å–º. —Ç–∞–∫–∂–µ',
                        '–ø—Ä–∏–º–µ—á–∞–Ω–∏—è', '—Å—Å—ã–ª–∫–∏', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '—Ñ–æ—Ç–æ:', '¬©',
                        'getty images', 'reuters', 'ap'
                    ]
                
                # –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏
                def clean_fact(text):
                    text = re.sub(r'\(–∑–Ω–∞—á–µ–Ω–∏—è\)|\[[^\]]+\]|\{\{[^\}]+\}\}', '', text)
                    text = re.sub(r'^—Å–º\.\s*|^—Ç–∞–∫–∂–µ\s*', '', text, flags=re.I)
                    text = re.sub(r'\s+([,.!?;:])', r'\1', text)
                    text = re.sub(r'^[,\s]+', '', text)
                    if len(text) < 30 or re.match(r'^[\s,.!?;:\-]+$', text):
                        return None
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã
                    text_lower = text.lower()
                    for junk in junk_phrases:
                        if junk in text_lower:
                            return None
                    return text.strip()
                
                cleaned_points = []
                seen = set()
                for point in key_points:
                    cleaned = clean_fact(point)
                    if cleaned and cleaned not in seen:
                        norm = cleaned.rstrip('.,!?;:').lower()
                        if norm not in seen:
                            seen.add(norm)
                            cleaned_points.append(cleaned)
                
                key_points = cleaned_points[:15]
                synthesis = " ".join(key_points[:3]) if key_points else ""
                if len(synthesis) > 300:
                    synthesis = synthesis[:300] + "..."
            else:
                # Fallback: –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —á–∞–Ω–∫–æ–≤
                logger.warning("‚ö†Ô∏è –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                fallback_texts = []
                import re
                for chunk in chunks[:3]:
                    text = chunk.get('text', '')
                    if text:
                        sentences = re.split(r'(?<=[.!?])\s+', text)
                        for sent in sentences[:3]:
                            sent = sent.strip()
                            if 30 < len(sent) < 500 and not re.search(r'https?://|¬©|—Ñ–æ—Ç–æ|–∫—É–ø–∏—Ç—å', sent, re.I):
                                fallback_texts.append(sent)
                key_points = fallback_texts[:10]
                synthesis = " ".join(key_points[:3]) if key_points else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
            
            # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è
            await self._store_knowledge(topic, {
                'summary': synthesis,
                'key_points': key_points,
                'confidence': analysis.get('confidence', 0.5),
                'depth': depth,
                'chunks_processed': len(chunks)
            })
            
            processing_time = time.time() - start_time
            
            return {
                'success': True,
                'topic': topic,
                'synthesis': synthesis,
                'key_findings': key_points[:10],
                'sources_used': investigation.get('pages_processed', 0),
                'chunks_processed': len(chunks),
                'depth_achieved': depth,
                'confidence': analysis.get('confidence', 0.5),
                'processing_time': processing_time
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ _process_deep_research: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
    
    def _synthesize_from_embeddings(self, results: List[Dict]) -> str:
        if not results:
            return ""
        # –ü—Ä–æ—Å—Ç–∞—è —Å–±–æ—Ä–∫–∞
        texts = [r['text'] for r in results if 'text' in r]
        return "\n\n".join(texts[:2])
    
    async def submit_task(self, task_data: Dict) -> str:
        task_id = str(uuid.uuid4())[:8]
        task = {
            'task_id': task_id,
            'created_at': datetime.now().isoformat(),
            'status': TaskStatus.PENDING.value,
            **task_data
        }
        if 'priority' not in task:
            task['priority'] = TaskPriority.MEDIUM.value

        self.tasks[task_id] = task

        try:
            if task['priority'] == TaskPriority.CRITICAL.value:
                heapq.heappush(self.priority_queue, (0, task_id))
            elif task['priority'] == TaskPriority.HIGH.value:
                heapq.heappush(self.priority_queue, (1, task_id))
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º put_nowait, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞–≤—Å–µ–≥–¥–∞
                self.task_queue.put_nowait(task)
        except asyncio.QueueFull:
            logger.error(f"‚ùå –û—á–µ—Ä–µ–¥—å –∑–∞–¥–∞—á –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∞, –∑–∞–¥–∞—á–∞ {task_id} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞")
            # –ú–æ–∂–Ω–æ –ª–∏–±–æ –∂–¥–∞—Ç—å, –ª–∏–±–æ –æ—Ç–º–µ–Ω—è—Ç—å
            raise RuntimeError("Task queue is full")

        await self._emit_event('task_started', {
            'task_id': task_id,
            'type': task['type'],
            'priority': task['priority'],
            'timestamp': task['created_at']
        })
        logger.info(f"üì• –ó–∞–¥–∞—á–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {task_id} ({task['type']})")
        return task_id
    
    async def get_task_status(self, task_id: str) -> Dict:
        if task_id not in self.tasks:
            return {'error': 'Task not found'}
        task = self.tasks[task_id]
        result = {
            'task_id': task_id,
            'type': task.get('type'),
            'status': task.get('status'),
            'created_at': task.get('created_at'),
            'priority': task.get('priority'),
            'metadata': task.get('metadata', {})
        }
        if task_id in self.task_results:
            result['result'] = self.task_results[task_id]
        return result
    
    async def cancel_task(self, task_id: str) -> bool:
        if task_id not in self.tasks:
            return False
        task = self.tasks[task_id]
        if task['status'] not in [TaskStatus.PENDING.value, TaskStatus.PROCESSING.value]:
            return False

        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ—Ç–º–µ–Ω—ë–Ω–Ω—É—é
        task['status'] = TaskStatus.CANCELLED.value
        task['cancelled_at'] = datetime.now().isoformat()

        # –£–¥–∞–ª—è–µ–º –∏–∑ –æ—á–µ—Ä–µ–¥–µ–π, –µ—Å–ª–∏ –µ—â—ë —Ç–∞–º (–≥—Ä—É–±–æ, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –æ—á–µ—Ä–µ–¥—å ‚Äì –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å
        self.priority_queue = [(p, tid) for p, tid in self.priority_queue if tid != task_id]
        heapq.heapify(self.priority_queue)

        # –î–ª—è asyncio.Queue –Ω–µ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ —É–¥–∞–ª–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç, –Ω–æ –º—ã –ø—Ä–æ—Å—Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ–º ‚Äì
        # –≤–æ—Ä–∫–µ—Ä –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç —Å—Ç–∞—Ç—É—Å –∏ –Ω–µ –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å.
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Ç–∞—Ç—É—Å–∞ –≤ _get_next_task, –Ω–æ –ø—Ä–æ—â–µ –∑–¥–µ—Å—å:
        # –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –≤ –æ–±—ã—á–Ω–æ–π –æ—á–µ—Ä–µ–¥–∏, –æ–Ω–∞ –≤—Å—ë —Ä–∞–≤–Ω–æ –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞, –Ω–æ –º—ã –µ—ë –æ—Ç–º–µ–Ω–∏–ª–∏,
        # –ø–æ—ç—Ç–æ–º—É –≤–æ—Ä–∫–µ—Ä –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
        # –î–ª—è —ç—Ç–æ–≥–æ –¥–æ–±–∞–≤–∏–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ _worker_loop.

        logger.info(f"‚ùå –ó–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞: {task_id}")
        return True
    
    async def _start_monitoring(self):
        async def monitoring_loop():
            while not self.is_shutting_down:
                try:
                    await self._update_monitoring_metrics()
                    await asyncio.sleep(self.config.monitoring_interval)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
                    await asyncio.sleep(5)
        self.monitoring_task = asyncio.create_task(monitoring_loop())
    
    async def _update_monitoring_metrics(self):
        queue_size = self.task_queue.qsize() + len(self.priority_queue)
        active_workers = len([t for t in self.worker_tasks if not t.done()])
        worker_utilization = active_workers / self.num_workers if self.num_workers > 0 else 0
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        self.metrics.update({
            'queue_size': queue_size,
            'worker_utilization': worker_utilization,
            'memory_usage_mb': memory_mb,
            'active_workers': active_workers,
            'pending_tasks': len([t for t in self.tasks.values() 
                                if t['status'] == TaskStatus.PENDING.value])
        })
    
    async def _schedule_background_tasks(self):
        async def hourly_maintenance():
            while not self.is_shutting_down:
                await asyncio.sleep(3600)
                await self.submit_task({'type': TaskType.SYSTEM_MAINTENANCE.value, 'priority': TaskPriority.LOW.value})
        async def daily_optimization():
            while not self.is_shutting_down:
                await asyncio.sleep(86400)
                await self.submit_task({'type': TaskType.GRAPH_OPTIMIZATION.value, 'priority': TaskPriority.LOW.value})
        async def cache_cleanup():
            while not self.is_shutting_down:
                await asyncio.sleep(21600)
                await self.submit_task({'type': TaskType.CACHE_CLEANUP.value, 'priority': TaskPriority.LOW.value})
        self.scheduled_tasks.extend([
            asyncio.create_task(hourly_maintenance()),
            asyncio.create_task(daily_optimization()),
            asyncio.create_task(cache_cleanup())
        ])
    
    async def _check_services_health(self, detailed: bool = False) -> Dict:
        health_results = {}
        for name, service in self.services.items():
            try:
                if hasattr(service, 'health_check'):
                    health = await service.health_check()
                    health_results[name] = {
                        'status': 'healthy' if health.get('healthy', False) else 'unhealthy',
                        'details': health if detailed else None
                    }
                else:
                    health_results[name] = {'status': 'unknown'}
            except Exception as e:
                health_results[name] = {'status': 'error', 'error': str(e)}
        return health_results
    
    async def get_system_metrics(self) -> Dict:
        all_metrics = {
            'coordinator': self.metrics.copy(),
            'task_stats': self.task_stats.copy(),
            'queue_info': {
                'priority_queue': len(self.priority_queue),
                'regular_queue': self.task_queue.qsize(),
                'total_tasks': len(self.tasks),
                'pending_tasks': len([t for t in self.tasks.values() 
                                    if t['status'] == TaskStatus.PENDING.value])
            }
        }
        for name, service in self.services.items():
            if hasattr(service, 'get_metrics'):
                try:
                    service_metrics = await service.get_metrics()
                    all_metrics[name] = service_metrics
                except Exception as e:
                    all_metrics[name] = {'error': str(e)}
        return all_metrics
    
    async def _emit_event(self, event_type: str, data: Dict):
        if event_type in self.event_subscribers:
            for callback in self.event_subscribers[event_type]:
                try:
                    await callback(data)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ —Å–æ–±—ã—Ç–∏—è {event_type}: {e}")
    
    def _update_metrics(self, task_data: Dict, result: Dict):
        task_type = task_data.get('type')
        if task_type in self.task_stats:
            stats = self.task_stats[task_type]
            stats['processed'] += 1
            if not result.get('success', False):
                stats['failed'] += 1
            processing_time = result.get('processing_time', 0)
            if stats['avg_time'] == 0:
                stats['avg_time'] = processing_time
            else:
                stats['avg_time'] = 0.9 * stats['avg_time'] + 0.1 * processing_time
        
        self.metrics['tasks_processed'] += 1
        if not result.get('success', False):
            self.metrics['tasks_failed'] += 1
    
    async def _update_task_status(self, task_id: str, status: TaskStatus):
        if task_id in self.tasks:
            self.tasks[task_id]['status'] = status.value
            self.tasks[task_id]['updated_at'] = datetime.now().isoformat()
    
    async def shutdown(self):
        if self.is_shutting_down:
            return
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã ServiceCoordinator...")
        self.is_shutting_down = True
        for task_id, task in self.tasks.items():
            if task['status'] == TaskStatus.PROCESSING.value:
                await self.cancel_task(task_id)
        for worker_task in self.worker_tasks:
            worker_task.cancel()
        if hasattr(self, 'monitoring_task'):
            self.monitoring_task.cancel()
        for scheduled_task in self.scheduled_tasks:
            scheduled_task.cancel()
        if self.worker_tasks:
            await asyncio.gather(*self.worker_tasks, return_exceptions=True)
        await self._save_state()
        logger.info("‚úÖ ServiceCoordinator –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")
    
    async def _save_state(self):
        try:
            state = {
                'metrics': self.metrics,
                'task_stats': self.task_stats,
                'tasks': {k: v for k, v in self.tasks.items()
                        if v['status'] in [TaskStatus.PENDING.value, TaskStatus.PROCESSING.value]},
                'saved_at': datetime.now().isoformat()
            }
            os.makedirs('./data/state', exist_ok=True)
            with open('./data/state/coordinator_state.json', 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            logger.info("üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
service_coordinator: Optional[ServiceCoordinator] = None

def get_coordinator() -> ServiceCoordinator:
    global service_coordinator
    if service_coordinator is None:
        from appp.core.config import Config
        config = Config.get()
        service_coordinator = ServiceCoordinator(config)
    return service_coordinator