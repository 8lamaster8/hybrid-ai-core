"""
üßπ –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä - –æ—á–∏—Å—Ç–∫–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import re
import html
import unicodedata
from typing import List, Optional


class TextCleaner:
    """
    –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.
    - –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    - –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —á–∏—Ç–∞–µ–º–æ–º—É –≤–∏–¥—É
    """
    
    def __init__(self):
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–∞
        self.url_pattern = re.compile(r'https?://\S+|www\.\S+')
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.extra_spaces_pattern = re.compile(r'\s+')
        self.non_printable_pattern = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')
        self.reference_pattern = re.compile(r'\[\d+\]|\[\w+\]|\[[^]]+\]')
        self.control_chars = re.compile(r'[\r\n\t]+')
        
        # –°—Ç–æ–ø-—Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —è–≤–ª—è—é—Ç—Å—è –º—É—Å–æ—Ä–æ–º
        self.junk_phrases = [
            '–∞—Ä—Ö–∏–≤–Ω–∞—è –∫–æ–ø–∏—è', 'wayback machine', '‚Üë', '–∫–æ–º–º.', 
            '–∏—Å—Ç–æ—á–Ω–∏–∫:', '—Å—Å—ã–ª–∫–∞:', '–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '—Å–Ω–æ—Å–∫–∞',
            '—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å', '–ø—Ä–∞–≤–∏—Ç—å', '—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è',
            '–∫–∞—Ç–µ–≥–æ—Ä–∏—è:', '–≤–∏–∫–∏–ø–µ–¥–∏—è', 'wikipedia', 'facebook',
            'twitter', 'instagram', 'tiktok', 'youtube', 'pinterest',
            '–∫—É–ø–∏—Ç—å', '—Ä–µ–∫–ª–∞–º–∞', '—Å–ø–∞–º', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è'
        ]
    
    def clean(self, text: str, remove_junk: bool = True, normalize_whitespace: bool = True) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            remove_junk: –£–¥–∞–ª—è—Ç—å –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            normalize_whitespace: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not isinstance(text, str):
            return ""
        
        # 1. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π
        text = html.unescape(text)
        
        # 2. –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
        text = self.html_tag_pattern.sub(' ', text)
        
        # 3. –£–¥–∞–ª–µ–Ω–∏–µ URL
        text = self.url_pattern.sub(' ', text)
        
        # 4. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ [1], [a], –∏ —Ç.–¥.
        text = self.reference_pattern.sub(' ', text)
        
        # 5. –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–µ—á–∞—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = self.non_printable_pattern.sub('', text)
        
        # 6. –ó–∞–º–µ–Ω–∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–æ–±–µ–ª–∞–º–∏
        text = self.control_chars.sub(' ', text)
        
        # 7. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        if normalize_whitespace:
            text = self.extra_spaces_pattern.sub(' ', text)
        
        # 8. –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑
        if remove_junk:
            text = self._remove_junk_phrases(text)
        
        # 9. –û–±—Ä–µ–∑–∫–∞ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ –∫—Ä–∞—è–º
        text = text.strip()
        
        return text
    
    def _remove_junk_phrases(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑"""
        lower_text = text.lower()
        for phrase in self.junk_phrases:
            if phrase in lower_text:
                # –£–¥–∞–ª—è–µ–º —Ñ—Ä–∞–∑—É –∏ –æ–∫—Ä—É–∂–∞—é—â–∏–µ –ø—Ä–æ–±–µ–ª—ã
                pattern = re.compile(re.escape(phrase), re.IGNORECASE)
                text = pattern.sub(' ', text)
        
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è
        text = self.extra_spaces_pattern.sub(' ', text)
        return text
    
    def extract_sentences(self, text: str, min_length: int = 20) -> List[str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–∏–º–≤–æ–ª–æ–≤)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö
        sentences = [s.strip() for s in sentences if s.strip()]
        sentences = [s for s in sentences if len(s) >= min_length]
        
        return sentences
    
    def normalize_unicode(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode (NFKC)"""
        return unicodedata.normalize('NFKC', text)
    
    def remove_repetitions(self, text: str, threshold: int = 3) -> str:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ (–ø—Ä–æ—Å—Ç–µ–π—à–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞).
        """
        sentences = self.extract_sentences(text, min_length=10)
        unique_sentences = []
        seen = set()
        
        for sent in sentences:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ —Å–∏–≥–Ω–∞—Ç—É—Ä—É
            sig = sent[:50].lower()
            if sig not in seen:
                seen.add(sig)
                unique_sentences.append(sent)
        
        return ' '.join(unique_sentences)


class ContentExtractor:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ HTML.
    (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è; –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 
    readability-lxml –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏)
    """
    
    def __init__(self):
        self.text_cleaner = TextCleaner()
    
    def extract_from_html(self, html_content: str) -> str:
        """
        –ü—Ä–æ—Å—Ç–µ–π—à–µ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML.
        –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BeautifulSoup –∏–ª–∏ lxml.
        """
        # –≠—Ç–æ –∑–∞–≥–ª—É—à–∫–∞; –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è BeautifulSoup/lxml
        # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–¥–µ—Å—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏; –Ω–∞—Å—Ç–æ—è—â–∞—è –ª–æ–≥–∏–∫–∞ –≤ detective.py
        return html_content