"""
üßπ –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä - –æ—á–∏—Å—Ç–∫–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import re
import html
import unicodedata
from typing import List, Optional


class TextCleaner:
    """
    –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.
    - –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    - –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —á–∏—Ç–∞–µ–º–æ–º—É –≤–∏–¥—É
    """
    
    def __init__(self):
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–∞
        self.url_pattern = re.compile(r'https?://\S+|www\.\S+')
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.extra_spaces_pattern = re.compile(r'\s+')
        self.non_printable_pattern = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')
        self.reference_pattern = re.compile(r'\[\d+\]|\[\w+\]|\[[^]]+\]')
        self.control_chars = re.compile(r'[\r\n\t]+')
        
        # –°—Ç–æ–ø-—Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —è–≤–ª—è—é—Ç—Å—è –º—É—Å–æ—Ä–æ–º
        self.junk_phrases = [
            '–∞—Ä—Ö–∏–≤–Ω–∞—è –∫–æ–ø–∏—è', 'wayback machine', '‚Üë', '–∫–æ–º–º.', 
            '–∏—Å—Ç–æ—á–Ω–∏–∫:', '—Å—Å—ã–ª–∫–∞:', '–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '—Å–Ω–æ—Å–∫–∞',
            '—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å', '–ø—Ä–∞–≤–∏—Ç—å', '—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è',
            '–∫–∞—Ç–µ–≥–æ—Ä–∏—è:', '–≤–∏–∫–∏–ø–µ–¥–∏—è', 'wikipedia', 'facebook',
            'twitter', 'instagram', 'tiktok', 'youtube', 'pinterest',
            '–∫—É–ø–∏—Ç—å', '—Ä–µ–∫–ª–∞–º–∞', '—Å–ø–∞–º', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è'
        ]
    
    def clean(self, text: str, remove_junk: bool = True, normalize_whitespace: bool = True) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            remove_junk: –£–¥–∞–ª—è—Ç—å –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            normalize_whitespace: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not isinstance(text, str):
            return ""
        
        # 1. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π
        text = html.unescape(text)
        
        # 2. –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
        text = self.html_tag_pattern.sub(' ', text)
        
        # 3. –£–¥–∞–ª–µ–Ω–∏–µ URL
        text = self.url_pattern.sub(' ', text)
        
        # 4. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ [1], [a], –∏ —Ç.–¥.
        text = self.reference_pattern.sub(' ', text)
        
        # 5. –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–µ—á–∞—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = self.non_printable_pattern.sub('', text)
        
        # 6. –ó–∞–º–µ–Ω–∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–æ–±–µ–ª–∞–º–∏
        text = self.control_chars.sub(' ', text)
        
        # 7. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        if normalize_whitespace:
            text = self.extra_spaces_pattern.sub(' ', text)
        
        # 8. –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑
        if remove_junk:
            text = self._remove_junk_phrases(text)
        
        # 9. –û–±—Ä–µ–∑–∫–∞ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ –∫—Ä–∞—è–º
        text = text.strip()
        
        return text
    
    def _remove_junk_phrases(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑"""
        lower_text = text.lower()
        for phrase in self.junk_phrases:
            if phrase in lower_text:
                # –£–¥–∞–ª—è–µ–º —Ñ—Ä–∞–∑—É –∏ –æ–∫—Ä—É–∂–∞—é—â–∏–µ –ø—Ä–æ–±–µ–ª—ã
                pattern = re.compile(re.escape(phrase), re.IGNORECASE)
                text = pattern.sub(' ', text)
        
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è
        text = self.extra_spaces_pattern.sub(' ', text)
        return text
    
    def extract_sentences(self, text: str, min_length: int = 20) -> List[str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–∏–º–≤–æ–ª–æ–≤)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö
        sentences = [s.strip() for s in sentences if s.strip()]
        sentences = [s for s in sentences if len(s) >= min_length]
        
        return sentences
    
    def normalize_unicode(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode (NFKC)"""
        return unicodedata.normalize('NFKC', text)
    
    def remove_repetitions(self, text: str, threshold: int = 3) -> str:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ (–ø—Ä–æ—Å—Ç–µ–π—à–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞).
        """
        sentences = self.extract_sentences(text, min_length=10)
        unique_sentences = []
        seen = set()
        
        for sent in sentences:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ —Å–∏–≥–Ω–∞—Ç—É—Ä—É
            sig = sent[:50].lower()
            if sig not in seen:
                seen.add(sig)
                unique_sentences.append(sent)
        
        return ' '.join(unique_sentences)


import re
from typing import Optional

try:
    from bs4 import BeautifulSoup, Comment
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å lxml, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –æ—Å—Ç–∞–≤–∏–º –∑–∞–≥–ª—É—à–∫—É
    BeautifulSoup = None

from appp.utils.text_processor import TextCleaner


class ContentExtractor:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ HTML.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.
    """

    # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º—É—Å–æ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤
    REMOVE_SELECTORS = [
        'script', 'style', 'noscript', 'meta', 'link',
        'nav', 'header', 'footer', 'aside',
        '.sidebar', '#sidebar', '.comments', '#comments',
        '.advertisement', '.ads', '.banner',
        '.cookie-notice', '.popup', '.modal',
        'form', 'input', 'button',
        '.social-share', '.share-buttons',
        '.related-posts', '.recommendations'
    ]

    # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    CONTENT_SELECTORS = [
        'article',
        'main',
        '[role="main"]',
        '.post-content',
        '.entry-content',
        '.article-content',
        '.content-body',
        '#content',
        '.content'
    ]

    def __init__(self, use_readability_if_available: bool = False):
        self.text_cleaner = TextCleaner()
        self.use_readability = use_readability_if_available

        # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å readability-lxml, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        self.readability = None
        if self.use_readability:
            try:
                from readability import Document
                self.readability = Document
            except ImportError:
                pass

    def extract_from_html(self, html_content: str, url: Optional[str] = None) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –∏–∑ HTML.

        Args:
            html_content: –ò—Å—Ö–æ–¥–Ω—ã–π HTML
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è)

        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not html_content:
            return ""

        # 1. –ï—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω readability –∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if self.readability:
            try:
                doc = self.readability(html_content, url=url)
                return self.text_cleaner.clean(doc.summary())
            except Exception as e:
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞–¥–∞–µ–º –Ω–∞ BeautifulSoup
                pass

        # 2. –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup
        if not BS4_AVAILABLE:
            # –ö—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π: —É–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ –ø—Ä–æ—Å—Ç—ã–º regexp (–æ—á–µ–Ω—å –≥—Ä—è–∑–Ω–æ)
            text = re.sub(r'<[^>]+>', ' ', html_content)
            return self.text_cleaner.clean(text)

        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            self._remove_unwanted(soup)

            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_container = self._find_content_container(soup)

            if content_container:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                raw_text = content_container.get_text(separator='\n', strip=True)
            else:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –±–µ—Ä—ë–º –≤–µ—Å—å body (–±–µ–∑ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
                body = soup.find('body')
                if body:
                    raw_text = body.get_text(separator='\n', strip=True)
                else:
                    # –ï—Å–ª–∏ –∏ body –Ω–µ—Ç ‚Äî –≤–µ—Å—å —Å—É–ø
                    raw_text = soup.get_text(separator='\n', strip=True)

            # 3. –û—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ TextCleaner
            cleaned = self.text_cleaner.clean(raw_text)
            return cleaned

        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –ª—é–±–æ–π –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ ‚Äî –ø–∞–¥–∞–µ–º –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –æ—á–∏—Å—Ç–∫—É
            text = re.sub(r'<[^>]+>', ' ', html_content)
            return self.text_cleaner.clean(text)

    def _remove_unwanted(self, soup: BeautifulSoup) -> None:
        """–£–¥–∞–ª—è–µ—Ç –∏–∑ —Å—É–ø–∞ –º—É—Å–æ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º."""
        # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()

        # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        for selector in self.REMOVE_SELECTORS:
            for element in soup.select(selector):
                element.decompose()

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —É–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        # for element in soup.find_all():
        #     if not element.get_text(strip=True) and element.name not in ['br', 'hr', 'img']:
        #         element.decompose()

    def _find_content_container(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        """–ò—â–µ—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º."""
        for selector in self.CONTENT_SELECTORS:
            container = soup.select_one(selector)
            if container:
                return container

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –ø–æ–ø—Ä–æ–±—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É: —Ç–µ–≥ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
        candidates = []
        for tag in soup.find_all(['div', 'section', 'article']):
            text_len = len(tag.get_text(strip=True))
            if text_len > 200:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                candidates.append((text_len, tag))

        if candidates:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–≥ —Å —Å–∞–º—ã–º –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
            candidates.sort(key=lambda x: x[0], reverse=True)
            return candidates[0][1]

        return None