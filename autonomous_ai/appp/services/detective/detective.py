"""
üöÄ –î–µ—Ç–µ–∫—Ç–∏–≤ ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º trafilatura –∏ RobustTextCleaner
"""

import asyncio
import aiohttp
import aiofiles
import hashlib
import json
import os
import random
import re
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse, quote_plus

from bs4 import BeautifulSoup

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False
    print("‚ö†Ô∏è trafilatura –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥")

try:
    from readability import Document
    READABILITY_AVAILABLE = True
except ImportError:
    READABILITY_AVAILABLE = False

from appp.core.logging import logger
from appp.utils.text_processor import TextCleaner
from appp.services.real_search import hybrid_searcher


class Detective:
    def __init__(self, config: Dict):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.cache_dir = "./data/cache/detective"
        self.cache = {}
        self.cache_ttl = 3600
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/119.0.0.0 Safari/537.36',
        ]
        self.blacklist = set(config.get('blacklist_domains', []))
        self.priority_domains = set(config.get('priority_domains', []))
        self.text_cleaner = TextCleaner()
        #self.robust_cleaner = RobustTextCleaner()
        self.stats = {
            'searches': 0,
            'pages_processed': 0,
            'requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0,
            'avg_response_time': 0,
            'unique_domains': set(),
            'total_content_length': 0
        }
        logger.info("üöÄ Detective —Å–æ–∑–¥–∞–Ω")

    async def initialize(self):
        logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Detective...")
        os.makedirs(self.cache_dir, exist_ok=True)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        connector = aiohttp.TCPConnector(limit=100, ssl=True)#ssl=False
        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        await self._load_cache()
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
        if not getattr(hybrid_searcher, '_initialized', False):
            await hybrid_searcher.initialize()
            hybrid_searcher._initialized = True
        logger.info("‚úÖ Detective –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return True

    async def fetch_page_content(self, url: str, query: str = "") -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º"""
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        result = await self._fetch_and_parse(url, query)
        if result.get('success'):
            content_len = len(result.get('content', ''))
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {url} ({content_len} —Å–∏–º–≤–æ–ª–æ–≤)")
            self.stats['pages_processed'] += 1
            return result
        else:
            logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {result.get('error')}")
            return None

    async def search(self, query: str, num_results: int = 20) -> Dict:
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –º—É—Å–æ—Ä–∞"""
        start = time.time()
        self.stats['searches'] += 1
        logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}'")
        try:
            results = await hybrid_searcher.smart_search(query, num_results * 2)

            is_russian = bool(re.search('[–∞-—è—ë]', query.lower()))
            filtered_results = []
            for r in results:
                url = r.get('url', '')
                title = r.get('title', '').lower()
                snippet = r.get('snippet', '').lower()

                if any(bad in url for bad in ['doubleclick', 'googleadservices', 'youtube.com', 'facebook.com', 'instagram', 'tiktok']):
                    continue

                if is_russian:
                    domain = urlparse(url).netloc
                    bad_domains = ['es.wikipedia', 'en.wikipedia', 'de.wikipedia', 'fr.wikipedia',
                                   'it.wikipedia', 'pt.wikipedia', 'ja.wikipedia', 'zh.wikipedia',
                                   'wikidata', 'wikimedia', 'wiktionary']
                    if any(bad in domain for bad in bad_domains):
                        topic_words = set(re.findall(r'\b\w{4,}\b', query.lower()))
                        title_words = set(re.findall(r'\b\w{4,}\b', title))
                        snippet_words = set(re.findall(r'\b\w{4,}\b', snippet))
                        if not (topic_words & title_words) and not (topic_words & snippet_words):
                            continue

                filtered_results.append(r)

            formatted = []
            for r in filtered_results[:num_results]:
                url = r.get('url')
                if not url:
                    continue
                domain = urlparse(url).netloc
                formatted.append({
                    'url': url,
                    'title': r.get('title', ''),
                    'snippet': r.get('snippet', ''),
                    'domain': domain,
                    'is_priority': any(p in domain for p in self.priority_domains),
                    'relevance_score': r.get('final_score', r.get('relevance', 0.7))
                })

            formatted = [f for f in formatted if not self._is_blacklisted(f['url'])]
            formatted.sort(key=lambda x: x.get('is_priority', False), reverse=True)

            elapsed = time.time() - start
            self.stats['avg_response_time'] = (
                (self.stats['avg_response_time'] * (self.stats['searches'] - 1) + elapsed)
                / self.stats['searches']
            )
            return {
                'success': True,
                'query': query,
                'results': formatted[:num_results],
                'stats': {'total_found': len(results), 'filtered': len(formatted), 'time': elapsed}
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            self.stats['errors'] += 1
            return {'success': False, 'error': str(e), 'results': []}

    async def investigate_topic_advanced(self, topic: str, questions: List[str] = None) -> Dict:
        """–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–º—ã: –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–∏—Å–∫–æ–≤, –∑–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤"""
        start = time.time()
        logger.info(f"üî¨ –ì–õ–£–ë–û–ö–û–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï: {topic}")
        if not questions:
            questions = self._generate_search_queries(topic)
        logger.info(f"üîé –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {questions}")

        search_tasks = [self.search(q, num_results=10) for q in questions[:5]]
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

        all_urls = []
        for res in search_results:
            if isinstance(res, dict) and res.get('success'):
                all_urls.extend([r['url'] for r in res['results']])

        unique_urls = list(dict.fromkeys(all_urls))
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(unique_urls)}")

        fetch_tasks = []
        for url in unique_urls[:10]:
            if not self._is_blacklisted(url):
                fetch_tasks.append(self.fetch_page_content(url, topic))
        pages = await asyncio.gather(*fetch_tasks, return_exceptions=True)

        valid_pages = [p for p in pages if isinstance(p, dict) and p.get('success')]
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(valid_pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

        all_chunks = []
        for page in valid_pages:
            chunks = self._extract_chunks(page.get('content', ''), page['url'])
            all_chunks.extend(chunks)

        unique_chunks = self._deduplicate_chunks(all_chunks)
        logger.info(f"üì¶ –ò–∑–≤–ª–µ—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_chunks)}")

        return {
            'success': True,
            'topic': topic,
            'pages_processed': len(valid_pages),
            'content_chunks': unique_chunks,
            'stats': {
                'total_chunks': len(all_chunks),
                'unique': len(unique_chunks),
                'time': time.time() - start if 'start' in locals() else 0
            }
        }

    async def _fetch_and_parse(self, url: str, query: str) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º trafilatura –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)"""
        try:
            html = await self._fetch_url(url)
            if not html:
                return {'success': False, 'url': url, 'error': 'fetch failed'}

            extracted = None

            # 1. –ü—Ä–æ–±—É–µ–º trafilatura
            if TRAFILATURA_AVAILABLE:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
                    text = trafilatura.extract(html, include_comments=False, include_tables=False, no_fallback=False)
                    if text and len(text) > 500:
                        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
                        title = title_match.group(1).strip() if title_match else ''
                        extracted = {
                            'title': title,
                            'content': text,
                            'language': 'ru' if re.search('[–∞-—è—ë]', text) else 'en',
                            'has_images': False,
                            'has_tables': False
                        }
                        logger.debug(f"‚úÖ trafilatura –∏–∑–≤–ª–µ–∫–ª–∞ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è {url}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è trafilatura error for {url}: {e}")

            # 2. –ï—Å–ª–∏ trafilatura –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –ø—Ä–æ–±—É–µ–º readability
            if not extracted and READABILITY_AVAILABLE:
                try:
                    doc = Document(html)
                    content = doc.summary()
                    title = doc.title()
                    # –û—á–∏—â–∞–µ–º –æ—Ç –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è HTML-—Ç–µ–≥–æ–≤
                    content = re.sub(r'<[^>]+>', ' ', content)
                    content = re.sub(r'\s+', ' ', content).strip()
                    if content and len(content) > 300:
                        extracted = {
                            'title': title,
                            'content': content,
                            'language': 'ru' if re.search('[–∞-—è—ë]', content) else 'en',
                            'has_images': False,
                            'has_tables': False
                        }
                        logger.debug(f"‚úÖ readability –∏–∑–≤–ª–µ–∫–ª–∞ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è {url}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è readability error for {url}: {e}")

            # 3. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à –ø–∞—Ä—Å–µ—Ä
            if not extracted:
                loop = asyncio.get_event_loop()
                extracted = await loop.run_in_executor(
                    None,
                    self._parse_html,
                    html, url
                )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_len = len(extracted.get('content', ''))
            min_len = self.config.get('min_content_length', 800)
            if content_len < min_len:
                logger.debug(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({content_len} < {min_len}): {url}")
                return {'success': False, 'url': url, 'error': f'content too short ({content_len})'}

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            quality = self._calc_quality(extracted, query)
            extracted['quality_score'] = quality

            return {
                'success': True,
                'url': url,
                'title': extracted.get('title', ''),
                'content': extracted.get('content', ''),
                'quality_score': quality,
                'metadata': {
                    'language': extracted.get('language', 'ru'),
                    'has_images': extracted.get('has_images', False),
                    'has_tables': extracted.get('has_tables', False),
                }
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            return {'success': False, 'url': url, 'error': str(e)}

    async def _fetch_url(self, url: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ URL —Å –∫—ç—à–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        key = hashlib.md5(url.encode()).hexdigest()
        if key in self.cache:
            if time.time() - self.cache[key]['timestamp'] < self.cache_ttl:
                self.stats['cache_hits'] += 1
                logger.debug(f"üîµ –ö—ç—à HIT: {url}")
                return self.cache[key]['content']
        self.stats['cache_misses'] += 1

        try:
            await asyncio.sleep(random.uniform(0.3, 0.7))
            headers = {'User-Agent': random.choice(self.user_agents)}
            async with self.session.get(url, headers=headers, ssl=False, timeout=15) as resp:
                if resp.status != 200:
                    logger.debug(f"‚ö†Ô∏è HTTP {resp.status} –¥–ª—è {url}")
                    return None
                try:
                    content = await resp.text()
                except UnicodeDecodeError:
                    raw = await resp.read()
                    content = raw.decode('utf-8', errors='ignore')
                self.cache[key] = {'content': content, 'timestamp': time.time()}
                self.stats['requests'] += 1
                logger.debug(f"üü¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {url} ({len(content)} –±–∞–π—Ç)")
                return content
        except asyncio.TimeoutError:
            logger.debug(f"‚è∞ –¢–∞–π–º–∞—É—Ç {url}")
            return None
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

    def _parse_html(self, html: str, url: str) -> Dict:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ BeautifulSoup —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥)"""
        result = {
            'title': '',
            'content': '',
            'language': 'ru',
            'has_images': False,
            'has_tables': False
        }
        try:
            soup = BeautifulSoup(html, 'lxml')

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ habr.com
            if 'habr.com' in url:
                article_body = soup.find('div', class_='article__body')
                if article_body:
                    for tag in article_body.find_all(['script', 'style', 'aside', 'div.comments',
                                                      'div.article__meta', 'div.post-meta', 'div.author-info',
                                                      'div.company-info', 'div.tags', 'div.hubs', 'div.stats',
                                                      'div.share', 'div.subscribe', 'div.banner', 'div.advertisement',
                                                      'div.recommendations', 'div.related', 'div.footer',
                                                      'div.article__footer', 'div.article__aside', 'div.article__header',
                                                      'div.voting', 'div.favs_count', 'div.views_count', 'div.time',
                                                      'span.time', 'span.views', 'span.comments', 'span.favs']):
                        tag.decompose()
                    paragraphs = []
                    for p in article_body.find_all(['p', 'li', 'blockquote', 'div.paragraph']):
                        text = p.get_text(strip=True)
                        if len(text) > 30:
                            paragraphs.append(text)
                    content = '\n\n'.join(paragraphs)
                    title = ''
                    for selector in ['h1', 'h2', 'h3', '.post__title', '.article__title', 'title', 'meta[property="og:title"]']:
                        elem = soup.select_one(selector)
                        if elem:
                            if elem.name == 'meta':
                                title = elem.get('content', '')
                            else:
                                title = elem.get_text().strip()
                            if title:
                                break
                    if not title and soup.title:
                        title = soup.title.string.strip()
                    if len(content) > 500000:
                        content = content[:500000]
                    result = {
                        'title': title[:200],
                        'content': content,
                        'language': 'ru',
                        'has_images': bool(soup.find_all('img')),
                        'has_tables': bool(soup.find_all('table'))
                    }
                    return result

            # –û–±—â–∞—è –æ—á–∏—Å—Ç–∫–∞
            for tag in soup(['script', 'style', 'noscript', 'iframe', 'nav', 'footer',
                           'header', 'aside', 'form', 'button', 'input', 'meta', 'link']):
                tag.decompose()

            for cls in ['comment', 'comments', 'sidebar', 'widget', 'advertisement',
                       'banner', 'popup', 'modal', 'cookie', 'newsletter', 'subscribe',
                       'share', 'social', 'menu', 'breadcrumb', 'pagination', 'related']:
                for elem in soup.find_all(class_=lambda c: c and cls in c.lower()):
                    elem.decompose()

            h1 = soup.find('h1')
            if h1:
                result['title'] = h1.get_text().strip()[:200]
            if not result['title'] and soup.title:
                result['title'] = soup.title.string.strip()[:200]

            for selector in ['article', 'main', 'div.content', 'div.article', 'div.post', '#content', '.content']:
                elem = soup.select_one(selector)
                if elem:
                    content = elem.get_text(separator='\n', strip=True)
                    break
            else:
                if soup.body:
                    lines = soup.body.get_text(separator='\n', strip=True).split('\n')
                    lines = [l.strip() for l in lines if len(l.strip()) > 40]
                    content = '\n'.join(lines)
                else:
                    content = ''

            # –û—á–∏—Å—Ç–∫–∞ –í–∏–∫–∏–ø–µ–¥–∏–∏
            if 'wikipedia.org' in url:
                lines = content.split('\n')
                skip = [
                    '–ú–∞—Ç–µ—Ä–∏–∞–ª –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏', '–°—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è', '–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è',
                    '–ü–µ—Ä–µ–π—Ç–∏ –∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏', '–ü–µ—Ä–µ–π—Ç–∏ –∫ –ø–æ–∏—Å–∫—É', '–°–∫—Ä—ã—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏',
                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è:', '–ò—Å—Ç–æ—á–Ω–∏–∫ ‚Äî', '–≠—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑',
                    '–õ–∏—Ü–µ–Ω–∑–∏—è Creative Commons', '–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏',
                    'ISBN', '–®–∞–±–ª–æ–Ω:', '–í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏', '–ü—Ä–∏–º–µ—á–∞–Ω–∏—è', '–°—Å—ã–ª–∫–∏',
                    '^ [0-9]+ ', '‚Üë', '‚Üì', '‚Üê', '‚Üí'
                ]
                for pattern in skip:
                    lines = [l for l in lines if pattern not in l]

                processed_lines = []
                for l in lines:
                    original = l
                    l = re.sub(r'\[\[[^\]]+\]\]', '', l)
                    l = re.sub(r'\{\{[^}]+\}\}', '', l)
                    l = re.sub(r'={2,}', '', l)
                    l = re.sub(r'^\*\s*', '', l)
                    l = re.sub(r'^#\s*', '', l)
                    l = re.sub(r'^\|\s*', '', l)

                    l = re.sub(r'\[\d+\]', '', l)
                    l = re.sub(r'\^\{\d+\}', '', l)
                    l = re.sub(r'\|\^?\{\d+\}', '', l)
                    l = re.sub(r'\s+\d+(?:\s+\d+)*\s*$', '', l)
                    l = re.sub(r'\s+\d+\.\d+\.\d+\s*', ' ', l)

                    l = re.sub(r'\\[a-zA-Z]+', ' ', l)
                    l = re.sub(r'[{}]', ' ', l)

                    l = l.strip()
                    if len(l) < 30:
                        if len(original.strip()) > 100:
                            l = original.strip()
                            l = re.sub(r'\[\d+\]', '', l)
                            l = re.sub(r'[{}]', '', l)
                            l = re.sub(r'\s+', ' ', l)
                            l = l.strip()
                            if len(l) >= 30:
                                processed_lines.append(l)
                        continue
                    processed_lines.append(l)
                lines = processed_lines

                seen = set()
                unique = []
                for l in lines:
                    norm = l[:100].lower()
                    if norm not in seen:
                        seen.add(norm)
                        unique.append(l)
                content = '\n'.join(unique)

            content = self.text_cleaner.clean(content)
            max_len = self.config.get('max_content_length', 100000)
            if len(content) > max_len:
                content = content[:max_len]

            result['content'] = content
            result['has_images'] = bool(soup.find_all('img'))
            result['has_tables'] = bool(soup.find_all('table'))
            result['language'] = 'ru' if re.search('[–∞-—è—ë]', content) else 'en'

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
        return result

    def _extract_chunks(self, content: str, source_url: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –æ—á–∏—Å—Ç–∫–æ–π —á–µ—Ä–µ–∑ TextCleaner"""
        if not content:
            return []

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É
        content = self.text_cleaner.clean(content)

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 100]

        if not paragraphs:
            sentences = re.split(r'(?<=[.!?])\s+', content)
            paragraphs = [s for s in sentences if len(s) > 80]

        chunks = []
        for i, para in enumerate(paragraphs[:8]):  # –º–∞–∫—Å–∏–º—É–º 8 —á–∞–Ω–∫–æ–≤
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º
            if len(para) < 50 or not any(c.isalpha() for c in para):
                continue
            chunks.append({
                'chunk_id': f"{hashlib.md5(source_url.encode()).hexdigest()[:8]}_{i}",
                'text': para[:8000],
                'source_url': source_url,
                'length': len(para)
            })
        return chunks

    def _deduplicate_chunks(self, chunks: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for c in chunks:
            sig = hashlib.md5(c['text'][:200].encode()).hexdigest()
            if sig not in seen:
                seen.add(sig)
                unique.append(c)
        return unique

    def _calc_quality(self, extracted: Dict, query: str) -> float:
        score = 0.0
        content = extracted.get('content', '')
        title = extracted.get('title', '')
        l = len(content)
        if l >= 3000:
            score += 0.4
        elif l >= 1500:
            score += 0.3
        elif l >= 800:
            score += 0.2
        elif l >= 300:
            score += 0.1
        if query.lower() in title.lower():
            score += 0.3
        elif any(w in title.lower() for w in query.lower().split()):
            score += 0.2
        return min(1.0, score)

    def _generate_search_queries(self, topic: str) -> List[str]:
        clean = topic.strip().rstrip('?')
        prefixes = ['—á—Ç–æ —Ç–∞–∫–æ–µ ', '–∫—Ç–æ —Ç–∞–∫–æ–π ', '–∫—Ç–æ —Ç–∞–∫–∞—è ', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ', '–±–∏–æ–≥—Ä–∞—Ñ–∏—è ']
        for p in prefixes:
            if clean.lower().startswith(p):
                clean = clean[len(p):].strip()
                break
        queries = [
            f"–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {clean}",
            f"—á—Ç–æ —Ç–∞–∫–æ–µ {clean}",
            f"{clean} –±–∏–æ–≥—Ä–∞—Ñ–∏—è",
            f"{clean} –≤–∏–∫–∏–ø–µ–¥–∏—è",
            f"{clean} –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
        ]
        parts = clean.split()
        if len(parts) >= 2:
            last_name = parts[-1]
            queries.append(f"{last_name} {parts[0]}")
        return list(dict.fromkeys(queries))[:5]

    def _is_blacklisted(self, url: str) -> bool:
        domain = urlparse(url).netloc.lower()
        for bad in self.blacklist:
            if bad in domain:
                return True
        return False

    async def _load_cache(self):
        try:
            f = os.path.join(self.cache_dir, 'cache.json')
            if os.path.exists(f):
                async with aiofiles.open(f, 'r', encoding='utf-8') as fp:
                    self.cache = json.loads(await fp.read())
                logger.info(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à –¥–µ—Ç–µ–∫—Ç–∏–≤–∞: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")

    async def _save_cache(self):
        try:
            f = os.path.join(self.cache_dir, 'cache.json')
            async with aiofiles.open(f, 'w', encoding='utf-8') as fp:
                await fp.write(json.dumps(self.cache, ensure_ascii=False, indent=2))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    async def clear_cache(self):
        self.cache.clear()
        await self._save_cache()
        logger.info("üßπ –ö—ç—à –¥–µ—Ç–µ–∫—Ç–∏–≤–∞ –æ—á–∏—â–µ–Ω")

    async def get_stats(self) -> Dict:
        return {
            'searches': self.stats['searches'],
            'pages_processed': self.stats['pages_processed'],
            'requests': self.stats['requests'],
            'cache_hits': self.stats['cache_hits'],
            'cache_misses': self.stats['cache_misses'],
            'hit_rate': self.stats['cache_hits'] / max(1, self.stats['cache_hits'] + self.stats['cache_misses']),
            'errors': self.stats['errors'],
            'avg_response_time': self.stats['avg_response_time'],
            'unique_domains': len(self.stats['unique_domains'])
        }

    async def health_check(self) -> Dict:
        try:
            async with self.session.get('https://www.google.com', timeout=5) as resp:
                return {'healthy': resp.status == 200}
        except:
            return {'healthy': False}

    async def cleanup(self):
        if self.session:
            await self.session.close()
        await self._save_cache()
        logger.info("‚úÖ Detective –∑–∞–≤–µ—Ä—à—ë–Ω")