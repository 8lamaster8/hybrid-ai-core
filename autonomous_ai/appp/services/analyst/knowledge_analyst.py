"""
üìö –ê–ù–ê–õ–ò–¢–ò–ö –ó–ù–ê–ù–ò–ô ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ + —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ + —à–∞–±–ª–æ–Ω—ã
"""

import asyncio
import re
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from appp.core.logging import logger
from appp.utils.text_processor import TextCleaner
from appp.services.ranking.ranking_service import RankingService, Fact

# –î–ª—è NER (GLiNER2)
try:
    from gliner2 import GLiNER2
    GLINER_AVAILABLE = True
except ImportError:
    GLINER_AVAILABLE = False
    logger.warning("‚ö†Ô∏è GLiNER2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, NER –æ—Ç–∫–ª—é—á—ë–Ω")

# –î–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
try:
    from sentence_transformers import util
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logger.warning("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")


class KnowledgeAnalyst:
    def __init__(self, config: Dict):
        self.config = config
        self.chunk_size = config.get('chunk_size', 1500)
        self.chunk_overlap = config.get('chunk_overlap', 300)
        self.min_chunk_length = config.get('min_chunk_length', 500)
        self.max_chunks_per_document = config.get('max_chunks_per_document', 50)
        self.enable_summarization = config.get('enable_summarization', True)
        self.language = config.get('language', 'ru')
        self.min_confidence = config.get('min_confidence', 0.6)
        self._last_facts_metadata = []

        self.text_cleaner = TextCleaner()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER (GLiNER2)
        self.ner_enabled = config.get('enable_ner', True) and GLINER_AVAILABLE
        self.ner_model_name = config.get('ner_model', 'fastino/gliner2-base-v1')
        
        if self.ner_enabled:
            try:
                self.gliner_model = GLiNER2.from_pretrained(self.ner_model_name)
                logger.info(f"üß† GLiNER2 NER –∑–∞–≥—Ä—É–∂–µ–Ω (–º–æ–¥–µ–ª—å: {self.ner_model_name})")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GLiNER2: {e}")
                self.ner_enabled = False

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –º–∏—Ä—ã (–ö–≤–∞–Ω—Ç–º–µ—Ö, –§–∏–ª–æ—Å–æ—Ñ–∏—è –∏ —Ç.–¥.)
        # GLiNER2 –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ
        self.ner_competencies = {
            'scientific_concept': {
                'labels': ["Scientific Law", "Quantum Phenomenon", "Hypothesis", "Chemical Compound", "Scientist"],
                'weight': 15.0  # –ù–∞—É–∫–∞ –≤ –Ω–∞—É—á–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ1
            },
            'quantum_physics_deep': {
                'labels': ["Quantum Phenomenon", "Scientific Law", "Physicist"],
                'weight': 14.0
            },
            'philosophical_concept': {
                'labels': ["Philosophical Doctrine", "Subjective Experience", "Thinker", "Ontological Term"],
                'weight': 15.0
            },
            'mathematical_theorem': {
                'labels': ["Mathematical Theorem", "Axiom", "Mathematical Notation", "Formula"],
                'weight': 14.0
            },
            'biological_system': {
                'labels': ["Biological Mechanism", "Anatomical Structure", "Process", "Species"],
                'weight': 13.0
            },
            'programming_concept': {
                'labels': ["Programming Language", "Algorithm", "Framework", "Library"],
                'weight': 11.0
            }
        }

        # –ë–∞–∑–æ–≤—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –æ–±—â–µ–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.base_mapping = {
            'person': 10.0, 'PER': 10.0,
            'location': 8.0, 'LOC': 8.0,
            'organization': 8.0, 'ORG': 8.0,
            'event': 7.0, 'date': 5.0
        }

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–æ–º–µ–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∫–æ–º–∏—Ç–µ—Ç–∞
        self.priority_domains = set(config.get('priority_domains', []))
        self.low_trust_domains = set(config.get('low_trust_domains', []))

        self.ranking_service = RankingService()
        self.stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'errors': 0,
            'ner_used': 0
        }
        logger.info("üìö KnowledgeAnalyst —Å–æ–∑–¥–∞–Ω —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º NER")

    async def initialize(self):
        return True

    # ----------------------------------------------------------------------
    # –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –ê–ù–ê–õ–ò–ó–ê (—Å –≥–ª–æ–±–∞–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)
    # ----------------------------------------------------------------------
    async def analyze(self, documents: List[Dict], query: str = "", is_discovery: bool = False) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
        - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        - –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        - –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø (–∫—Ä–æ–º–µ discovery)
        - —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ RankingService
        - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        - —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        """
        start_time = datetime.now()

        unique_chunks = []
        fact_objects = []
        ranked = []
        key_points = []
        summary = ""
        top_facts = []
        profile_name = self._detect_query_profile(query)

        try:
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            all_chunks = []
            for doc in documents:
                doc_result = await self._process_document(doc)
                all_chunks.extend(doc_result.get('chunks', []))

            unique_chunks = self._deduplicate_chunks(all_chunks)
            logger.info(f"   ‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(unique_chunks)}")

            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã (–ø–µ—Ä–µ–¥–∞—ë–º is_discovery)
            fact_objects = await self._extract_key_facts(
                chunks=unique_chunks,
                query=query,
                top_k=50,
                is_discovery=is_discovery
            )
            logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤ –î–û –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(fact_objects)}")

            # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ discovery)
            if not is_discovery and len(fact_objects) > 10 and SENTENCE_TRANSFORMERS_AVAILABLE:
                fact_objects = await self._semantic_deduplication(fact_objects, threshold=0.85)
                logger.info(f"   ‚úÖ –ü–æ—Å–ª–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(fact_objects)} —Ñ–∞–∫—Ç–æ–≤")

            # 4. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (–≤—Å–µ–≥–¥–∞)
            if fact_objects:
                profile_name = self._detect_query_profile(query)
                facts_for_ranking = []
                for fo in fact_objects:
                    fact = Fact(
                        text=fo['text'],
                        source_domain=fo.get('domain', ''),
                        position_ratio=fo.get('position_ratio', 0.5),
                        ner_score=fo.get('ner_score', 0.0),
                        ner_types=fo.get('ner_types', []),
                        length=fo.get('length', len(fo['text'])),
                        contains_definition=fo.get('contains_definition', False),
                        contains_causal=fo.get('contains_causal', False)
                    )
                    facts_for_ranking.append(fact)

                query_emb = await self._get_query_embedding(query)
                ranked = await self.ranking_service.rank(
                    query=query,
                    facts=facts_for_ranking,
                    query_embedding=query_emb,
                    priority_domains=self.priority_domains,
                    low_trust_domains=self.low_trust_domains,
                    profile_name=profile_name
                )
                logger.info(f"   ‚úÖ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, —Ñ–∞–∫—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–Ω–≥–∞: {len(ranked)}")

                top_facts = [fact for fact, score in ranked[:15]]
                key_points = [fact.text for fact in top_facts]
                self._last_facts_metadata = top_facts[:15] if top_facts else []

            # 5. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            if self.enable_summarization and unique_chunks:
                best_chunk = max(unique_chunks, key=lambda x: x.get('quality_score', 0))
                if best_chunk and best_chunk.get('text'):
                    summary = self._generate_extractive_summary(best_chunk['text'], sentences_count=3)

            # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = self._calculate_confidence(unique_chunks, fact_objects, query)

            processing_time = (datetime.now() - start_time).total_seconds()

            self.stats['documents_processed'] += len(documents)
            self.stats['chunks_created'] += len(unique_chunks)

            return {
                'success': True,
                'documents_count': len(documents),
                'summary': summary,
                'key_points': key_points[:15],
                'key_facts_metadata': top_facts[:15] if top_facts else [],
                'profile': profile_name,
                'query': query,
                'confidence': confidence,
                'processing_time': processing_time
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}", exc_info=True)
            self.stats['errors'] += 1
            return {
                'success': False,
                'error': str(e),
                'summary': '',
                'key_points': []
            }

    # ----------------------------------------------------------------------
    # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø
    # ----------------------------------------------------------------------
    async def _semantic_deduplication(self, facts: List[Dict], threshold: float = 0.85) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É) –∏–∑ —Å–ø–∏—Å–∫–∞ —Ñ–∞–∫—Ç–æ–≤."""
        if len(facts) < 2:
            return facts
        
        try:
            from appp.services.embedding.bge_m3 import embedder
            
            texts = [f['text'] for f in facts]
            embeddings = await embedder.embed(texts)
            
            if not embeddings or len(embeddings) != len(facts):
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏")
                return facts
            
            unique_facts = []
            unique_embeddings = []
            
            for i, (fact, emb) in enumerate(zip(facts, embeddings)):
                is_duplicate = False
                for unique_emb in unique_embeddings:
                    try:
                        similarity = util.cos_sim(emb, unique_emb).item()
                        if similarity > threshold:
                            is_duplicate = True
                            break
                    except:
                        pass
                
                if not is_duplicate:
                    unique_facts.append(fact)
                    unique_embeddings.append(emb)
            
            logger.debug(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(facts)} -> {len(unique_facts)}")
            return unique_facts
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return facts

    # ----------------------------------------------------------------------
    # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –§–ê–ö–¢–û–í –° –ú–ï–¢–ê–î–ê–ù–ù–´–ú–ò (–° –ñ–Å–°–¢–ö–û–ô –§–ò–õ–¨–¢–†–ê–¶–ò–ï–ô)
    # ----------------------------------------------------------------------
    async def _extract_key_facts(
        self,
        chunks: List[Dict],
        query: str,
        top_k: int = 50,
        is_discovery: bool = False
    ) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ —á–∞–Ω–∫–æ–≤ –∏ –æ–±–æ–≥–∞—â–∞–µ—Ç –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
        if not chunks:
            return []

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞)
        keywords = [w.lower() for w in re.findall(r'\b\w{4,}\b', query)
                    if w.lower() not in {'–∫–æ–≥–¥–∞','—á—Ç–æ','–∫–∞–∫','–≥–¥–µ','–ø–æ—á–µ–º—É','–∑–∞—á–µ–º',
                                        '–∫–∞–∫–æ–π','–∫–∞–∫–∞—è','–∫–∞–∫–∏–µ','–∫—Ç–æ'}]

        all_facts = []
        junk_phrases = self._get_junk_phrases()

        for chunk in chunks:
            text = chunk.get('text', '')
            if not text:
                continue

            sentences = re.split(r'(?<=[.!?])\s+', text)
            source_url = chunk.get('source_url', '')
            domain = self._extract_domain(source_url)

            chunk_index = chunk.get('chunk_index', 0)
            total_chunks = chunk.get('total_chunks', 1)
            position_ratio = chunk_index / max(total_chunks, 1)

            for sent in sentences:
                sent = sent.strip()
                sent_lower = sent.lower()
                
                # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è ---
                if is_discovery:
                    # –ú—è–≥–∫–∏–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è discovery
                    if len(sent) < 20:
                        continue
                    if re.search(r'[{}\[\]<>]', sent) and len(re.findall(r'[{}\[\]<>]', sent)) > 5:
                        continue
                    if any(phrase in sent_lower for phrase in junk_phrases):
                        continue
                    if re.search(r'https?://|www\.', sent):
                        continue
                else:
                    # –ñ—ë—Å—Ç–∫–∏–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    if len(sent) < 40:
                        continue
                    if re.search(r'[{}\[\]<>]', sent) and len(re.findall(r'[{}\[\]<>]', sent)) > 3:
                        continue
                    alpha_ratio = sum(c.isalpha() for c in sent) / len(sent)
                    if alpha_ratio < 0.5:
                        continue
                    if sent[0].islower() and len(sent) < 100:
                        if not any(word in sent_lower for word in ['—è–≤–ª—è–µ—Ç—Å—è', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–∏–º–µ–µ—Ç', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ']):
                            continue
                    words = sent.split()
                    if len(words) < 5:
                        continue
                    long_words = [w for w in words if len(w) > 3]
                    if len(long_words) < 2:
                        continue
                    if any(phrase in sent_lower for phrase in junk_phrases):
                        continue
                    if sent.startswith(('[[', ']]', '{{', '}}', '==', '*', '#', '|', ';', ':', '^')):
                        continue
                    if re.search(r'https?://|www\.', sent):
                        continue
                    if re.match(r'^[–ê-–ØA-Z][^.]*:', sent):
                        continue
                    if re.match(r'^[^‚Äî]{1,30} ‚Äî', sent):
                        continue
                    if re.match(r'^[IVX]+\.|^[A-Z–ê-–Ø]\.', sent):
                        continue
                    if len(words) >= 3:
                        capitalized = sum(1 for w in words[1:] if w and w[0].isupper())
                        if capitalized / max(len(words)-1, 1) > 0.5:
                            continue
                    if sent[-1] not in {'.', '!', '?'}:
                        continue

                # --- –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (–æ–¥–∏–Ω–∞–∫–æ–≤–æ –¥–ª—è –æ–±–æ–∏—Ö —Ä–µ–∂–∏–º–æ–≤) ---
                ner_score, ner_types = await self._compute_ner_features(sent, query)
                length = len(sent)
                contains_definition = bool(re.search(r'‚Äî| —ç—Ç–æ | —è–≤–ª—è–µ—Ç—Å—è |–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç', sent))
                contains_causal = bool(re.search(r'–ø–æ—Ç–æ–º—É —á—Ç–æ|—Ç–∞–∫ –∫–∞–∫|—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ|–ø–æ—ç—Ç–æ–º—É|–∏–∑-–∑–∞|–≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ', sent_lower))

                base_score = 0.0
                if re.search(r'\b\d{4}\b', sent):
                    base_score += 3.0
                if re.search(r'\b\d+\b', sent):
                    base_score += 1.0
                for kw in keywords:
                    if kw in sent_lower:
                        base_score += 1.0

                total_score = base_score + ner_score

                fact_entry = {
                    'text': sent,
                    'domain': domain,
                    'source_url': source_url,
                    'position_ratio': position_ratio,
                    'ner_score': ner_score,
                    'ner_types': ner_types,
                    'length': length,
                    'contains_definition': contains_definition,
                    'contains_causal': contains_causal,
                    'base_score': base_score,
                    'total_score': total_score,
                    'chunk_id': chunk.get('chunk_id', '')
                }
                all_facts.append(fact_entry)

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É (–≤—Å–µ–≥–¥–∞)
        unique_facts = self._deduplicate_facts_by_text(all_facts)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ total_score
        unique_facts.sort(key=lambda x: x['total_score'], reverse=True)

        return unique_facts[:top_k]

    # ----------------------------------------------------------------------
    # –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –®–ê–ë–õ–û–ù–û–í –° –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ï–ô
    # ----------------------------------------------------------------------
    def _prepare_template_data(self, profile: str, facts_metadata: List[Fact], query: str) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ —Ñ–∞–∫—Ç–æ–≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ —Å –∂—ë—Å—Ç–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –º–µ–∂–¥—É –ø–æ–ª—è–º–∏."""
        data = {
            'query': query,
            'default_answer': '\n'.join([f.text for f in facts_metadata[:10]])
        }
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        used_texts = set()
        
        def get_unique_facts(facts_list: List[Fact], max_count: int, seen_set: set) -> List[str]:
            """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ –±—ã–ª–æ –≤ seen_set"""
            unique = []
            for fact in facts_list:
                norm_text = ' '.join(fact.text.lower().split())
                
                is_duplicate = False
                for used in seen_set:
                    if len(norm_text) > 50 and len(used) > 50:
                        shorter = min(len(norm_text), len(used))
                        longer = max(len(norm_text), len(used))
                        if shorter / longer > 0.7:
                            words1 = set(norm_text.split())
                            words2 = set(used.split())
                            intersection = words1 & words2
                            if len(intersection) / max(len(words1), len(words2)) > 0.6:
                                is_duplicate = True
                                break
                
                if not is_duplicate and len(unique) < max_count:
                    unique.append(fact.text)
                    seen_set.add(norm_text)
            return unique

        if profile == 'mathematical_theorem':
            data['theorem_name'] = query.strip().rstrip('?').replace('—Ç–µ–æ—Ä–µ–º–∞', '').strip()
            
            definitions = get_unique_facts(
                [f for f in facts_metadata if f.contains_definition], 3, used_texts
            )
            data['statement'] = '\n'.join(definitions) if definitions else '–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.'
            
            named_facts = get_unique_facts(
                [f for f in facts_metadata if 'PER' in f.ner_types or 'DATE' in f.ner_types], 2, used_texts
            )
            data['historical_context'] = ' '.join(named_facts) if named_facts else '–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.'
            
            formula_facts = get_unique_facts(
                [f for f in facts_metadata if re.search(r'[=+\-*/^(){}]', f.text)], 1, used_texts
            )
            data['formulation'] = formula_facts[0] if formula_facts else ''
            
            proof_facts = get_unique_facts(facts_metadata, 1, used_texts)
            data['proof_summary'] = proof_facts[0] if proof_facts else ''
            
            applications = get_unique_facts(facts_metadata, 2, used_texts)
            data['applications'] = '\n'.join(applications) if applications else ''
            
            related = get_unique_facts(facts_metadata, 2, used_texts)
            data['related_concepts'] = '\n'.join(related) if related else ''

        elif profile == 'historical_event':
            data['event_name'] = query.strip().rstrip('?')
            
            dates_facts = get_unique_facts(
                [f for f in facts_metadata if 'DATE' in f.ner_types], 5, used_texts
            )
            data['timeline'] = '\n'.join(dates_facts) if dates_facts else '–•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.'
            data['key_dates'] = '\n'.join(dates_facts[:3]) if dates_facts else '–ù–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            people_facts = get_unique_facts(
                [f for f in facts_metadata if 'PER' in f.ner_types], 5, used_texts
            )
            data['key_figures'] = '\n'.join(people_facts) if people_facts else '–£—á–∞—Å—Ç–Ω–∏–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            causes_facts = get_unique_facts(
                [f for f in facts_metadata if f.contains_causal], 3, used_texts
            )
            data['causes'] = '\n'.join(causes_facts) if causes_facts else '–ü—Ä–∏—á–∏–Ω—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            other_facts = get_unique_facts(
                [f for f in facts_metadata if not ('DATE' in f.ner_types or 'PER' in f.ner_types or f.contains_causal)], 5, used_texts
            )
            data['consequences'] = other_facts[0] if other_facts else '–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            data['interesting_facts'] = '\n'.join(other_facts[1:4]) if len(other_facts) > 1 else ''

        elif profile == 'programming_concept':
            data['concept_name'] = query.strip().rstrip('?')
            
            lang = 'python'
            for f in facts_metadata:
                if 'python' in f.text.lower():
                    lang = 'python'
                elif 'java' in f.text.lower():
                    lang = 'java'
                elif 'c++' in f.text.lower() or 'cpp' in f.text.lower():
                    lang = 'cpp'
            data['language'] = lang
            
            def_fact = get_unique_facts(
                [f for f in facts_metadata if f.contains_definition], 1, used_texts
            )
            data['definition'] = def_fact[0] if def_fact else '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.'
            
            code_candidates = get_unique_facts(
                [f for f in facts_metadata if re.search(r'[=;{}\[\]()]', f.text)], 2, used_texts
            )
            data['syntax_example'] = code_candidates[0] if code_candidates else '–ü—Ä–∏–º–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω.'
            data['practical_example'] = code_candidates[1] if len(code_candidates) > 1 else data['syntax_example']
            
            remaining = get_unique_facts(facts_metadata, 4, used_texts)
            data['use_cases'] = remaining[0] if len(remaining) > 0 else ''
            data['advantages'] = remaining[1] if len(remaining) > 1 else ''
            data['disadvantages'] = remaining[2] if len(remaining) > 2 else ''
            data['alternatives'] = remaining[3] if len(remaining) > 3 else ''

        elif profile == 'scientific_concept':
            data['concept_name'] = query.strip().rstrip('?')
            
            def_facts = get_unique_facts(
                [f for f in facts_metadata if f.contains_definition], 2, used_texts
            )
            data['scientific_definition'] = '\n'.join(def_facts) if def_facts else '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.'
            
            principles = get_unique_facts(
                [f for f in facts_metadata if f.contains_causal or 'PER' in f.ner_types], 3, used_texts
            )
            data['principles'] = '\n'.join(principles) if principles else '–ü—Ä–∏–Ω—Ü–∏–ø—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            formula_facts = get_unique_facts(
                [f for f in facts_metadata if re.search(r'[=+\-*/^(){}]', f.text)], 2, used_texts
            )
            data['mathematical_description'] = '\n'.join(formula_facts) if formula_facts else '–ù–µ –Ω–∞–π–¥–µ–Ω–æ.'
            
            exp_facts = get_unique_facts(
                [f for f in facts_metadata if 'DATE' in f.ner_types or 'PERCENT' in f.ner_types], 2, used_texts
            )
            data['experimental_evidence'] = '\n'.join(exp_facts) if exp_facts else '–ù–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            remaining = get_unique_facts(facts_metadata, 4, used_texts)
            data['application_domains'] = remaining[0] if len(remaining) > 0 else ''
            data['current_state'] = remaining[1] if len(remaining) > 1 else ''

        elif profile == 'factoid':
            short_answer_facts = get_unique_facts(facts_metadata, 1, used_texts)
            data['short_answer'] = short_answer_facts[0] if short_answer_facts else '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.'
            
            bullet_facts = get_unique_facts(facts_metadata, 10, used_texts)
            data['bullet_points'] = bullet_facts
            
            sources = set()
            for f in facts_metadata[:15]:
                if f.source_domain:
                    sources.add(f.source_domain)
            data['sources'] = list(sources)[:5]

        elif profile == 'how_why':
            causal_facts = get_unique_facts(
                [f for f in facts_metadata if f.contains_causal], 5, used_texts
            )
            data['explanations'] = '\n'.join(causal_facts) if causal_facts else '–û–±—ä—è—Å–Ω–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.'
            
            mechanism_keywords = ['—Ä–∞–±–æ—Ç–∞–µ—Ç', '–ø—Ä–æ—Ü–µ—Å—Å', '—ç—Ç–∞–ø', '—à–∞–≥', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç']
            mechanism = get_unique_facts(
                [f for f in facts_metadata if any(kw in f.text.lower() for kw in mechanism_keywords)], 3, used_texts
            )
            data['mechanism'] = '\n'.join(mechanism) if mechanism else '–ú–µ—Ö–∞–Ω–∏–∑–º –Ω–µ –æ–ø–∏—Å–∞–Ω.'
            
            factors = get_unique_facts(
                [f for f in facts_metadata if '–≤–ª–∏—è' in f.text.lower() or '—Ñ–∞–∫—Ç–æ—Ä' in f.text.lower() or '–ø—Ä–∏—á–∏–Ω' in f.text.lower()], 3, used_texts
            )
            data['factors'] = '\n'.join(factors) if factors else '–§–∞–∫—Ç–æ—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            other = get_unique_facts(facts_metadata, 3, used_texts)
            data['additional_info'] = '\n'.join(other)

        elif profile == 'evaluation':
            comparative = get_unique_facts(
                [f for f in facts_metadata if '–ª—É—á—à–µ' in f.text.lower() or '—Ö—É–∂–µ' in f.text.lower() or '–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è' in f.text.lower() or '—Å—Ä–∞–≤–Ω' in f.text.lower()], 3, used_texts
            )
            data['comparison'] = '\n'.join(comparative) if comparative else '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.'
            
            advantages = get_unique_facts(
                [f for f in facts_metadata if '–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤' in f.text.lower() or '–¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤' in f.text.lower() or '–ø–ª—é—Å' in f.text.lower()], 3, used_texts
            )
            data['advantages'] = '\n'.join(advantages) if advantages else '–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            disadvantages = get_unique_facts(
                [f for f in facts_metadata if '–Ω–µ–¥–æ—Å—Ç–∞—Ç' in f.text.lower() or '–º–∏–Ω—É—Å' in f.text.lower() or '–ø—Ä–æ–±–ª–µ–º' in f.text.lower()], 3, used_texts
            )
            data['disadvantages'] = '\n'.join(disadvantages) if disadvantages else '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã.'
            
            recommendations = get_unique_facts(
                [f for f in facts_metadata if '—Ä–µ–∫–æ–º–µ–Ω–¥' in f.text.lower() or '—Å–æ–≤–µ—Ç' in f.text.lower() or '—Å–ª–µ–¥—É–µ—Ç' in f.text.lower()], 2, used_texts
            )
            data['recommendations'] = '\n'.join(recommendations) if recommendations else ''

        else:  # profile == 'default' –∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π
            summary_facts = get_unique_facts(facts_metadata, 1, used_texts)
            data['summary'] = summary_facts[0] if summary_facts else '–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.'
            
            details_facts = get_unique_facts(facts_metadata, 5, used_texts)
            data['details'] = '\n'.join(details_facts) if details_facts else '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.'
            
            extra_facts = get_unique_facts(facts_metadata, 4, used_texts)
            data['extra'] = '\n'.join(extra_facts) if extra_facts else ''

        return data

    # ----------------------------------------------------------------------
    # NER –ü–†–ò–ó–ù–ê–ö–ò (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GLiNER2)
    # ----------------------------------------------------------------------
    async def _compute_ner_features(self, sent: str, query: str) -> Tuple[float, List[str]]:
        """
        –£–º–Ω—ã–π NER: –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ —Å–º—ã—Å–ª –≤–æ–ø—Ä–æ—Å–∞.
        """
        if not self.ner_enabled or not hasattr(self, 'gliner_model'):
            return 0.0, []

        try:
            # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å —á–µ—Ä–µ–∑ –¥–µ—Ç–µ–∫—Ç–æ—Ä
            profile = self._detect_query_profile(query)
            
            # 2. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –ø–æ–¥ —ç—Ç–æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            competency = self.ner_competencies.get(profile, {})
            specific_labels = competency.get('labels', ["Concept", "Object"])
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ (–ë–∞–∑–∞ + –°–ø–µ—Ü–∏—Ñ–∏–∫–∞)
            # –ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: 5-7 –º–µ—Ç–æ–∫ ‚Äî —ç—Ç–æ –∏–¥–µ–∞–ª –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            current_labels = list(self.base_mapping.keys())[:3] + specific_labels
            
            # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.to_thread –¥–ª—è GLiNER2)
            entities = await asyncio.to_thread(
                self.gliner_model.extract_entities, 
                sent, 
                current_labels, 
                threshold=0.45
            )

            ner_score = 0.0
            ner_types = []

            for ent in entities:
                label = ent['label']
                score = ent['score']
                
                # –°—á–∏—Ç–∞–µ–º –≤–µ—Å: –µ—Å–ª–∏ –º–µ—Ç–∫–∞ –∏–∑ "—É–º–Ω–æ–≥–æ" —Å–ø–∏—Å–∫–∞ ‚Äî –¥–∞–µ–º —Å–ø–µ—Ü. –≤–µ—Å, –∏–Ω–∞—á–µ –±–∞–∑—É
                weight = competency.get('weight', 10.0) if label in specific_labels else self.base_mapping.get(label, 4.0)
                
                ner_score += weight * score
                ner_types.append(label)

            self.stats['ner_used'] += 1
            return ner_score, list(set(ner_types))

        except Exception as e:
            logger.debug(f"GLiNER2 error: {e}")
            return 0.0, []

    # ----------------------------------------------------------------------
    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê –ó–ê–ü–†–û–°–ê (–ü–†–û–§–ò–õ–¨ –î–õ–Ø –®–ê–ë–õ–û–ù–û–í)
    # ----------------------------------------------------------------------
    def _detect_query_profile(self, query: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–∫ NER –∏ —à–∞–±–ª–æ–Ω–∞ –æ—Ç–≤–µ—Ç–∞.
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Å–º–µ–∂–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ (–±–∏–æ–ª–æ–≥–∏—è/—Ñ–∏–∑–∏–∫–∞), —á—Ç–æ–±—ã GLiNER2 –Ω–µ –ø—É—Ç–∞–ª—Å—è.
        """
        q = query.lower()

        # 1. –§–∏–ª–æ—Å–æ—Ñ–∏—è –∏ –°–æ–∑–Ω–∞–Ω–∏–µ (–ö–≤–∞–ª–∏–∞, –û–Ω—Ç–æ–ª–æ–≥–∏—è)
        if re.search(r'—Ñ–∏–ª–æ—Å–æ—Ñ|—Å–æ–∑–Ω–∞–Ω|–∫–≤–∞–ª–∏–∞|—Å—É–±—ä–µ–∫—Ç–∏–≤–Ω|–æ–Ω—Ç–æ–ª–æ–≥|—ç–ø–∏—Å—Ç–µ–º|—Ñ–µ–Ω–æ–º–µ–Ω–æ–ª–æ–≥|–±—ã—Ç–∏–µ|—Å–º—ã—Å–ª|—ç—Ç–∏–∫[–∞]|–∏–Ω—Ç–µ–Ω—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç|–Ω–∏—Ü—à–µ|–∫–∞–Ω—Ç|—Ö–∞–π–¥–µ–≥–≥–µ—Ä|–∞–ø—Ä–∏–æ—Ä–Ω', q):
            return 'philosophical_concept'

        # 2. –ì–ª—É–±–æ–∫–∞—è –ö–≤–∞–Ω—Ç–æ–≤–∞—è –§–∏–∑–∏–∫–∞ –∏ –ú–µ—Ö–∞–Ω–∏–∫–∞
        if re.search(r'–∫–≤–∞–Ω—Ç–æ–≤[–∞—è–∏]|–¥–µ–∫–æ–≥–µ—Ä–µ–Ω—Ü|—Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü|—à—Ä–µ–¥–µ—Ä|–≤–∏–≥–Ω–µ—Ä|–≤–æ–ª–Ω–æ–≤–æ–π\s+—Ñ—É–Ω–∫—Ü|–∫–æ—Ä–ø—É—Å–∫—É–ª—è—Ä|–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç|–≥–∞–º–∏–ª—å—Ç–æ–Ω–∏–∞–Ω|–∫—Ö–¥|—Å—Ç—Ä—É–Ω|—ç–π–Ω—à—Ç–µ–π–Ω', q):
            return 'quantum_physics_deep'

        # 3. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –§–æ—Ä–º–∞–ª–∏–∑–º
        if re.search(r'—Ç–µ–æ—Ä–µ–º[–∞—ã]|–ø–∏—Ñ–∞–≥–æ—Ä|—ç–π–ª–µ—Ä|—Ñ–µ—Ä–º–∞|–≥–∏–ª—å–±–µ—Ä—Ç|–∞–∫—Å–∏–æ–º[–∞—ã]|—Ñ–æ—Ä–º—É–ª[–∞—ã]|–∏–Ω—Ç–µ–≥—Ä–∞–ª|–ø—Ä–æ–∏–∑–≤–æ–¥–Ω|–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü|–ª–æ–≥–∞—Ä–∏—Ñ–º|—á–∏—Å–ª–æ\s+–ø–∏', q):
            return 'mathematical_theorem'

        # 4. –ë–∏–æ–ª–æ–≥–∏—è, –ì–µ–Ω–µ—Ç–∏–∫–∞ –∏ –°–ª–æ–∂–Ω—ã–µ –°–∏—Å—Ç–µ–º—ã
        if re.search(r'–±–∏–æ–ª–æ–≥–∏[—è–∏]|–≥–µ–Ω–µ—Ç|—Å–∏–Ω–∞–ø|–Ω–µ–π—Ä–æ–Ω|—ç–≤–æ–ª—é—Ü|–¥–Ω–∫|—Ä–Ω–∫|—Å–∏–º–±–∏–æ–∑|—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü|–±–µ–ª–æ–∫|—Ñ–µ—Ä–º–µ–Ω—Ç|–º–∏—Ç–æ—Ö–æ–Ω–¥—Ä', q):
            return 'biological_system'

        # 5. –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ / IT / –ò–ò
        if re.search(r'–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä|—è–∑—ã–∫\s+–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä|—Ñ—É–Ω–∫—Ü[–∏—è]|–∫–ª–∞—Å—Å|–∞–ª–≥–æ—Ä–∏—Ç–º|–±–∏–±–ª–∏–æ—Ç–µ–∫[–∞–∏]|python|java|c\+\+|javascript|—Ñ—Ä–µ–π–º–≤–æ—Ä–∫|–Ω–µ–π—Ä–æ—Å–µ—Ç—å|–æ–±—É—á–µ–Ω–∏–µ\s+–º–æ–¥–µ–ª–∏', q):
            return 'programming_concept'

        # 6. –û–±—â–∏–µ –Ω–∞—É—á–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (–§–∏–∑–∏–∫–∞, –•–∏–º–∏—è, –ö–æ—Å–º–æ—Å)
        if re.search(r'—Ñ–∏–∑–∏–∫[–∞]|—Ö–∏–º–∏[—è]|–∫–æ—Å–º–æ—Å|–∞—Å—Ç—Ä–æ–Ω|—Ç–µ–æ—Ä–∏[—è]|–∑–∞–∫–æ–Ω|–ø—Ä–∏–Ω—Ü–∏–ø|–≥–∏–ø–æ—Ç–µ–∑[–∞]|–ø–∞—Ä–∞–¥–æ–∫—Å|—ç—Ñ—Ñ–µ–∫—Ç|—Ä–µ–∞–∫—Ü–∏—è', q):
            return 'scientific_concept'

        # 7. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è / –õ–∏—á–Ω–æ—Å—Ç–∏
        if re.search(r'–∏—Å—Ç–æ—Ä–∏[—è–∏]|—Å–æ–±—ã—Ç–∏[–µ—è]|–≤–æ–π–Ω[–∞—ã]|—Ä–µ–≤–æ–ª—é—Ü[–∏—è]|–±–∏–æ–≥—Ä–∞—Ñ–∏[—è–∏]|—Ä–æ–¥–∏–ª—Å—è|—É–º–µ—Ä|–≥–æ–¥—É|–≤–µ–∫[–∞]|–∏–º–ø–µ—Ä–∏—è|—Ü–∞—Ä—å|–∫–æ—Ä–æ–ª—å', q):
            return 'historical_event'

        # 8. –¢–∏–ø–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã (Factoid / How-Why / Evaluation)
        if re.search(r'\b–∫—Ç–æ\b|\b–≥–¥–µ\b|\b–∫–æ–≥–¥–∞\b|\b—á—Ç–æ —Ç–∞–∫–æ–µ\b', q):
            return 'factoid'
        
        if re.search(r'\b–∫–∞–∫\b|\b–ø–æ—á–µ–º—É\b|\b–∑–∞—á–µ–º\b|\b–ø—Ä–∏—á–∏–Ω–∞\b', q):
            return 'how_why'

        if re.search(r'\b–ª—É—á—à–µ\b|\b—Ö—É–∂–µ\b|\b—Å—Ç–æ–∏—Ç –ª–∏\b|\b—Å—Ä–∞–≤–Ω–∏—Ç—å\b|\b–æ—Ç–ª–∏—á–∏–µ\b', q):
            return 'evaluation'

        return 'default'

    # ----------------------------------------------------------------------
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ----------------------------------------------------------------------
    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL."""
        from urllib.parse import urlparse
        try:
            return urlparse(url).netloc.lower()
        except:
            return ''

    def _get_junk_phrases(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑."""
        return [
            '–º–∞—Ç–µ—Ä–∏–∞–ª –∏–∑ –≤–∏–∫–∏–ø–µ–¥–∏–∏', '—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è', '–ø–µ—Ä–µ–π—Ç–∏ –∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏',
            '–ø–µ—Ä–µ–π—Ç–∏ –∫ –ø–æ–∏—Å–∫—É', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è:', '—à–∞–±–ª–æ–Ω:', '–∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî', '–¥–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è:',
            '–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ', '–∞–≤—Ç–æ—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª–∞:', '–ª–∏—Ü–µ–Ω–∑–∏—è creative commons',
            '—ç—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑', '—É —ç—Ç–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –¥—Ä—É–≥–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è',
            '—Å–º. —Ç–∞–∫–∂–µ', '–ø—Ä–∏–º–µ—á–∞–Ω–∏—è', '—Å—Å—ã–ª–∫–∏', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '—Ñ–æ—Ç–æ:', '¬©',
            'getty images', 'reuters', 'ap', '‚Üë', '‚Üì', '‚Üê', '‚Üí'
        ]

    def _deduplicate_facts_by_text(self, facts: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞."""
        seen = set()
        unique = []
        for f in facts:
            sig = f['text'][:100].lower()
            if sig not in seen:
                seen.add(sig)
                unique.append(f)
        return unique

    async def _get_query_embedding(self, query: str):
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ —Å —Ç–∞–π–º–∞—É—Ç–æ–º."""
        try:
            from appp.services.embedding.bge_m3 import embedder
            if embedder is None or embedder.model is None:
                return None
            return await asyncio.wait_for(embedder.embed(query), timeout=10.0)
        except asyncio.TimeoutError:
            logger.warning("‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞")
            return None
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None

    def _generate_extractive_summary(self, text: str, sentences_count: int = 3) -> str:
        """–≠–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ–∑—é–º–µ —á–µ—Ä–µ–∑ sumy (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)."""
        try:
            from sumy.parsers.plaintext import PlaintextParser
            from sumy.nlp.tokenizers import Tokenizer
            from sumy.summarizers.lsa import LsaSummarizer

            if not text or len(text) < 200:
                return ""
            parser = PlaintextParser.from_string(text, Tokenizer("russian"))
            summarizer = LsaSummarizer()
            summary = summarizer(parser.document, sentences_count)
            return " ".join(str(sentence) for sentence in summary)
        except ImportError:
            return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return ""

    def _calculate_confidence(self, chunks: List[Dict], facts: List[Dict] = None, query: str = "") -> float:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        - –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤
        - –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏ –¥–æ–º–µ–Ω–æ–≤
        - –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        - —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ–≤
        - –Ω–∞–ª–∏—á–∏—è NER-—Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        if not chunks:
            return 0.0
        
        avg_chunk_quality = sum(c.get('quality_score', 0) for c in chunks) / len(chunks)
        
        domain_scores = []
        for chunk in chunks:
            url = chunk.get('source_url', '')
            domain = self._extract_domain(url)
            if any(pd in domain for pd in self.priority_domains):
                domain_scores.append(1.0)
            elif any(ld in domain for ld in self.low_trust_domains):
                domain_scores.append(0.3)
            else:
                domain_scores.append(0.6)
        avg_domain_trust = sum(domain_scores) / len(domain_scores) if domain_scores else 0.5
        
        unique_sources = set(chunk.get('source_url', '') for chunk in chunks if chunk.get('source_url'))
        source_count_score = min(len(unique_sources) / 5.0, 1.0)
        
        consistency_score = 0.5
        if facts and len(facts) >= 3:
            all_text = " ".join([f.get('text', '') for f in facts[:10]])
            words = re.findall(r'\b\w{4,}\b', all_text.lower())
            from collections import Counter
            word_counts = Counter(words)
            common_words = [w for w, c in word_counts.most_common(5) if c >= 3]
            consistency_score = min(len(common_words) / 3.0, 1.0) if common_words else 0.4
        
        ner_score = 0.0
        if facts:
            ner_count = sum(1 for f in facts if f.get('ner_types'))
            ner_score = min(ner_count / max(len(facts), 1) * 2, 1.0)
        
        confidence = (
            avg_chunk_quality * 0.3 +
            avg_domain_trust * 0.25 +
            source_count_score * 0.2 +
            consistency_score * 0.15 +
            ner_score * 0.1
        )
        
        if len(unique_sources) == 1:
            confidence *= 0.8
        elif len(unique_sources) == 0:
            confidence *= 0.5
        
        return min(confidence, 1.0)

    async def _process_document(self, doc: Dict) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        content = doc.get('content', '')
        if not content:
            return {'chunks': []}

        cleaned_content = self.text_cleaner.clean(content)
        chunks = self._split_into_chunks(cleaned_content, doc.get('url', ''))

        total_chunks = len(chunks)
        for idx, chunk in enumerate(chunks):
            chunk['chunk_index'] = idx
            chunk['total_chunks'] = total_chunks
            chunk['quality_score'] = self._evaluate_chunk_quality(chunk['text'])

        chunks = [c for c in chunks if c['quality_score'] >= 0.5]
        chunks = chunks[:self.max_chunks_per_document]

        return {'chunks': chunks}

    def _split_into_chunks(self, text: str, source_url: str) -> List[Dict]:
        if not text:
            return []
        paragraphs = re.split(r'\n\s*\n', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        chunks = []
        current_chunk = []
        current_length = 0

        for para in paragraphs:
            para_len = len(para)
            if para_len >= self.chunk_size * 0.7:
                if current_chunk:
                    chunks.append(self._create_chunk('\n\n'.join(current_chunk), source_url))
                    current_chunk = []
                    current_length = 0
                chunks.append(self._create_chunk(para, source_url))
            else:
                if current_length + para_len <= self.chunk_size:
                    current_chunk.append(para)
                    current_length += para_len
                else:
                    chunks.append(self._create_chunk('\n\n'.join(current_chunk), source_url))
                    current_chunk = [para]
                    current_length = para_len
        if current_chunk:
            chunks.append(self._create_chunk('\n\n'.join(current_chunk), source_url))
        return chunks

    def _create_chunk(self, text: str, source_url: str) -> Dict:
        chunk_id = hashlib.md5(f"{source_url}_{text[:100]}".encode()).hexdigest()[:12]
        return {
            'chunk_id': chunk_id,
            'text': text,
            'source_url': source_url,
            'length': len(text),
            'created_at': datetime.now().isoformat()
        }

    def _deduplicate_chunks(self, chunks: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for chunk in chunks:
            sig = chunk['text'][:200]
            if sig not in seen:
                seen.add(sig)
                unique.append(chunk)
        return unique

    def _evaluate_chunk_quality(self, text: str) -> float:
        length = len(text)
        if length >= 800:
            return 0.8
        elif length >= 500:
            return 0.6
        elif length >= 300:
            return 0.4
        else:
            return 0.2

    async def get_analyst_stats(self) -> Dict:
        return self.stats

    async def health_check(self) -> Dict:
        return {
            'healthy': True,
            'message': 'KnowledgeAnalyst is operational',
            'timestamp': datetime.now().isoformat()
        }