"""
üìö –ê–ù–ê–õ–ò–¢–ò–ö –ó–ù–ê–ù–ò–ô ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ + —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import asyncio
import re
import hashlib
import yaml
import os
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from appp.core.logging import logger
from appp.utils.text_processor import TextCleaner
from appp.services.ranking.ranking_service import RankingService, Fact
from appp.services.embedding.bge_m3 import embedder

# –î–ª—è NER (GLiNER2)
try:
    from gliner2 import GLiNER2
    GLINER_AVAILABLE = True
except ImportError:
    GLINER_AVAILABLE = False
    logger.warning("‚ö†Ô∏è GLiNER2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, NER –æ—Ç–∫–ª—é—á—ë–Ω")

# –î–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
try:
    from sentence_transformers import util
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logger.warning("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")


class KnowledgeAnalyst:
    def __init__(self, config: Dict):
        self.config = config
        self.chunk_size = config.get('chunk_size', 1500)
        self.chunk_overlap = config.get('chunk_overlap', 300)
        self.min_chunk_length = config.get('min_chunk_length', 500)
        self.max_chunks_per_document = config.get('max_chunks_per_document', 50)
        self.enable_summarization = config.get('enable_summarization', True)
        self.language = config.get('language', 'ru')
        self.min_confidence = config.get('min_confidence', 0.6)
        self._last_facts_metadata = []

        self.text_cleaner = TextCleaner()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML (–∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞)
        self.profiles_config = self._load_profiles_config()
        self.quality_config = self._load_quality_config()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER (GLiNER2)
        self.ner_enabled = config.get('enable_ner', True) and GLINER_AVAILABLE
        self.ner_model_name = config.get('ner_model', 'fastino/gliner2-base-v1')
        
        if self.ner_enabled:
            try:
                self.gliner_model = GLiNER2.from_pretrained(self.ner_model_name)
                logger.info(f"üß† GLiNER2 NER –∑–∞–≥—Ä—É–∂–µ–Ω (–º–æ–¥–µ–ª—å: {self.ner_model_name})")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GLiNER2: {e}")
                self.ner_enabled = False

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –º–∏—Ä—ã (–∏–∑ profiles.yaml)
        self.ner_competencies = self.profiles_config.get('profiles', {})
        self.base_mapping = self.profiles_config.get('base_mapping', {})
        # Regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è fallback (–∏–∑ profiles.yaml)
        self.regex_patterns = self._prepare_regex_patterns(self.profiles_config.get('regex_patterns', {}))

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–æ–º–µ–Ω–æ–≤ –∏ –º—É—Å–æ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ quality.yaml
        self.priority_domains = set(self.quality_config.get('priority_domains', []))
        self.low_trust_domains = set(self.quality_config.get('low_trust_domains', []))
        self.junk_phrases = self.quality_config.get('junk_phrases', [])
        self.ad_indicators = self.quality_config.get('ad_indicators', [])

        self.ranking_service = RankingService()
        self.stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'errors': 0,
            'ner_used': 0
        }

        logger.info("üìö KnowledgeAnalyst —Å–æ–∑–¥–∞–Ω, –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ YAML")

    def _load_profiles_config(self) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ—Ñ–∏–ª–µ–π (NER-–º–µ—Ç–∫–∏ –∏ regex) –∏–∑ YAML-—Ñ–∞–π–ª–∞ –≤ –∫–æ—Ä–Ω–µ–≤–æ–π configs."""
        default_config = {
            'profiles': {},
            'base_mapping': {
                'person': 10.0, 'PER': 10.0,
                'location': 8.0, 'LOC': 8.0,
                'organization': 8.0, 'ORG': 8.0,
                'event': 7.0, 'date': 5.0, 'DATE': 5.0
            },
            'regex_patterns': {}
        }
        # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ 4 —É—Ä–æ–≤–Ω—è –≤–≤–µ—Ä—Ö –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞ (appp/services/analyst/knowledge_analyst.py -> –∫–æ—Ä–µ–Ω—å)
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        config_path = os.path.join(base_dir, 'configs', 'profiles.yaml')
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                if user_config:
                    if 'profiles' in user_config:
                        default_config['profiles'] = user_config['profiles']
                    if 'base_mapping' in user_config:
                        default_config['base_mapping'] = user_config['base_mapping']
                    if 'regex_patterns' in user_config:
                        default_config['regex_patterns'] = user_config['regex_patterns']
                logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π –∏–∑ {config_path}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ profiles.yaml: {e}, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è defaults")
        else:
            logger.warning(f"‚ö†Ô∏è {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        return default_config

    def _load_quality_config(self) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ (–¥–æ–º–µ–Ω—ã, –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã) –∏–∑ quality.yaml –≤ –∫–æ—Ä–Ω–µ–≤–æ–π configs."""
        default_config = {
            'priority_domains': [],
            'low_trust_domains': [],
            'ad_indicators': [],
            'junk_phrases': []
        }
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        config_path = os.path.join(base_dir, 'configs', 'quality.yaml')
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                if user_config:
                    for key in default_config:
                        if key in user_config:
                            default_config[key] = user_config[key]
                logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑ {config_path}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ quality.yaml: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏")
        return default_config

    def _prepare_regex_patterns(self, regex_dict: dict) -> List[Tuple[str, str]]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å regex_patterns –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (profile, pattern)."""
        patterns = []
        for profile, pattern_list in regex_dict.items():
            if isinstance(pattern_list, list):
                combined = '|'.join(pattern_list)
            else:
                combined = pattern_list
            patterns.append((profile, combined))
        # –î–æ–±–∞–≤–ª—è–µ–º default –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        patterns.append(('default', '.*'))
        return patterns

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º)."""
        return True

    # ----------------------------------------------------------------------
    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø (NER + regex)
    # ----------------------------------------------------------------------
    async def _get_query_ner_profile(self, query: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –ø–æ NER-–º–µ—Ç–∫–∞–º —Å–∞–º–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
        if not self.ner_enabled:
            return None
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–∫–∏ –∏–∑ ner_competencies –∏ base_mapping
            all_labels = list(self.base_mapping.keys())
            for prof in self.ner_competencies.values():
                all_labels.extend(prof['labels'])
            all_labels = list(set(all_labels))

            entities = await asyncio.to_thread(
                self.gliner_model.extract_entities, query, all_labels, threshold=0.5
            )

            # –°—á–∏—Ç–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
            profile_scores = {p: 0.0 for p in self.ner_competencies}
            for ent in entities:
                label = ent['label']
                score = ent['score']
                for prof_name, comp in self.ner_competencies.items():
                    if label in comp['labels']:
                        profile_scores[prof_name] += comp['weight'] * score
            
            if profile_scores:
                best = max(profile_scores, key=profile_scores.get)
                if profile_scores[best] > 5.0:  # –ø–æ—Ä–æ–≥
                    logger.debug(f"NER –∑–∞–ø—Ä–æ—Å–∞ –≤—ã–±—Ä–∞–ª –ø—Ä–æ—Ñ–∏–ª—å {best} —Å–æ —Å—á—ë—Ç–æ–º {profile_scores[best]:.1f}")
                    return best
        except Exception as e:
            logger.debug(f"NER –Ω–∞ –∑–∞–ø—Ä–æ—Å–µ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return None

    def _detect_query_profile_regex(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º (–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞)."""
        q = query.lower()
        for profile, pattern in self.regex_patterns:
            if re.search(pattern, q):
                return profile
        return 'default'

    async def detect_profile(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –ø–æ NER, –∑–∞—Ç–µ–º –ø–æ regex."""
        profile = await self._get_query_ner_profile(query)
        if profile is not None:
            return profile
        return self._detect_query_profile_regex(query)

    # ----------------------------------------------------------------------
    # –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –ê–ù–ê–õ–ò–ó–ê
    # ----------------------------------------------------------------------
    async def analyze(self, documents: List[Dict], query: str = "", is_discovery: bool = False) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤, —Ñ–∞–∫—Ç–æ–≤, —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –∫–ª—é—á–∞–º–∏: success, summary, key_points, key_facts_metadata, profile, query, confidence, ...
        """
        start_time = datetime.now()
        profile_name = await self.detect_profile(query)
        logger.info(f"üîç –û–ø—Ä–µ–¥–µ–ª—ë–Ω –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–ø—Ä–æ—Å–∞: {profile_name}")

        try:
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            all_chunks = []
            for doc in documents:
                doc_result = await self._process_document(doc, is_discovery=is_discovery)
                all_chunks.extend(doc_result.get('chunks', []))

            unique_chunks = self._deduplicate_chunks(all_chunks)
            logger.info(f"   ‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(unique_chunks)}")

            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã
            fact_objects = await self._extract_key_facts(
                chunks=unique_chunks,
                query=query,
                top_k=50,
                is_discovery=is_discovery
            )
            if fact_objects is None:
                fact_objects = []

            logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤ –¥–æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(fact_objects)}")

            # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (–µ—Å–ª–∏ –Ω–µ discovery)
            if not is_discovery and len(fact_objects) > 10 and SENTENCE_TRANSFORMERS_AVAILABLE:
                fact_objects = await self._semantic_deduplication(fact_objects, threshold=0.85)
                logger.info(f"   ‚úÖ –ü–æ—Å–ª–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(fact_objects)} —Ñ–∞–∫—Ç–æ–≤")

            # 4. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            top_facts = []
            key_points = []
            if fact_objects:
                facts_for_ranking = []
                for fo in fact_objects:
                    fact = Fact(
                        text=fo['text'],
                        source_domain=fo.get('domain', ''),
                        position_ratio=fo.get('position_ratio', 0.5),
                        ner_score=fo.get('ner_score', 0.0),
                        ner_types=fo.get('ner_types', []),
                        length=fo.get('length', len(fo['text'])),
                        contains_definition=fo.get('contains_definition', False),
                        contains_causal=fo.get('contains_causal', False)
                    )
                    facts_for_ranking.append(fact)

                query_emb = await self._get_query_embedding(query)
                ranked = await self.ranking_service.rank(
                    query=query,
                    facts=facts_for_ranking,
                    query_embedding=query_emb,
                    priority_domains=self.priority_domains,
                    low_trust_domains=self.low_trust_domains,
                    profile_name=profile_name
                )
                logger.info(f"   ‚úÖ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, —Ñ–∞–∫—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–Ω–≥–∞: {len(ranked)}")

                if ranked:
                    logger.info(f"   üîç –ü—Ä–∏–º–µ—Ä —Ñ–∞–∫—Ç–∞: {ranked[0][0].text[:100]}...")
                else:
                    logger.info("   üîç ranked –ø—É—Å—Ç")

                top_facts = [fact for fact, score in ranked[:15]]
                key_points = [fact.text for fact in top_facts]
                self._last_facts_metadata = top_facts[:15] if top_facts else []
            else:
                logger.info("   üîç –ù–µ—Ç —Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è")

            # 5. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            summary = ""
            if self.enable_summarization and unique_chunks:
                best_chunk = max(unique_chunks, key=lambda x: x.get('quality_score', 0))
                if best_chunk and best_chunk.get('text'):
                    summary = self._generate_extractive_summary(best_chunk['text'], sentences_count=3)

            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = self._calculate_confidence(unique_chunks, fact_objects, query)

            processing_time = (datetime.now() - start_time).total_seconds()
            self.stats['documents_processed'] += len(documents)
            self.stats['chunks_created'] += len(unique_chunks)

            logger.info(f"   üîç key_points –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º: {len(key_points)}")

            return {
                'success': True,
                'documents_count': len(documents),
                'summary': summary,
                'key_points': key_points[:15],
                'key_facts_metadata': top_facts[:15] if top_facts else [],
                'profile': profile_name,
                'query': query,
                'confidence': confidence,
                'processing_time': processing_time
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}", exc_info=True)
            self.stats['errors'] += 1
            return {
                'success': False,
                'error': str(e),
                'summary': '',
                'key_points': [],
                'profile': profile_name,
                'query': query
            }

    # ----------------------------------------------------------------------
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ----------------------------------------------------------------------
    async def _extract_key_facts(self, chunks, query, top_k=50, is_discovery=False):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ —á–∞–Ω–∫–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ junk_phrases."""
        if not chunks:
            return []
        all_facts = []
        junk_phrases = self.junk_phrases  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –∏–∑ quality.yaml

        for chunk in chunks:
            text = chunk.get('text', '')
            if not text:
                continue
            sentences = re.split(r'(?<=[.!?])\s+', text)
            for sent in sentences:
                sent = sent.strip()
                if len(sent) < 10:
                    continue
                sent_lower = sent.lower()
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã
                if any(phrase in sent_lower for phrase in junk_phrases):
                    continue
                if re.search(r'https?://|www\.', sent):
                    continue
                all_facts.append({
                    'text': sent,
                    'domain': self._extract_domain(chunk.get('source_url', '')),
                    'source_url': chunk.get('source_url', ''),
                    'position_ratio': 0.5,
                    'ner_score': 0.0,
                    'ner_types': [],
                    'length': len(sent),
                    'contains_definition': False,
                    'contains_causal': False,
                    'base_score': 1.0,
                    'total_score': 1.0,
                    'chunk_id': ''
                })
        return all_facts[:top_k]

    async def _semantic_deduplication(self, facts: List[Dict], threshold: float = 0.85) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã."""
        if len(facts) < 2:
            return facts
        try:
            from appp.services.embedding.bge_m3 import embedder
            texts = [f['text'] for f in facts]
            embeddings = await embedder.embed(texts)
            if not embeddings or len(embeddings) != len(facts):
                return facts
            unique_facts = []
            unique_embeddings = []
            for i, (fact, emb) in enumerate(zip(facts, embeddings)):
                is_duplicate = False
                for unique_emb in unique_embeddings:
                    try:
                        similarity = util.cos_sim(emb, unique_emb).item()
                        if similarity > threshold:
                            is_duplicate = True
                            break
                    except:
                        pass
                if not is_duplicate:
                    unique_facts.append(fact)
                    unique_embeddings.append(emb)
            return unique_facts
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return facts

    def _extract_domain(self, url: str) -> str:
        from urllib.parse import urlparse
        try:
            return urlparse(url).netloc.lower()
        except:
            return ''

    def _deduplicate_facts_by_text(self, facts: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for f in facts:
            sig = f['text'][:100].lower()
            if sig not in seen:
                seen.add(sig)
                unique.append(f)
        return unique

    async def _get_query_embedding(self, query: str):
        try:
            if embedder is None or embedder.model is None:
                return None
            return await asyncio.wait_for(embedder.embed(query), timeout=10.0)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None

    def _generate_extractive_summary(self, text: str, sentences_count: int = 3) -> str:
        try:
            from sumy.parsers.plaintext import PlaintextParser
            from sumy.nlp.tokenizers import Tokenizer
            from sumy.summarizers.lsa import LsaSummarizer
            if not text or len(text) < 200:
                return ""
            parser = PlaintextParser.from_string(text, Tokenizer("russian"))
            summarizer = LsaSummarizer()
            summary = summarizer(parser.document, sentences_count)
            return " ".join(str(sentence) for sentence in summary)
        except ImportError:
            return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return ""

    def _calculate_confidence(self, chunks, facts, query):
        if not chunks:
            return 0.0
        avg_chunk_quality = sum(c.get('quality_score', 0) for c in chunks) / len(chunks)
        domain_scores = []
        for chunk in chunks:
            url = chunk.get('source_url', '')
            domain = self._extract_domain(url)
            if domain in self.priority_domains:
                domain_scores.append(1.0)
            elif domain in self.low_trust_domains:
                domain_scores.append(0.3)
            else:
                domain_scores.append(0.6)
        avg_domain_trust = sum(domain_scores) / len(domain_scores) if domain_scores else 0.5
        unique_sources = set(chunk.get('source_url', '') for chunk in chunks if chunk.get('source_url'))
        source_count_score = min(len(unique_sources) / 5.0, 1.0)
        consistency_score = 0.5
        if facts and len(facts) >= 3:
            all_text = " ".join([f.get('text', '') for f in facts[:10]])
            words = re.findall(r'\b\w{4,}\b', all_text.lower())
            from collections import Counter
            word_counts = Counter(words)
            common_words = [w for w, c in word_counts.most_common(5) if c >= 3]
            consistency_score = min(len(common_words) / 3.0, 1.0) if common_words else 0.4
        ner_score = 0.0
        if facts:
            ner_count = sum(1 for f in facts if f.get('ner_types'))
            ner_score = min(ner_count / max(len(facts), 1) * 2, 1.0)
        confidence = (
            avg_chunk_quality * 0.3 +
            avg_domain_trust * 0.25 +
            source_count_score * 0.2 +
            consistency_score * 0.15 +
            ner_score * 0.1
        )
        if len(unique_sources) == 1:
            confidence *= 0.8
        elif len(unique_sources) == 0:
            confidence *= 0.5
        return min(confidence, 1.0)

    async def _process_document(self, doc: Dict, is_discovery: bool = False) -> Dict:
        content = doc.get('content', '')
        if not content:
            return {'chunks': []}
        cleaned_content = self.text_cleaner.clean(content)
        chunks = self._split_into_chunks(cleaned_content, doc.get('url', ''))
        total_chunks = len(chunks)
        for idx, chunk in enumerate(chunks):
            chunk['chunk_index'] = idx
            chunk['total_chunks'] = total_chunks
            chunk['quality_score'] = self._evaluate_chunk_quality(chunk['text'])
        quality_threshold = 0.2 if is_discovery else 0.5
        chunks = [c for c in chunks if c['quality_score'] >= quality_threshold]
        max_chunks = self.max_chunks_per_document * 2 if is_discovery else self.max_chunks_per_document
        chunks = chunks[:max_chunks]
        return {'chunks': chunks}

    def _split_into_chunks(self, text: str, source_url: str) -> List[Dict]:
        if not text:
            return []
        paragraphs = re.split(r'\n\s*\n', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        chunks = []
        current_chunk = []
        current_length = 0
        for para in paragraphs:
            para_len = len(para)
            if para_len >= self.chunk_size * 0.7:
                if current_chunk:
                    chunks.append(self._create_chunk('\n\n'.join(current_chunk), source_url))
                    current_chunk = []
                    current_length = 0
                chunks.append(self._create_chunk(para, source_url))
            else:
                if current_length + para_len <= self.chunk_size:
                    current_chunk.append(para)
                    current_length += para_len
                else:
                    chunks.append(self._create_chunk('\n\n'.join(current_chunk), source_url))
                    current_chunk = [para]
                    current_length = para_len
        if current_chunk:
            chunks.append(self._create_chunk('\n\n'.join(current_chunk), source_url))
        return chunks

    def _create_chunk(self, text: str, source_url: str) -> Dict:
        chunk_id = hashlib.md5(f"{source_url}_{text[:100]}".encode()).hexdigest()[:12]
        return {
            'chunk_id': chunk_id,
            'text': text,
            'source_url': source_url,
            'length': len(text),
            'created_at': datetime.now().isoformat()
        }

    def _deduplicate_chunks(self, chunks: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for chunk in chunks:
            sig = chunk['text'][:200]
            if sig not in seen:
                seen.add(sig)
                unique.append(chunk)
        return unique

    def _evaluate_chunk_quality(self, text: str) -> float:
        length = len(text)
        if length >= 800:
            return 0.8
        elif length >= 500:
            return 0.6
        elif length >= 300:
            return 0.4
        else:
            return 0.2

    async def get_analyst_stats(self) -> Dict:
        return self.stats

    async def health_check(self) -> Dict:
        return {
            'healthy': True,
            'message': 'KnowledgeAnalyst is operational',
            'timestamp': datetime.now().isoformat()
        }