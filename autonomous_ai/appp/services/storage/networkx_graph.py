"""
üï∏Ô∏è NETWORKX –ì–†–ê–§ –ó–ù–ê–ù–ò–ô
–•—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∞, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite
"""

import asyncio
import json
import os
import sqlite3
import pickle
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
import networkx as nx

from appp.core.logging import logger

from enum import Enum

class NodeType(Enum):
    TOPIC = "topic"
    CHUNK = "chunk"
    FACT = "fact"
    ENTITY = "entity"
    CONCEPT = "concept"
    QUESTION = "question"
    ANSWER = "answer"
    SOURCE = "source"


class RelationType(Enum):
    # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ
    IS_A = "is_a"
    PART_OF = "part_of"
    HAS_PART = "has_part"
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ
    DEFINED_AS = "defined_as"
    EXAMPLE_OF = "example_of"
    CONTRASTS_WITH = "contrasts_with"
    SIMILAR_TO = "similar_to"
    
    # –ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ
    CAUSES = "causes"
    LEADS_TO = "leads_to"
    PREVENTS = "prevents"
    ENABLES = "enables"
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ
    PRECEDES = "precedes"
    FOLLOWS = "follows"
    
    # –ê—Ç—Ä–∏–±—É—Ç–∏–≤–Ω—ã–µ
    HAS_PROPERTY = "has_property"
    HAS_VALUE = "has_value"
    LOCATED_IN = "located_in"
    CREATED_BY = "created_by"
    
    # –î–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ
    MENTIONED_IN = "mentioned_in"
    SUPPORTED_BY = "supported_by"
    CONTRADICTED_BY = "contradicted_by"
    
    # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ
    NEAR = "near"
    CONTAINS = "contains"
    BELONGS_TO = "belongs_to"

class NetworkXGraphService:
    """
    –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ NetworkX.
    - –•—Ä–∞–Ω–µ–Ω–∏–µ —É–∑–ª–æ–≤ (—Å—É—â–Ω–æ—Å—Ç–∏, —Ç–µ–º—ã, —á–∞–Ω–∫–∏)
    - –°–≤—è–∑–∏ (–æ—Ç–Ω–æ—à–µ–Ω–∏—è)
    - –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ SQLite
    - –ê–Ω–∞–ª–∏–∑: —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞, —Å–º–µ–∂–Ω—ã–µ —Ç–µ–º—ã, —Å—Ç–∞—Ä—ã–µ –∑–Ω–∞–Ω–∏—è
    """
    
    def __init__(
        self,
        db_path: str = "./data/graphs/knowledge_graph.db",
        auto_save: bool = True,
        save_interval: int = 60,
        enable_compression: bool = True
    ):
        self.db_path = db_path
        self.auto_save = auto_save
        self.save_interval = save_interval
        self.enable_compression = enable_compression
        
        self.graph: nx.Graph = nx.Graph()
        self.node_metadata: Dict[str, Dict] = {}
        
        # –î–ª—è –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.dirty = False
        self.save_task: Optional[asyncio.Task] = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'nodes_added': 0,
            'edges_added': 0,
            'nodes_removed': 0,
            'edges_removed': 0,
            'queries': 0,
            'last_save_time': None,
            'last_save_size': 0,
            'errors': 0
        }
        
        logger.info(f"üï∏Ô∏è NetworkXGraphService —Å–æ–∑–¥–∞–Ω (db: {db_path})")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ SQLite
            await self._load_from_db()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            if self.auto_save:
                self.save_task = asyncio.create_task(self._auto_save_loop())
            
            logger.info(f"‚úÖ –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω: {self.graph.number_of_nodes()} —É–∑–ª–æ–≤, "
                       f"{self.graph.number_of_edges()} —Å–≤—è–∑–µ–π")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞: {e}")
            return False


    def calculate_edge_weight(self, 
                            confidence: float,           # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ñ–∞–∫—Ç–µ (0-1)
                            source_importance: float,    # –≤–∞–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                            temporal_decay: bool = True, # —É—á—ë—Ç –≤—Ä–µ–º–µ–Ω–∏
                            usage_count: int = 0) -> float:  # —á–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        weight = confidence
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –¥–æ–º–µ–Ω
        if source_domain in self.priority_domains:
            weight += 0.2
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑–∫–æ–¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –¥–æ–º–µ–Ω
        if source_domain in self.low_trust_domains:
            weight -= 0.3
        
        # –ë–æ–Ω—É—Å –∑–∞ —á–∞—Å—Ç–æ—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        weight += min(0.5, usage_count * 0.1)
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞—Å–ø–∞–¥ (—Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –≤–µ—Å—è—Ç –º–µ–Ω—å—à–µ)
        if temporal_decay and 'created_at' in data:
            days_old = (datetime.now() - created_date).days
            decay = max(0.5, 1.0 - days_old * 0.01)  # –Ω–µ –º–µ–Ω—å—à–µ 0.5
            weight *= decay
        
        return max(0.1, min(2.0, weight))  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Ç 0.1 –¥–æ 2.0
    
    def _node_type_to_str(self, node_type: NodeType) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç NodeType –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ–µ"""
        return node_type.value

    def _str_to_node_type(self, type_str: str) -> NodeType:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ NodeType (—Å –∑–∞–ø–∞—Å–æ–º –Ω–∞ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ)"""
        try:
            return NodeType(type_str)
        except ValueError:
            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç–∞—Ä—ã–π —Ç–∏–ø, –º–∞–ø–ø–∏–º –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–π
            mapping = {
                'key_point': NodeType.FACT,
                'update': NodeType.CHUNK,
                'insight': NodeType.FACT,
                'topic': NodeType.TOPIC,
                'chunk': NodeType.CHUNK,
                'entity': NodeType.ENTITY
            }
            return mapping.get(type_str, NodeType.CONCEPT)

    def _relation_type_to_str(self, rel_type: RelationType) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç RelationType –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ–µ"""
        return rel_type.value

    def _str_to_relation_type(self, type_str: str) -> RelationType:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ RelationType"""
        try:
            return RelationType(type_str)
        except ValueError:
            # –ú–∞–ø–ø–∏–Ω–≥ —Å—Ç–∞—Ä—ã—Ö —Ç–∏–ø–æ–≤
            mapping = {
                'contains': RelationType.CONTAINS,
                'has_point': RelationType.HAS_PART,
                'has_insight': RelationType.HAS_PART,
                'updated': RelationType.SUPPORTED_BY,
                'contains_entity': RelationType.CONTAINS,
                'related': RelationType.SIMILAR_TO
            }
            return mapping.get(type_str, RelationType.SIMILAR_TO)


    async def add_knowledge_chunk(
        self,
        topic: str,
        chunk: Dict,
        relations: List[Dict] = None
    ):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π –≤ –≥—Ä–∞—Ñ —Å —É–º–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏.
        
        Args:
            topic: –¢–µ–º–∞ (–∫–æ—Ä–Ω–µ–≤–æ–π —É–∑–µ–ª)
            chunk: –ß–∞–Ω–∫ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            relations: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        """
        chunk_id = chunk.get('chunk_id', f"chunk_{datetime.now().timestamp()}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–µ–ª —á–∞–Ω–∫–∞
        self.graph.add_node(chunk_id, type='chunk', topic=topic, **chunk)
        self.node_metadata[chunk_id] = {
            'created_at': datetime.now().isoformat(),
            'type': 'chunk',
            'topic': topic
        }
        self.stats['nodes_added'] += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–µ–ª —Ç–µ–º—ã (–µ—Å–ª–∏ –Ω–µ—Ç)
        topic_id = f"topic_{topic}"
        if not self.graph.has_node(topic_id):
            self.graph.add_node(topic_id, type='topic', name=topic)
            self.node_metadata[topic_id] = {
                'created_at': datetime.now().isoformat(),
                'type': 'topic',
                'name': topic
            }
            self.stats['nodes_added'] += 1
        
        # ========== –í–´–ß–ò–°–õ–Ø–ï–ú –£–ú–ù–´–ô –í–ï–° ==========
        
        # –ë–µ—Ä—ë–º confidence –∏–∑ —á–∞–Ω–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        confidence = chunk.get('confidence', 0.5)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        text_length = len(chunk.get('text', ''))
        if text_length > 2000:
            quality_bonus = 0.3
        elif text_length > 1000:
            quality_bonus = 0.2
        elif text_length > 500:
            quality_bonus = 0.1
        else:
            quality_bonus = 0.0
        
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        weight = confidence + quality_bonus
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (–µ—Å–ª–∏ —á–∞–Ω–∫ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω)
        if chunk.get('key_points'):
            weight += 0.2
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤–µ—Å
        weight = max(0.3, min(1.5, weight))
        
        # ========== –î–û–ë–ê–í–õ–Ø–ï–ú –°–í–Ø–ó–¨ –° –í–ï–°–û–ú ==========
        
        self.graph.add_edge(
            topic_id, 
            chunk_id, 
            relation='contains', 
            weight=weight,
            confidence=confidence,
            quality_bonus=quality_bonus,
            created_at=datetime.now().isoformat(),
            text_length=text_length
        )
        self.stats['edges_added'] += 1
        
        logger.info(f"   üîó –°–≤—è–∑—å {topic} -> —á–∞–Ω–∫, –≤–µ—Å={weight:.2f} (confidence={confidence}, –±–æ–Ω—É—Å={quality_bonus})")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        if relations:
            for rel in relations:
                await self._add_relation(rel, chunk_id)
        
        self.dirty = True


    async def update_edge_weight(self, source_id: str, target_id: str, relation_type: str = 'contains'):
        """
        –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Å —Ä–µ–±—Ä–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ (–¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Å–≤—è–∑–µ–π).
        """
        for u, v, data in self.graph.edges(data=True):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö (–≥—Ä–∞—Ñ –Ω–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
            if (u == source_id and v == target_id) or (u == target_id and v == source_id):
                if data.get('relation') == relation_type:
                    current_weight = data.get('weight', 1.0)
                    usage_count = data.get('usage_count', 0) + 1
                    
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å, –Ω–æ –Ω–µ –±–æ–ª—å—à–µ 2.0
                    new_weight = min(2.0, current_weight + 0.1)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º
                    data['weight'] = new_weight
                    data['usage_count'] = usage_count
                    data['last_used'] = datetime.now().isoformat()
                    
                    logger.debug(f"üìà –í–µ—Å —Å–≤—è–∑–∏ —É–≤–µ–ª–∏—á–µ–Ω: {new_weight:.2f} (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π: {usage_count})")
                    self.dirty = True
                    return True
        return False


    
    async def add_topic_knowledge(
        self,
        topic: str,
        knowledge: Dict,
        depth: int = 1
    ):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ.
        
        Args:
            topic: –¢–µ–º–∞
            knowledge: –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ (–∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏, –æ–±–∑–æ—Ä)
            depth: –ì–ª—É–±–∏–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        """
        topic_id = f"topic_{topic}"
        
        # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —É–∑–µ–ª —Ç–µ–º—ã
        node_data = {
            'type': 'topic',
            'name': topic,
            'depth': depth,
            'overview': knowledge.get('overview', ''),
            'updated_at': datetime.now().isoformat()
        }
        
        if self.graph.has_node(topic_id):
            # –û–±–Ω–æ–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
            self.graph.nodes[topic_id].update(node_data)
        else:
            self.graph.add_node(topic_id, **node_data)
            self.node_metadata[topic_id] = {
                'created_at': datetime.now().isoformat(),
                'type': 'topic',
                'name': topic
            }
            self.stats['nodes_added'] += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —É–∑–ª—ã
        key_points = knowledge.get('key_points', [])
        for i, point in enumerate(key_points):
            point_id = f"point_{topic}_{i}_{hash(point.get('content', '')) % 10000}"
            self.graph.add_node(
                point_id,
                type='key_point',
                content=point.get('content', ''),
                confidence=point.get('confidence', 0),
                importance=point.get('importance', 0.5),
                topic=topic
            )
            self.graph.add_edge(topic_id, point_id, relation='has_point')
            self.stats['nodes_added'] += 1
            self.stats['edges_added'] += 1
        
        self.dirty = True
    
    async def add_exploration_result(
        self,
        topic: str,
        exploration: Dict,
        depth_achieved: int
    ):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å —Ü–∏–∫–ª–∞–º–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç.
        """
        topic_id = f"topic_{topic}"
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã —Ç–µ–º—ã
        if self.graph.has_node(topic_id):
            self.graph.nodes[topic_id]['exploration_depth'] = depth_achieved
            self.graph.nodes[topic_id]['last_explored'] = datetime.now().isoformat()
            self.graph.nodes[topic_id]['comprehensive_summary'] = exploration.get('comprehensive_summary', '')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å–∞–π—Ç—ã
        insights = exploration.get('key_insights', [])
        for i, insight in enumerate(insights):
            insight_id = f"insight_{topic}_{i}_{datetime.now().timestamp()}"
            self.graph.add_node(
                insight_id,
                type='insight',
                content=insight,
                topic=topic,
                depth=depth_achieved
            )
            self.graph.add_edge(topic_id, insight_id, relation='has_insight')
            self.stats['nodes_added'] += 1
            self.stats['edges_added'] += 1
        
        self.dirty = True
    
    async def update_topic_knowledge(
        self,
        topic: str,
        new_information: Dict,
        update_type: str = 'refresh'
    ):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∑–Ω–∞–Ω–∏—è.
        """
        topic_id = f"topic_{topic}"
        
        if not self.graph.has_node(topic_id):
            logger.warning(f"–¢–µ–º–∞ {topic} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            await self.add_topic_knowledge(topic, new_information, depth=1)
            return
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.graph.nodes[topic_id]['updated_at'] = datetime.now().isoformat()
        self.graph.nodes[topic_id]['update_count'] = self.graph.nodes[topic_id].get('update_count', 0) + 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —á–∞–Ω–∫
        chunk_id = f"update_{topic}_{datetime.now().timestamp()}"
        self.graph.add_node(
            chunk_id,
            type='update',
            content=new_information.get('answer', ''),
            confidence=new_information.get('confidence', 0),
            timestamp=datetime.now().isoformat()
        )
        self.graph.add_edge(topic_id, chunk_id, relation='updated')
        
        self.stats['nodes_added'] += 1
        self.stats['edges_added'] += 1
        self.dirty = True
    
    # ========== –ù–û–í–´–ï –ú–ï–¢–û–î–´ –î–õ–Ø RELATED-–¢–ï–ú –ò –°–ê–ú–û–û–ë–£–ß–ï–ù–ò–Ø ==========
    
    async def get_related_nodes(self, topic: str, max_nodes: int = 3) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —É–∑–ª–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –¥–∞–Ω–Ω–æ–π —Ç–µ–º–æ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
        
        Args:
            topic: –¢–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–µ–π
            max_nodes: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —É–∑–ª–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ–º
        """
        try:
            # –ò—â–µ–º —É–∑–µ–ª —Ç–µ–º—ã
            topic_id = None
            for node, attrs in self.graph.nodes(data=True):
                if attrs.get('type') == 'topic' and attrs.get('name', '').lower() == topic.lower():
                    topic_id = node
                    break
            
            if not topic_id:
                logger.debug(f"–¢–µ–º–∞ '{topic}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –≥—Ä–∞—Ñ–µ")
                return []
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —É–∑–ª—ã
            related = set()
            
            # –ü—Ä—è–º—ã–µ —Å–æ—Å–µ–¥–∏ –ø–æ –≥—Ä–∞—Ñ—É
            for neighbor in self.graph.neighbors(topic_id):
                node_data = self.graph.nodes[neighbor]
                node_type = node_data.get('type', '')
                node_name = node_data.get('name', '')
                
                # –ï—Å–ª–∏ —Å–æ—Å–µ–¥ - —Ç–µ–º–∞, –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É
                if node_type == 'topic' and node_name:
                    related.add(node_name)
                
                # –ï—Å–ª–∏ —Å–æ—Å–µ–¥ - —á–∞–Ω–∫ –∏–ª–∏ —Å—É—â–Ω–æ—Å—Ç—å, –∏—â–µ–º —á–µ—Ä–µ–∑ –Ω–µ–≥–æ –¥—Ä—É–≥–∏–µ —Ç–µ–º—ã
                elif node_type in ['chunk', 'entity', 'key_point']:
                    for n2 in self.graph.neighbors(neighbor):
                        if n2 != topic_id:
                            n2_data = self.graph.nodes[n2]
                            if n2_data.get('type') == 'topic':
                                n2_name = n2_data.get('name', '')
                                if n2_name:
                                    related.add(n2_name)
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â–∏–µ —Ç–µ–º—ã –¥–ª—è fallback
            if not related:
                fallback_map = {
                    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç': ['–Ω–µ–π—Ä–æ—Å–µ—Ç–∏', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ'],
                    '–∏–∏': ['–Ω–µ–π—Ä–æ—Å–µ—Ç–∏', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ'],
                    '–Ω–µ–π—Ä–æ—Å–µ—Ç–∏': ['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ'],
                    '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ': ['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–Ω–µ–π—Ä–æ—Å–µ—Ç–∏', '–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º'],
                }
                
                topic_lower = topic.lower()
                for key, values in fallback_map.items():
                    if key in topic_lower or topic_lower in key:
                        return values[:max_nodes]
            
            return list(related)[:max_nodes]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ get_related_nodes: {e}")
            return []

    async def get_weak_topics(self, min_connections: int = 2, limit: int = 5) -> List[str]:
        """
        –ü–æ–∏—Å–∫ —Å–ª–∞–±–æ –∏–∑—É—á–µ–Ω–Ω—ã—Ö —Ç–µ–º (–º–∞–ª–æ —Å–≤—è–∑–µ–π).
        –î–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            min_connections: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –¥–ª—è —Å–∏–ª—å–Ω–æ–π —Ç–µ–º—ã
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ç–µ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å–ª–∞–±—ã—Ö —Ç–µ–º
        """
        try:
            weak = []
            
            for node, attrs in self.graph.nodes(data=True):
                if attrs.get('type') == 'topic':
                    # –°—á–∏—Ç–∞–µ–º —Å—Ç–µ–ø–µ–Ω—å —É–∑–ª–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π)
                    degree = self.graph.degree(node)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã
                    name = attrs.get('name', '')
                    if not name:
                        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ node_id
                        if node.startswith('topic_'):
                            name = node[6:]
                        else:
                            continue
                    
                    if degree < min_connections:
                        weak.append({
                            'name': name,
                            'connections': degree,
                            'node': node
                        })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —Å–≤—è–∑–µ–π (—Å–∞–º—ã–µ —Å–ª–∞–±—ã–µ –ø–µ—Ä–≤—ã–µ)
            weak.sort(key=lambda x: x['connections'])
            
            return [w['name'] for w in weak[:limit]]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ get_weak_topics: {e}")
            return []

    async def get_old_topics(self, days_threshold: int = 7, limit: int = 5) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –¥–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–≤—à–∏—Ö—Å—è —Ç–µ–º.
        –î–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            days_threshold: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ç–µ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ä—ã—Ö —Ç–µ–º
        """
        try:
            old = []
            cutoff = datetime.now() - timedelta(days=days_threshold)
            
            for node, attrs in self.graph.nodes(data=True):
                if attrs.get('type') == 'topic':
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã
                    name = attrs.get('name', '')
                    if not name:
                        if node.startswith('topic_'):
                            name = node[6:]
                        else:
                            continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    updated = attrs.get('updated_at', attrs.get('created_at', '2000-01-01'))
                    try:
                        if isinstance(updated, str):
                            updated_date = datetime.fromisoformat(updated)
                        else:
                            updated_date = datetime(2000, 1, 1)
                        
                        if updated_date < cutoff:
                            old.append({
                                'name': name,
                                'last_updated': updated_date,
                                'days_old': (datetime.now() - updated_date).days
                            })
                    except:
                        # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É, —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ä–æ–π
                        old.append({
                            'name': name,
                            'last_updated': datetime(2000, 1, 1),
                            'days_old': 999
                        })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ç–∞—Ä–æ—Å—Ç–∏
            old.sort(key=lambda x: x['days_old'], reverse=True)
            
            return [o['name'] for o in old[:limit]]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ get_old_topics: {e}")
            return []
    
    # ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ==========
    
    async def get_weak_knowledge_areas(self, limit: int = 5) -> List[str]:
        """
        –ü–æ–∏—Å–∫ —Å–ª–∞–±–æ –∏–∑—É—á–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π (—Ç–µ–º —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π).
        """
        weak_areas = []
        
        # –£–∑–ª—ã-—Ç–µ–º—ã
        topic_nodes = [
            n for n, attrs in self.graph.nodes(data=True)
            if attrs.get('type') == 'topic'
        ]
        
        for node in topic_nodes:
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å–≤—è–∑–µ–π
            out_degree = self.graph.degree(node)
            
            # –î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            last_updated = self.graph.nodes[node].get('updated_at', '2000-01-01')
            last_date = datetime.fromisoformat(last_updated) if isinstance(last_updated, str) else datetime(2000,1,1)
            days_old = (datetime.now() - last_date).days
            
            # –í–µ—Å —Å–ª–∞–±–æ—Å—Ç–∏: –º–∞–ª–µ–Ω—å–∫–∞—è —Å—Ç–µ–ø–µ–Ω—å + –±–æ–ª—å—à–∞—è –¥–∞–≤–Ω–æ—Å—Ç—å
            weakness_score = (1 / (out_degree + 1)) * 10 + days_old * 0.1
            
            weak_areas.append((weakness_score, self.graph.nodes[node].get('name', node)))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–ª–∞–±–æ—Å—Ç–∏
        weak_areas.sort(key=lambda x: x[0], reverse=True)
        
        return [name for score, name in weak_areas[:limit]]
    
    async def get_related_topics(self, seed_topics: List[str], limit: int = 3) -> List[str]:
        """
        –ü–æ–∏—Å–∫ —Å–º–µ–∂–Ω—ã—Ö —Ç–µ–º —á–µ—Ä–µ–∑ –æ–±—â–∏–µ —á–∞–Ω–∫–∏/—Å—É—â–Ω–æ—Å—Ç–∏.
        """
        related = set()
        
        for seed in seed_topics:
            seed_id = f"topic_{seed}"
            if not self.graph.has_node(seed_id):
                continue
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∞–Ω–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å seed
            neighbors = list(self.graph.neighbors(seed_id))
            
            for n in neighbors:
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å–º–æ—Ç—Ä–∏–º, —Å –∫–∞–∫–∏–º–∏ –µ—â–µ —Ç–µ–º–∞–º–∏ –æ–Ω —Å–≤—è–∑–∞–Ω
                if self.graph.nodes[n].get('type') == 'chunk':
                    chunk_neighbors = list(self.graph.neighbors(n))
                    for cn in chunk_neighbors:
                        if cn != seed_id and self.graph.nodes[cn].get('type') == 'topic':
                            topic_name = self.graph.nodes[cn].get('name', cn)
                            related.add(topic_name)
        
        return list(related)[:limit]
    
    async def get_old_knowledge(self, days_old: int = 7, limit: int = 10) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –±–æ–ª–µ–µ N –¥–Ω–µ–π.
        """
        old_topics = []
        cutoff = datetime.now() - timedelta(days=days_old)
        
        for node, attrs in self.graph.nodes(data=True):
            if attrs.get('type') == 'topic':
                updated = attrs.get('updated_at', attrs.get('created_at', '2000-01-01'))
                try:
                    updated_date = datetime.fromisoformat(updated)
                    if updated_date < cutoff:
                        old_topics.append({
                            'node': node,
                            'topic': attrs.get('name', node),
                            'last_updated': updated,
                            'days_old': (datetime.now() - updated_date).days
                        })
                except:
                    pass
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ç–∞—Ä–æ—Å—Ç–∏
        old_topics.sort(key=lambda x: x['days_old'], reverse=True)
        
        return old_topics[:limit]
    
    async def optimize(self) -> Dict:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ (—Å–∂–∞—Ç–∏–µ, —É–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —É–∑–ª–æ–≤).
        """
        logger.info("üîÑ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...")
        
        removed_nodes = 0
        removed_edges = 0
        
        # –£–¥–∞–ª—è–µ–º –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É–∑–ª—ã (–±–µ–∑ —Å–≤—è–∑–µ–π)
        isolated = list(nx.isolates(self.graph))
        self.graph.remove_nodes_from(isolated)
        removed_nodes += len(isolated)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ä–µ–±—Ä–∞
        loops = list(nx.selfloop_edges(self.graph))
        self.graph.remove_edges_from(loops)
        removed_edges += len(loops)
        
        self.stats['nodes_removed'] += removed_nodes
        self.stats['edges_removed'] += removed_edges
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        await self.save()
        
        return {
            'removed_isolated_nodes': removed_nodes,
            'removed_self_loops': removed_edges,
            'nodes_after': self.graph.number_of_nodes(),
            'edges_after': self.graph.number_of_edges()
        }
    
    async def analyze_structure(self) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞.
        """
        analysis = {
            'nodes': self.graph.number_of_nodes(),
            'edges': self.graph.number_of_edges(),
            'density': nx.density(self.graph),
            'connected_components': nx.number_connected_components(self.graph),
            'avg_clustering': nx.average_clustering(self.graph),
            'node_types': defaultdict(int),
            'relation_types': defaultdict(int)
        }
        
        # –¢–∏–ø—ã —É–∑–ª–æ–≤
        for _, attrs in self.graph.nodes(data=True):
            node_type = attrs.get('type', 'unknown')
            analysis['node_types'][node_type] += 1
        
        # –¢–∏–ø—ã —Å–≤—è–∑–µ–π
        for _, _, attrs in self.graph.edges(data=True):
            rel_type = attrs.get('relation', 'unknown')
            analysis['relation_types'][rel_type] += 1
        
        # –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —É–∑–ª—ã (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å)
        try:
            if self.graph.number_of_nodes() > 1:
                centrality = nx.degree_centrality(self.graph)
                top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]
                analysis['top_central_nodes'] = [
                    {'node': n, 'centrality': c} for n, c in top_nodes
                ]
        except:
            pass
        
        return analysis
    
    async def _add_relation(self, rel: Dict, source_chunk_id: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ"""
        source = rel.get('source', '')
        target = rel.get('target', '')
        rel_type = rel.get('type', 'related')
        
        if not source or not target:
            return
        
        # –°–æ–∑–¥–∞–µ–º —É–∑–ª—ã –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        for entity in [source, target]:
            if not self.graph.has_node(entity):
                self.graph.add_node(entity, type='entity')
                self.node_metadata[entity] = {
                    'created_at': datetime.now().isoformat(),
                    'type': 'entity'
                }
                self.stats['nodes_added'] += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–æ
        self.graph.add_edge(source, target, relation=rel_type, weight=0.8, source_chunk=source_chunk_id)
        self.stats['edges_added'] += 1
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —á–∞–Ω–∫ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        self.graph.add_edge(source_chunk_id, source, relation='contains_entity')
        self.graph.add_edge(source_chunk_id, target, relation='contains_entity')
        self.stats['edges_added'] += 2
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥—Ä–∞—Ñ–∞"""
        return {
            'nodes': self.graph.number_of_nodes(),
            'edges': self.graph.number_of_edges(),
            **self.stats,
            'node_metadata_count': len(self.node_metadata),
            'graph_ready': True
        }
    
    async def save(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –≤ SQLite"""
        try:
            start_time = datetime.now()
            
            # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≥—Ä–∞—Ñ
            graph_data = pickle.dumps(self.graph)
            metadata_data = pickle.dumps(self.node_metadata)
            
            if self.enable_compression:
                import zlib
                graph_data = zlib.compress(graph_data)
                metadata_data = zlib.compress(metadata_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SQLite
            conn = sqlite3.connect(self.db_path)
            c = conn.cursor()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ –Ω–µ—Ç
            c.execute('''
                CREATE TABLE IF NOT EXISTS graph_store (
                    id INTEGER PRIMARY KEY,
                    graph BLOB,
                    metadata BLOB,
                    timestamp TEXT,
                    version TEXT
                )
            ''')
            
            c.execute('''
                INSERT INTO graph_store (graph, metadata, timestamp, version)
                VALUES (?, ?, ?, ?)
            ''', (
                graph_data,
                metadata_data,
                datetime.now().isoformat(),
                '1.0'
            ))
            
            conn.commit()
            conn.close()
            
            self.dirty = False
            self.stats['last_save_time'] = datetime.now().isoformat()
            self.stats['last_save_size'] = len(graph_data) + len(metadata_data)
            
            logger.debug(f"üíæ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω–µ–Ω ({self.graph.number_of_nodes()} —É–∑–ª–æ–≤) –∑–∞ "
                        f"{(datetime.now() - start_time).total_seconds():.2f} —Å–µ–∫")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")
            self.stats['errors'] += 1
    
    async def _load_from_db(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥—Ä–∞—Ñ–∞ –∏–∑ SQLite"""
        if not os.path.exists(self.db_path):
            logger.info("–§–∞–π–ª –ë–î –≥—Ä–∞—Ñ–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –≥—Ä–∞—Ñ")
            return
        
        try:
            conn = sqlite3.connect(self.db_path)
            c = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='graph_store'")
            if not c.fetchone():
                conn.close()
                return
            
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å
            c.execute('''
                SELECT graph, metadata FROM graph_store
                ORDER BY id DESC LIMIT 1
            ''')
            
            row = c.fetchone()
            conn.close()
            
            if row:
                graph_data, metadata_data = row
                
                if self.enable_compression:
                    import zlib
                    graph_data = zlib.decompress(graph_data)
                    metadata_data = zlib.decompress(metadata_data)
                
                self.graph = pickle.loads(graph_data)
                self.node_metadata = pickle.loads(metadata_data)
                
                logger.info(f"üì¶ –ì—Ä–∞—Ñ –∑–∞–≥—Ä—É–∂–µ–Ω: {self.graph.number_of_nodes()} —É–∑–ª–æ–≤")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä–∞—Ñ–∞: {e}")
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –≥—Ä–∞—Ñ
            self.graph = nx.Graph()
            self.node_metadata = {}
    
    async def _auto_save_loop(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞"""
        while True:
            await asyncio.sleep(self.save_interval)
            if self.dirty:
                await self.save()
    
    async def get_weak_topics(self, min_weight: float = 0.7, limit: int = 5) -> List[str]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–º—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å —Å–≤—è–∑–µ–π –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞.
        –î–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è - –≤—ã–±–∏—Ä–∞—Ç—å —Å–ª–∞–±–æ –∏–∑—É—á–µ–Ω–Ω—ã–µ —Ç–µ–º—ã.
        
        Args:
            min_weight: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å (–Ω–∏–∂–µ —ç—Ç–æ–≥–æ - —Ç–µ–º–∞ —Å–ª–∞–±–∞—è)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å–ª–∞–±—ã—Ö —Ç–µ–º
        """
        try:
            topic_weights = []
            
            for node, attrs in self.graph.nodes(data=True):
                if attrs.get('type') == 'topic':
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã
                    topic_name = attrs.get('name', '')
                    if not topic_name and node.startswith('topic_'):
                        topic_name = node[6:]
                    
                    if not topic_name:
                        continue
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π —ç—Ç–æ–π —Ç–µ–º—ã
                    weights = []
                    for u, v, data in self.graph.edges(data=True):
                        if (u == node or v == node) and data.get('relation') == 'contains':
                            weights.append(data.get('weight', 1.0))
                    
                    if weights:
                        avg_weight = sum(weights) / len(weights)
                        if avg_weight < min_weight:
                            topic_weights.append((avg_weight, topic_name))
                            logger.debug(f"   –°–ª–∞–±–∞—è —Ç–µ–º–∞: {topic_name}, —Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å={avg_weight:.2f}")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é –≤–µ—Å–∞ (—Å–∞–º—ã–µ —Å–ª–∞–±—ã–µ –ø–µ—Ä–≤—ã–µ)
            topic_weights.sort(key=lambda x: x[0])
            
            result = [name for weight, name in topic_weights[:limit]]
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–ª–∞–±—ã—Ö —Ç–µ–º: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ get_weak_topics: {e}")
            return []


    async def close(self):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã NetworkXGraphService...")
        
        if self.save_task:
            self.save_task.cancel()
            try:
                await self.save_task
            except asyncio.CancelledError:
                pass
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã—Ç–∏–µ–º
        if self.dirty:
            await self.save()
        
        logger.info("‚úÖ NetworkXGraphService –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")
    
    async def health_check(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤
            nodes = self.graph.number_of_nodes()
            return {
                'healthy': True,
                'nodes': nodes,
                'edges': self.graph.number_of_edges(),
                'message': 'Graph service is operational',
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'healthy': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    async def get_metrics(self) -> Dict:
        """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        stats = await self.get_stats()
        analysis = await self.analyze_structure()
        return {
            **stats,
            'density': analysis.get('density', 0),
            'connected_components': analysis.get('connected_components', 0),
            'avg_clustering': analysis.get('avg_clustering', 0),
            'node_type_distribution': dict(analysis.get('node_types', {}))
        }