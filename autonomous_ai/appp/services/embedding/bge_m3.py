"""
üß† BGE-M3 –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
–ú–æ–¥–µ–ª—å –æ—Ç BAAI, –ª–æ–∫–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import asyncio
import logging
import hashlib
import json
import os
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import numpy as np

from sentence_transformers import SentenceTransformer  # <-- –ò–ú–ü–û–†–¢ –ù–ê –í–ï–†–•–ù–ï–ú –£–†–û–í–ù–ï

from appp.core.logging import logger

logger = logging.getLogger(__name__)


class BGE_M3_Embedder:
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ BAAI/bge-m3 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–∞—Ç—á–∏–Ω–≥, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é.
    """

    def __init__(
        self,
        model_name: str = "BAAI/bge-m3",
        model_path: Optional[str] = None,
        device: str = "cpu",
        normalize_embeddings: bool = True,
        cache_dir: str = "./data/cache/embeddings",
        max_cache_size: int = 10000,
        batch_size: int = 32,
        embedding_dimension: int = 768
    ):
        self.model_name = model_name
        self.model_path = model_path or model_name
        self.device = device
        self.normalize = normalize_embeddings
        self.cache_dir = cache_dir
        self.max_cache_size = max_cache_size
        self.batch_size = batch_size
        self.embedding_dimension = embedding_dimension

        self.model: Optional[SentenceTransformer] = None
        self.cache: Dict[str, np.ndarray] = {}
        self.cache_metadata: Dict[str, Dict] = {}

        self.stats = {
            'embedding_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'batch_processed': 0,
            'total_embeddings': 0,
            'avg_embedding_time': 0.0,
            'errors': 0
        }

        os.makedirs(self.cache_dir, exist_ok=True)
        logger.info(f"üß† BGE-M3 Embedder —Å–æ–∑–¥–∞–Ω (device: {device})")

    async def initialize(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∫—ç—à–∞"""
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists(self.model_path):
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏ {self.model_path}, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ö–∞–±–∞...")
            try:
                # –ò–º–ø–æ—Ä—Ç —É–∂–µ –µ—Å—Ç—å –≤–≤–µ—Ä—Ö—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º SentenceTransformer
                model = SentenceTransformer(self.model_name)
                model.save(self.model_path)
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.model_path}")
            except Exception as e:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
                self.model = None
                return False

        try:
            loop = asyncio.get_event_loop()
            self.model = await loop.run_in_executor(
                None,
                lambda: SentenceTransformer(self.model_path, device=self.device)
            )
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_name}")

            await self._load_cache()
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.model = None
            return False

    async def embed(
        self,
        text: Union[str, List[str]],
        use_cache: bool = True,
        normalize: Optional[bool] = None
    ) -> Union[np.ndarray, List[np.ndarray]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞/—Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤."""
        start_time = datetime.now()

        if normalize is None:
            normalize = self.normalize

        if isinstance(text, str):
            embedding = await self._embed_single(text, use_cache, normalize)
            self._update_stats(start_time)
            return embedding

        if not text:
            return []

        if use_cache:
            embeddings = []
            texts_to_embed = []
            indices = []

            for i, t in enumerate(text):
                cache_key = self._get_cache_key(t)
                if cache_key in self.cache:
                    self.stats['cache_hits'] += 1
                    embeddings.append(self.cache[cache_key])
                else:
                    self.stats['cache_misses'] += 1
                    texts_to_embed.append(t)
                    indices.append(i)
                    embeddings.append(None)

            if not texts_to_embed:
                self.stats['embedding_requests'] += len(text)
                self.stats['total_embeddings'] += len(text)
                self._update_stats(start_time)
                return embeddings

            new_embeddings = await self._embed_batch(texts_to_embed, normalize)

            for idx, emb in zip(indices, new_embeddings):
                cache_key = self._get_cache_key(text[idx])
                self.cache[cache_key] = emb
                self.cache_metadata[cache_key] = {
                    'created': datetime.now().isoformat(),
                    'length': len(text[idx])
                }
                embeddings[idx] = emb

            await self._prune_cache()

            self.stats['embedding_requests'] += len(text)
            self.stats['total_embeddings'] += len(text)
            self._update_stats(start_time)

            return embeddings
        else:
            embeddings = await self._embed_batch(text, normalize)
            self.stats['embedding_requests'] += len(text)
            self.stats['total_embeddings'] += len(text)
            self._update_stats(start_time)
            return embeddings

    async def _embed_single(
        self,
        text: str,
        use_cache: bool,
        normalize: bool
    ) -> np.ndarray:
        if use_cache:
            cache_key = self._get_cache_key(text)
            if cache_key in self.cache:
                self.stats['cache_hits'] += 1
                return self.cache[cache_key]
            self.stats['cache_misses'] += 1

        embedding = await self._embed_batch([text], normalize)
        embedding = embedding[0]

        if use_cache:
            cache_key = self._get_cache_key(text)
            self.cache[cache_key] = embedding
            self.cache_metadata[cache_key] = {
                'created': datetime.now().isoformat(),
                'length': len(text)
            }
            await self._prune_cache()

        return embedding

    async def _embed_batch(self, texts: List[str], normalize: bool) -> List[np.ndarray]:
        loop = asyncio.get_event_loop()

        def encode():
            if self.model is None:
                raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                normalize_embeddings=normalize,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            return embeddings

        embeddings = await loop.run_in_executor(None, encode)
        self.stats['batch_processed'] += 1

        return [emb for emb in embeddings]

    def _get_cache_key(self, text: str) -> str:
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    async def _prune_cache(self):
        if len(self.cache) <= self.max_cache_size:
            return

        sorted_items = sorted(
            self.cache_metadata.items(),
            key=lambda x: x[1].get('created', '')
        )

        to_remove = int(len(self.cache) * 0.2)
        for key, _ in sorted_items[:to_remove]:
            if key in self.cache:
                del self.cache[key]
            if key in self.cache_metadata:
                del self.cache_metadata[key]

        logger.debug(f"üßπ –ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ–±—Ä–µ–∑–∞–Ω: —É–¥–∞–ª–µ–Ω–æ {to_remove} –∑–∞–ø–∏—Å–µ–π")

    async def _load_cache(self):
        try:
            cache_file = os.path.join(self.cache_dir, 'embeddings_cache.npz')
            meta_file = os.path.join(self.cache_dir, 'cache_metadata.json')

            if os.path.exists(cache_file) and os.path.exists(meta_file):
                data = np.load(cache_file, allow_pickle=True)
                self.cache = {k: v for k, v in data.items()}
                with open(meta_file, 'r', encoding='utf-8') as f:
                    self.cache_metadata = json.load(f)
                logger.info(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")

    async def save_cache(self):
        try:
            if not self.cache:
                return

            os.makedirs(self.cache_dir, exist_ok=True)
            cache_file = os.path.join(self.cache_dir, 'embeddings_cache.npz')
            np.savez_compressed(cache_file, **self.cache)
            meta_file = os.path.join(self.cache_dir, 'cache_metadata.json')
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_metadata, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    async def clear_cache(self):
        self.cache.clear()
        self.cache_metadata.clear()
        logger.info("üßπ –ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—á–∏—â–µ–Ω")
        await self.save_cache()
        return len(self.cache)

    def _update_stats(self, start_time: datetime):
        elapsed = (datetime.now() - start_time).total_seconds()
        n = self.stats['total_embeddings']
        if n > 0:
            self.stats['avg_embedding_time'] = (
                (self.stats['avg_embedding_time'] * (n - 1) + elapsed) / n
            )

    async def get_metrics(self) -> Dict[str, Any]:
        return {
            'embedding_requests': self.stats['embedding_requests'],
            'cache_hits': self.stats['cache_hits'],
            'cache_misses': self.stats['cache_misses'],
            'cache_hit_rate': self.stats['cache_hits'] / max(1, self.stats['cache_hits'] + self.stats['cache_misses']),
            'cache_size': len(self.cache),
            'batch_processed': self.stats['batch_processed'],
            'total_embeddings': self.stats['total_embeddings'],
            'avg_embedding_time': self.stats['avg_embedding_time'],
            'errors': self.stats['errors'],
            'model_name': self.model_name,
            'device': self.device,
            'embedding_dimension': self.embedding_dimension
        }

    def get_embedding_function(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ChromaDB.
        ChromaDB –æ–∂–∏–¥–∞–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é.
        """
        def embed_function(texts: List[str]) -> List[List[float]]:
            if self.model is None:
                raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                normalize_embeddings=self.normalize,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            return embeddings.tolist()
        return embed_function

    async def health_check(self) -> Dict[str, Any]:
        try:
            test_text = "test"
            emb = await self.embed(test_text, use_cache=False)
            return {
                'healthy': True,
                'message': 'Embedder is operational',
                'model': self.model_name,
                'device': self.device,
                'embedding_dim': len(emb) if isinstance(emb, np.ndarray) else 0,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'healthy': False,
                'message': str(e),
                'timestamp': datetime.now().isoformat()
            }

    async def close(self):
        await self.save_cache()
        logger.info("‚úÖ BGE-M3 Embedder –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
embedder = BGE_M3_Embedder()