"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ª—é–±—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
"""
import re
import json
import csv
from typing import List, Dict, Any, Optional
from pathlib import Path
from io import StringIO

from app.core.logging import logger
from app.services.knowledge.base import KnowledgeChunk


class UniversalDatasetProcessor:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ª—é–±—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    def __init__(self, chunk_size: int = 1000, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
    
    def detect_format(self, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            content_stripped = content.strip()
            
            if not content_stripped:
                return "empty"
            
            # –£–¥–∞–ª—è–µ–º BOM –µ—Å–ª–∏ –µ—Å—Ç—å
            if content_stripped.startswith('\ufeff'):
                content_stripped = content_stripped[1:]
            
            # 1. JSON –º–∞—Å—Å–∏–≤
            if content_stripped.startswith('[') and content_stripped.endswith(']'):
                try:
                    data = json.loads(content_stripped)
                    if isinstance(data, list):
                        logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω JSON –º–∞—Å—Å–∏–≤")
                        return "json_array"
                except:
                    pass
            
            # 2. JSON –æ–±—ä–µ–∫—Ç
            if content_stripped.startswith('{') and content_stripped.endswith('}'):
                try:
                    json.loads(content_stripped)
                    logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω JSON –æ–±—ä–µ–∫—Ç")
                    return "json_object"
                except:
                    pass
            
            # 3. JSONL
            lines = content_stripped.split('\n')[:20]
            valid_json_lines = 0
            for line in lines:
                line = line.strip()
                if line and line.startswith('{') and line.endswith('}'):
                    try:
                        json.loads(line)
                        valid_json_lines += 1
                    except:
                        continue
            
            if valid_json_lines >= 2:
                logger.info(f"üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω JSONL ({valid_json_lines} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫)")
                return "jsonl"
            
            # 4. CSV
            lines = content_stripped.split('\n')[:10]
            if len(lines) >= 2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ CSV
                try:
                    dialect = csv.Sniffer().sniff(lines[0])
                    if dialect:
                        logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω CSV")
                        return "csv"
                except:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞–ø—è—Ç—ã—Ö –∏–ª–∏ —Ç–æ—á–µ–∫ —Å –∑–∞–ø—è—Ç–æ–π
                    first_line = lines[0]
                    if ',' in first_line or ';' in first_line:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –±—É–∫–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                        if any(c.isalpha() for c in first_line):
                            logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω CSV (–ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É)")
                            return "csv"
            
            # 5. QA —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
            qa_patterns = [
                r'–í–æ–ø—Ä–æ—Å[:\s]+.*?–û—Ç–≤–µ—Ç[:\s]+',
                r'Question[:\s]+.*?Answer[:\s]+',
                r'Q:[^A]*A:',
                r'–≤–æ–ø—Ä–æ—Å[:\s].*?–æ—Ç–≤–µ—Ç[:\s]'
            ]
            
            for pattern in qa_patterns:
                if re.search(pattern, content, re.IGNORECASE | re.DOTALL):
                    logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π QA —Ñ–æ—Ä–º–∞—Ç")
                    return "qa_text"
            
            # 6. –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
            logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            return "text"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞: {e}")
            return "text"
    
    def process_content(self, content: str, file_name: str, metadata: Optional[Dict] = None) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ª—é–±–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        try:
            format_type = self.detect_format(content)
            logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª '{file_name}' –∫–∞–∫ {format_type}")
            
            if not content.strip():
                logger.warning("‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç–æ–π")
                return []
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
            if format_type == "json_array":
                chunks = self._process_json_array(content, file_name, metadata)
            elif format_type == "json_object":
                chunks = self._process_json_object(content, file_name, metadata)
            elif format_type == "jsonl":
                chunks = self._process_jsonl(content, file_name, metadata)
            elif format_type == "csv":
                chunks = self._process_csv(content, file_name, metadata)
            elif format_type == "qa_text":
                chunks = self._process_qa_text(content, file_name, metadata)
            else:
                chunks = self._process_text(content, file_name, metadata)
            
            logger.info(f"‚úÖ –§–∞–π–ª '{file_name}' –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            if not chunks:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞ '{file_name}'")
                # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —á–∞–Ω–∫ –∏–∑ –≤—Å–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                if len(content.strip()) > self.min_chunk_length:
                    chunk = self._create_text_chunk(
                        text=content.strip(),
                        index=0,
                        file_name=file_name,
                        metadata=metadata
                    )
                    chunks = [chunk]
                    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –æ–¥–∏–Ω —á–∞–Ω–∫ –∏–∑ –≤—Å–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ")
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_name}: {e}", exc_info=True)
            # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —á–∞–Ω–∫
            try:
                if content and len(content.strip()) > self.min_chunk_length:
                    chunk = self._create_text_chunk(
                        text=content.strip()[:5000],
                        index=0,
                        file_name=file_name,
                        metadata={**(metadata or {}), "error": str(e)[:100]}
                    )
                    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –∞–≤–∞—Ä–∏–π–Ω—ã–π —á–∞–Ω–∫")
                    return [chunk]
            except:
                pass
            return []
    
    def _process_json_array(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –º–∞—Å—Å–∏–≤–∞"""
        try:
            data = json.loads(content)
            chunks = []
            
            if isinstance(data, list):
                for i, item in enumerate(data):
                    if isinstance(item, (str, int, float, bool)):
                        chunk = self._create_text_chunk(
                            text=str(item),
                            index=i,
                            file_name=file_name,
                            metadata={**(metadata or {}), "json_type": "primitive"}
                        )
                        chunks.append(chunk)
                    elif isinstance(item, dict):
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dict –≤ —Ç–µ–∫—Å—Ç
                        text = json.dumps(item, ensure_ascii=False, indent=2)
                        chunk = self._create_text_chunk(
                            text=text,
                            index=i,
                            file_name=file_name,
                            metadata={**(metadata or {}), "json_type": "object"}
                        )
                        chunks.append(chunk)
            
            return chunks
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON –º–∞—Å—Å–∏–≤–∞: {e}")
            return []
    
    def _process_json_object(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –æ–±—ä–µ–∫—Ç–∞"""
        try:
            data = json.loads(content)
            text = json.dumps(data, ensure_ascii=False, indent=2)
            
            chunk = self._create_text_chunk(
                text=text,
                index=0,
                file_name=file_name,
                metadata={**(metadata or {}), "json_type": "single_object"}
            )
            return [chunk]
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON –æ–±—ä–µ–∫—Ç–∞: {e}")
            return []
    
    def _process_jsonl(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON Lines"""
        chunks = []
        lines = content.strip().split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            try:
                data = json.loads(line)
                if isinstance(data, dict):
                    text = json.dumps(data, ensure_ascii=False, indent=2)
                    chunk = self._create_text_chunk(
                        text=text,
                        index=i,
                        file_name=file_name,
                        metadata={**(metadata or {}), "json_type": "jsonl"}
                    )
                    chunks.append(chunk)
                elif isinstance(data, (str, int, float, bool)):
                    chunk = self._create_text_chunk(
                        text=str(data),
                        index=i,
                        file_name=file_name,
                        metadata={**(metadata or {}), "json_type": "jsonl_primitive"}
                    )
                    chunks.append(chunk)
            except:
                # –ï—Å–ª–∏ –Ω–µ JSON, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
                if len(line) > self.min_chunk_length:
                    chunk = self._create_text_chunk(
                        text=line,
                        index=i,
                        file_name=file_name,
                        metadata={**(metadata or {}), "json_type": "text_line"}
                    )
                    chunks.append(chunk)
        
        return chunks
    
    def _process_csv(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ CSV"""
        try:
            chunks = []
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            for delimiter in [',', ';', '\t']:
                try:
                    reader = csv.reader(StringIO(content), delimiter=delimiter)
                    rows = list(reader)
                    
                    if len(rows) > 1:  # –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞
                        for i, row in enumerate(rows):
                            if row:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                                text = f"–°—Ç—Ä–æ–∫–∞ {i+1}: {', '.join(row)}"
                                chunk = self._create_text_chunk(
                                    text=text,
                                    index=i,
                                    file_name=file_name,
                                    metadata={**(metadata or {}), "csv_row": i, "delimiter": delimiter}
                                )
                                chunks.append(chunk)
                        break
                except:
                    continue
            
            return chunks
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV: {e}")
            return []
    
    def _process_qa_text(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ QA —Ñ–æ—Ä–º–∞—Ç–∞"""
        chunks = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ QA –ø–∞—Ä
        patterns = [
            r'(?:–í–æ–ø—Ä–æ—Å|Question|Q)[:\s]*(.*?)(?:–û—Ç–≤–µ—Ç|Answer|A)[:\s]*(.*?)(?=(?:–í–æ–ø—Ä–æ—Å|Question|Q)[:\s]|$)',
            r'(?:–í|Q)[:\s\.]*(.*?)(?:–û|A)[:\s\.]*(.*?)(?=(?:–í|Q)[:\s\.]|$)',
            r'([^:\n]+)[:\s]*(.*?)\n([^:\n]+)[:\s]*(.*?)(?=\n[^:\n]+[:\s]|$)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            if matches:
                for i, match in enumerate(matches):
                    if len(match) >= 2:
                        question = match[0].strip()
                        answer = match[1].strip()
                        
                        if question and answer and len(question) > 3 and len(answer) > 3:
                            chunk = self._create_qa_chunk(
                                question=question,
                                answer=answer,
                                index=i,
                                file_name=file_name,
                                metadata=metadata
                            )
                            chunks.append(chunk)
                
                if chunks:
                    logger.info(f"üìù –ù–∞–π–¥–µ–Ω–æ {len(chunks)} QA –ø–∞—Ä")
                    break
        
        return chunks
    
    def _process_text(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        chunks = []
        
        # 1. –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –Ω–∞ –∞–±–∑–∞—Ü—ã –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –ø–æ –æ–¥–∏–Ω–∞—Ä–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º
        if not paragraphs:
            paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        
        # 3. –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ —Ç–æ—á–∫–∞–º
        if not paragraphs:
            sentences = re.split(r'[.!?]+', content)
            paragraphs = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        # 4. –ï—Å–ª–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        if not paragraphs and len(content) > self.chunk_size:
            for i in range(0, len(content), self.chunk_size - self.overlap):
                chunk_text = content[i:i + self.chunk_size]
                if len(chunk_text.strip()) > self.min_chunk_length:
                    chunk = self._create_text_chunk(
                        text=chunk_text,
                        index=i,
                        file_name=file_name,
                        metadata={**(metadata or {}), "chunk_type": "fixed_size"}
                    )
                    chunks.append(chunk)
            return chunks
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph) > self.min_chunk_length:
                # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
                if len(paragraph) > self.chunk_size:
                    for j in range(0, len(paragraph), self.chunk_size - self.overlap):
                        chunk_text = paragraph[j:j + self.chunk_size]
                        if len(chunk_text.strip()) > self.min_chunk_length:
                            chunk = self._create_text_chunk(
                                text=chunk_text,
                                index=f"{i}_{j}",
                                file_name=file_name,
                                metadata={**(metadata or {}), "chunk_type": "paragraph_split"}
                            )
                            chunks.append(chunk)
                else:
                    chunk = self._create_text_chunk(
                        text=paragraph,
                        index=i,
                        file_name=file_name,
                        metadata={**(metadata or {}), "chunk_type": "paragraph"}
                    )
                    chunks.append(chunk)
        
        return chunks
    
    def _create_qa_chunk(self, question: str, answer: str, index: int, file_name: str, metadata: Optional[Dict]) -> KnowledgeChunk:
        """–°–æ–∑–¥–∞–Ω–∏–µ QA —á–∞–Ω–∫–∞"""
        content = f"–í–æ–ø—Ä–æ—Å: {question}\n\n–û—Ç–≤–µ—Ç: {answer}"
        
        return KnowledgeChunk(
            id=f"{Path(file_name).stem}_qa_{index}_{hash(content[:50])}",
            content=content[:5000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            metadata={
                "source": file_name,
                "file_name": file_name,
                "type": "qa_pair",
                "question": question[:500],
                "answer": answer[:2000],
                "index": index,
                "content_type": "qa",
                **(metadata or {})
            }
        )
    
    def _create_text_chunk(self, text: str, index: int, file_name: str, metadata: Optional[Dict]) -> KnowledgeChunk:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞–Ω–∫–∞"""
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())
        
        return KnowledgeChunk(
            id=f"{Path(file_name).stem}_txt_{index}_{hash(text[:50])}",
            content=text[:5000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            metadata={
                "source": file_name,
                "file_name": file_name,
                "type": "text",
                "index": index,
                "content_type": "text",
                "text_length": len(text),
                **(metadata or {})
            }
        )