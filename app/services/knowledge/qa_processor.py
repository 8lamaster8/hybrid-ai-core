"""
–ü—Ä–æ—Å—Ç–æ–π QA –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
"""
import json
import re
from typing import List, Dict, Optional
from pathlib import Path

from app.core.logging import logger
from app.services.knowledge.base import KnowledgeChunk


class QADatasetProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö QA –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    def __init__(self):
        self.min_question_length = 5
        self.min_answer_length = 5
    
    def process_content(self, content: str, file_name: str, metadata: Optional[Dict] = None) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            content_stripped = content.strip()
            logger.info(f"üîÑ QADatasetProcessor –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç '{file_name}'")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç
            if self._is_json(content_stripped):
                return self._process_json(content_stripped, file_name, metadata)
            elif self._is_jsonl(content_stripped):
                return self._process_jsonl(content_stripped, file_name, metadata)
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—Å—Ç–µ
                chunks = self._extract_qa_from_text(content_stripped, file_name, metadata)
                if chunks:
                    return chunks
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ QA, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç
                return self._split_into_chunks(content_stripped, file_name, metadata)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_name}: {e}", exc_info=True)
            return []
    
    def _is_json(self, content: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ JSON"""
        content = content.strip()
        return (content.startswith('[') and content.endswith(']')) or \
               (content.startswith('{') and content.endswith('}'))
    
    def _is_jsonl(self, content: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JSON Lines —Ñ–æ—Ä–º–∞—Ç"""
        lines = content.strip().split('\n')[:5]
        if len(lines) < 2:
            return False
        
        json_lines = 0
        for line in lines:
            line = line.strip()
            if line and line.startswith('{') and line.endswith('}'):
                try:
                    json.loads(line)
                    json_lines += 1
                except:
                    pass
        
        return json_lines >= 2
    
    def _process_json(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        try:
            data = json.loads(content)
            chunks = []
            
            if isinstance(data, list):
                for i, item in enumerate(data):
                    chunks.extend(self._process_qa_item(item, i, file_name, metadata))
            elif isinstance(data, dict):
                chunks.extend(self._process_qa_item(data, 0, file_name, metadata))
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω JSON —Ñ–∞–π–ª {file_name}: {len(chunks)} QA –ø–∞—Ä")
            return chunks
            
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {e}")
            return []
    
    def _process_jsonl(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON Lines —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        chunks = []
        lines = content.strip().split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            try:
                item = json.loads(line)
                chunks.extend(self._process_qa_item(item, i, file_name, metadata))
            except:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                pass
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω JSONL —Ñ–∞–π–ª {file_name}: {len(chunks)} QA –ø–∞—Ä")
        return chunks
    
    def _extract_qa_from_text(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ QA –ø–∞—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        chunks = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ QA –ø–∞—Ä
        patterns = [
            r'(?:–í–æ–ø—Ä–æ—Å|Question|Q)[:\s]*(.*?)[\s\n]*(?:–û—Ç–≤–µ—Ç|Answer|A)[:\s]*(.*?)(?=\n\n|\n(?:–í–æ–ø—Ä–æ—Å|Question|Q)[:\s]|$)',
            r'(?:Q:|Question:|–í–æ–ø—Ä–æ—Å:|–í:)[\s\n]*(.*?)[\s\n]*(?:A:|Answer:|–û—Ç–≤–µ—Ç:|–û:)[\s\n]*(.*?)(?=\n\n|\n(?:Q:|Question:|–í–æ–ø—Ä–æ—Å:|–í:)|$)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            if matches:
                for i, match in enumerate(matches):
                    if len(match) >= 2:
                        question = match[0].strip()
                        answer = match[1].strip()
                        
                        if (len(question) >= self.min_question_length and 
                            len(answer) >= self.min_answer_length):
                            chunks.append(self._create_qa_chunk(
                                question=question,
                                answer=answer,
                                index=i,
                                file_name=file_name,
                                metadata=metadata
                            ))
                
                if chunks:
                    logger.info(f"üìù –ù–∞–π–¥–µ–Ω–æ {len(chunks)} QA –ø–∞—Ä –≤ —Ç–µ–∫—Å—Ç–µ")
                    break
        
        return chunks
    
    def _split_into_chunks(self, content: str, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        if not paragraphs:
            paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph) > 10:
                chunks.append(self._create_text_chunk(
                    text=paragraph,
                    index=i,
                    file_name=file_name,
                    metadata={**(metadata or {}), "type": "paragraph"}
                ))
        
        logger.info(f"üìÑ –†–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} –∞–±–∑–∞—Ü–µ–≤: {file_name}")
        return chunks
    
    def _process_qa_item(self, item: any, index: int, file_name: str, metadata: Optional[Dict]) -> List[KnowledgeChunk]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ QA –¥–∞–Ω–Ω—ã—Ö"""
        chunks = []
        
        if isinstance(item, dict):
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞
            question_keys = ['question', 'input', 'q', '–≤–æ–ø—Ä–æ—Å', 'query', 'prompt']
            answer_keys = ['answer', 'output', 'a', '–æ—Ç–≤–µ—Ç', 'response', 'completion']
            
            question = None
            answer = None
            
            for q_key in question_keys:
                if q_key in item:
                    question = str(item[q_key])
                    break
            
            for a_key in answer_keys:
                if a_key in item:
                    answer = str(item[a_key])
                    break
            
            if question and answer:
                if (len(question) >= self.min_question_length and 
                    len(answer) >= self.min_answer_length):
                    chunks.append(self._create_qa_chunk(
                        question=question,
                        answer=answer,
                        index=index,
                        file_name=file_name,
                        metadata=metadata
                    ))
            elif question:
                # –¢–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å
                if len(question) >= self.min_question_length:
                    chunks.append(self._create_text_chunk(
                        text=question,
                        index=index,
                        file_name=file_name,
                        metadata={**(metadata or {}), "type": "question_only"}
                    ))
            elif answer:
                # –¢–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
                if len(answer) >= self.min_answer_length:
                    chunks.append(self._create_text_chunk(
                        text=answer,
                        index=index,
                        file_name=file_name,
                        metadata={**(metadata or {}), "type": "answer_only"}
                    ))
            else:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å –æ–±—ä–µ–∫—Ç –∫–∞–∫ —Ç–µ–∫—Å—Ç
                text = json.dumps(item, ensure_ascii=False)[:1000]
                chunks.append(self._create_text_chunk(
                    text=text,
                    index=index,
                    file_name=file_name,
                    metadata={**(metadata or {}), "type": "json_object"}
                ))
        
        elif isinstance(item, str):
            # –ü—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞
            if len(item.strip()) > 10:
                chunks.append(self._create_text_chunk(
                    text=item,
                    index=index,
                    file_name=file_name,
                    metadata=metadata
                ))
        
        return chunks
    
    def _create_qa_chunk(self, question: str, answer: str, index: int, file_name: str, metadata: Optional[Dict]) -> KnowledgeChunk:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–ª—è QA –ø–∞—Ä—ã"""
        content = f"–í–æ–ø—Ä–æ—Å: {question}\n\n–û—Ç–≤–µ—Ç: {answer}"
        
        return KnowledgeChunk(
            id=f"{Path(file_name).stem}_qa_{index}_{hash(question[:50])}",
            content=content,
            metadata={
                "source": file_name,
                "file_name": file_name,
                "type": "qa_pair",
                "question": question[:200],
                "answer": answer[:500],
                "index": index,
                **(metadata or {})
            }
        )
    
    def _create_text_chunk(self, text: str, index: int, file_name: str, metadata: Optional[Dict]) -> KnowledgeChunk:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞–Ω–∫–∞"""
        return KnowledgeChunk(
            id=f"{Path(file_name).stem}_text_{index}_{hash(text[:50])}",
            content=text[:3000],
            metadata={
                "source": file_name,
                "file_name": file_name,
                "type": metadata.get("type", "text") if metadata else "text",
                "index": index,
                **(metadata or {})
            }
        )